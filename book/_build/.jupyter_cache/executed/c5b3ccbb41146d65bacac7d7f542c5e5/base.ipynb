{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25039fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "file_path1 = '../data/training_ready_climatology_data.csv'\n",
    "file_path2 = '../data/training_ready_snotel_data.csv'\n",
    "file_path3 = '../data/training_ready_terrain_data.csv'\n",
    "# Read each CSV file into a Dask DataFrame\n",
    "df1 = dd.read_csv(file_path1)\n",
    "df2 = dd.read_csv(file_path2)\n",
    "df3 = dd.read_csv(file_path3)\n",
    "# Perform data type conversion for latitude and longitude columns\n",
    "df1['lat'] = df1['lat'].astype(float)\n",
    "df1['lon'] = df1['lon'].astype(float)\n",
    "df2['lat'] = df2['lat'].astype(float)\n",
    "df2['lon'] = df2['lon'].astype(float)\n",
    "df3['lat'] = df3['lat'].astype(float)\n",
    "df3['lon'] = df3['lon'].astype(float)\n",
    "#rename the columns to match the other dataframes\n",
    "df2 = df2.rename(columns={\"Date\": \"date\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43de425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n",
    "merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'date'])\n",
    "\n",
    "# Merge the third DataFrame based on 'lat' and 'lon'\n",
    "merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c200ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/gokulprathin/swe-workflow-book/book/data/model_training_data.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df2.to_csv('../data/model_training_data.csv', index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df5d523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/gokulprathin/swe-workflow-book/book/data/model_training_cleaned.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_csv = '../data/model_training_data.csv'\n",
    "\n",
    "# List of columns you want to extract\n",
    "selected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n",
    "                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n",
    "                    'elevation',\n",
    "                    'slope', 'curvature', 'aspect', 'eastness',\n",
    "                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n",
    "# Read the CSV file into a Dask DataFrame\n",
    "df = dd.read_csv(input_csv, usecols=selected_columns)\n",
    "\n",
    "df = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n",
    "\n",
    "# Replace 'output.csv' with the desired output file name\n",
    "output_csv = '../data/model_training_cleaned.csv'\n",
    "\n",
    "# Write the selected columns to a new CSV file\n",
    "df.to_csv(output_csv, index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf07ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "working_dir = f\"../data\"\n",
    "work_dir = working_dir\n",
    "final_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\n",
    "chunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8668c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file '../data/final_merged_data_3yrs_all_active_stations_v1.csv' exists. Skipping\n"
     ]
    }
   ],
   "source": [
    "amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n",
    "snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n",
    "gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n",
    "terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n",
    "fsca_file = f'{working_dir}/fsca_final_training_all.csv'\n",
    "final_final_output_file = f'{work_dir}/{final_output_name}'\n",
    "\n",
    "if os.path.exists(final_final_output_file):\n",
    "    print(f\"The file '{final_final_output_file}' exists. Skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a599b0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files with a smaller chunk size and compression\n",
    "amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n",
    "print(\"amsr.columns = \", amsr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3cdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n",
      "       'snow_depth', 'air_temperature_observed_f'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n",
    "print(\"snotel.columns = \", snotel.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f57204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n",
      "       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n",
      "       'relative_humidity_rmax', 'relative_humidity_rmin',\n",
      "       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n",
    "# Drop the 'Unnamed: 0' column\n",
    "gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n",
    "print(\"gridmet.columns = \", gridmet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb830353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n",
      "       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n",
    "# rename columns to match the other dataframes\n",
    "terrain = terrain.rename(columns={\n",
    "    \"latitude\": \"lat\", \n",
    "    \"longitude\": \"lon\"\n",
    "})\n",
    "# select only the columns we need for the final output\n",
    "terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n",
    "print(\"terrain.columns = \", terrain.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd4dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowcover.columns =  Index(['date', 'lat', 'lon', 'fSCA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n",
    "# rename columns to match the other dataframes\n",
    "snowcover = snowcover.rename(columns={\n",
    "    \"latitude\": \"lat\", \n",
    "    \"longitude\": \"lon\"\n",
    "})\n",
    "print(\"snowcover.columns = \", snowcover.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79ad4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the dataframes are partitioned\n"
     ]
    }
   ],
   "source": [
    "# Repartition DataFrames for optimized processing\n",
    "amsr = amsr.repartition(partition_size=chunk_size)\n",
    "snotel = snotel.repartition(partition_size=chunk_size)\n",
    "gridmet = gridmet.repartition(partition_size=chunk_size)\n",
    "gridmet = gridmet.rename(columns={'day': 'date'})\n",
    "terrain = terrain.repartition(partition_size=chunk_size)\n",
    "snow_cover = snowcover.repartition(partition_size=chunk_size)\n",
    "print(\"all the dataframes are partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50240e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to merge amsr and snotel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_snotel.csv\n",
      "start to merge gridmet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_gridmet.csv\n",
      "start to merge terrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_terrain.csv\n",
      "start to merge snowcover\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_snow_cover.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed. ../data/final_merged_data_3yrs_all_active_stations_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames based on specified columns\n",
    "print(\"start to merge amsr and snotel\")\n",
    "merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge gridmet\")\n",
    "merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge terrain\")\n",
    "merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge snowcover\")\n",
    "merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "# Save the merged DataFrame to a CSV file in chunks\n",
    "output_file = os.path.join(working_dir, final_output_name)\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f'Merge completed. {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c63a4000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n",
    "df = dd.read_csv(f'{work_dir}/{final_output_name}', dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n",
    "print('Data cleaning completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beb02f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted training data is saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "def sort_training_data(input_training_csv, sorted_training_csv):\n",
    "    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n",
    "    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "\n",
    "    # Persist the Dask DataFrame in memory\n",
    "    ddf = ddf.persist()\n",
    "\n",
    "    # Sort Dask DataFrame by three columns: date, lat, and Lon\n",
    "    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n",
    "\n",
    "    # Save the sorted Dask DataFrame to a new CSV file\n",
    "    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n",
    "    print(f\"sorted training data is saved to {sorted_training_csv}\")\n",
    "\n",
    "final_final_output_file = f'{work_dir}/{final_output_name}'\n",
    "sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}