{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAdqXXvf1M7"
      },
      "source": [
        "# Model Prediction/Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0bqKyYqf5DT"
      },
      "source": [
        "Finally, we are at the final Chapter where we see the end-product of the model created. As we venture into this critical phase, the model_predict script emerges, guiding the way toward understanding and anticipating the future of snow water equivalent (SWE) through the ExtraTree model. This chapter delves into the intricacies of this script, unraveling the processes that transforms raw, unprocessed data into precise predictions that illuminate the path forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.1 Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFDziC_Df-lP"
      },
      "source": [
        "### 9.1.1 Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction process begins with loading of data from a CSV file. This data includes a vast array of variables that are essential for making accurate SWE predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LkzaZTCtgJ5R"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "  \"\"\"\n",
        "  Load data from a CSV file.\n",
        "  Args: file_path (str): Path to the CSV file containing the data.\n",
        "  Returns: pd.DataFrame: A pandas DataFrame containing the loaded data.\n",
        "   \"\"\"\n",
        "  return pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUwlJCu6ge2k"
      },
      "source": [
        "### 9.1.2 Preprocessing the Data\n",
        "Once loaded, the data undergoes a transformation process to ensure it aligns with the model's requirements. This step includes converting dates, renaming columns for consistency, and selecting relevant features.<br />\n",
        "Prepprocessing is crucial to ensure that the data is in the correct format for the model, which directly impacts the accurancy of the prefictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wXQ-4JBdgjdG"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "  \"\"\"\n",
        "  Preprocess the input data for model prediction.\n",
        "  Args: data (pd.DataFrame): Input data in the form of a pandas DataFrame.\n",
        "  Returns: pd.DataFrame: Preprocessed data ready for prediction.\n",
        "  \"\"\"\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "  data.replace('--', pd.NA, inplace=True)\n",
        "  data.rename(columns={'Latitude': 'lat', 'Longitude': 'lon',\n",
        "                       'vpd': 'mean_vapor_pressure_deficit',\n",
        "                       'vs': 'wind_speed', 'pr': 'precipitation_amount',\n",
        "                       'etr': 'potential_evapotranspiration', 'tmmn': 'air_temperature_tmmn',\n",
        "                       'tmmx': 'air_temperature_tmmx', 'rmin': 'relative_humidity_rmin',\n",
        "                       'rmax': 'relative_humidity_rmax', 'cumulative_AMSR_SWE': 'cumulative_SWE',\n",
        "                       'cumulative_AMSR_Flag': 'cumulative_Flag', 'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n",
        "                       'cumulative_etr': 'cumulative_potential_evapotranspiration', 'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n",
        "                       'cumulative_rmax': 'cumulative_relative_humidity_rmax', 'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n",
        "                       'cumulative_pr': 'cumulative_precipitation_amount', 'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n",
        "                       'cumulative_vs': 'cumulative_wind_speed', 'AMSR_SWE': 'SWE', 'AMSR_Flag': 'Flag', }, inplace=True)\n",
        "  print(data.head())\n",
        "  print(data.columns)\n",
        "  selected_columns.remove(\"swe_value\")\n",
        "  desired_order = selected_columns + ['lat', 'lon',]\n",
        "  data = data[desired_order]\n",
        "  data = data.reindex(columns=desired_order)\n",
        "  print(\"reorganized columns: \", data.columns)\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.2 Model Loading and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZg6If3Lhecf"
      },
      "source": [
        "### 9.2.1 Loading the Model\n",
        "The script retrieves the pre-trained ExtraTree model, which is used to generate predictions based on the processed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d7EsL7aVhltv"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load a machine learning model from a file.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model file.\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded machine learning model.\n",
        "    \"\"\"\n",
        "    return joblib.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRBPyBVxhu3r"
      },
      "source": [
        "### 9.2.2 Predicting SWE\n",
        "\n",
        "The `predict_swe` function prepares the input data and generates predictions using the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uv3nAmw-h3EO"
      },
      "outputs": [],
      "source": [
        "def predict_swe(model, data):\n",
        "  \"\"\"\n",
        "  Predict snow water equivalent (SWE) using a machine learning model.\n",
        "  Args: model: The machine learning model for prediction.\n",
        "  data (pd.DataFrame): Input data for prediction.\n",
        "  Returns: pd.DataFrame: Dataframe with predicted SWE values.\n",
        "  \"\"\"\n",
        "  data = data.fillna(-999)\n",
        "  input_data = data\n",
        "  input_data = data.drop([\"lat\", \"lon\"], axis=1)\n",
        "  predictions = model.predict(input_data)\n",
        "  data['predicted_swe'] = predictions\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYeFXD4xiJpi"
      },
      "source": [
        "- It fills missing values with a designated placeholder (-999), a common practice to ensure machine learning algorithms, can process the data without encountering errors due to missing values. This step reflects a balance between data integrity and computational requirements, enabling the model to make predictions even in the absence of complete information.\n",
        "\n",
        "- At the core of predict_swe is the model's `predict()` method invocation. This step is where the machine learning model, trained on historical data, applies its learned patterns to the new, unseen data. The decision to drop geographical identifiers (lat, lon) before prediction underscores a focus on the environmental and temporal factors influencing SWE, aligning the model's inputs with its training regime.\n",
        "\n",
        "- The function concludes by appending the model's predictions back to the original dataset as a new column, `predicted_swe`. This enrichment transforms the dataset from a static snapshot of past and present conditions into a dynamic forecast of future snow water equivalents. This step is critical for stakeholders relying on accurate SWE predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2.3 Predict Function\n",
        "\n",
        "The predict function is what manages the entire prediction process from start to finish. It starts by loading the pre-trained model, which embodies the project's strength of making predictions by preserving and leveraging the accumulated knowledge encapsulated within the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict():\n",
        "    \"\"\"\n",
        "    Main function for predicting snow water equivalent (SWE).\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    height = 666\n",
        "    width = 694\n",
        "    model_path = f'{homedir}/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n",
        "    print(f\"Using model: {model_path}\")\n",
        "\n",
        "    new_data_path = f'{work_dir}/testing_all_ready_{test_start_date}.csv'\n",
        "    latest_output_path = f'{work_dir}/test_data_predicted_latest_{test_start_date}.csv'\n",
        "    output_path = f'{work_dir}/test_data_predicted_{generate_random_string(5)}.csv'\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "        print(f\"File '{output_path}' has been removed.\")\n",
        "\n",
        "    model = load_model(model_path)\n",
        "    new_data = load_data(new_data_path)\n",
        "    #print(\"new_data shape: \", new_data.head())\n",
        "\n",
        "    preprocessed_data = preprocess_data(new_data)\n",
        "    if len(new_data) < len(preprocessed_data):\n",
        "      raise ValueError(\"Why the preprocessed data increased?\")\n",
        "\n",
        "    predicted_data = predict_swe(model, preprocessed_data)\n",
        "    print(\"how many predicted? \", len(predicted_data))\n",
        "\n",
        "    if \"date\" not in preprocessed_data:\n",
        "      preprocessed_data[\"date\"] = test_start_date\n",
        "    predicted_data = merge_data(preprocessed_data, predicted_data)\n",
        "\n",
        "    predicted_data.to_csv(output_path, index=False)\n",
        "    print(\"Prediction successfully done \", output_path)\n",
        "\n",
        "    shutil.copy(output_path, latest_output_path)\n",
        "    print(f\"Copied to {latest_output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following model loading, the function navigates the data landscape, loading new data for prediction and preprocessing it to align with the model's requirements. This step is critical, as it transforms raw data into a format that the model can interpret, ensuring the accuracy and relevance of the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.3 Post-Processing and Merging Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3.1 Merging Predicted Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmbl6QxXiVIE"
      },
      "source": [
        "`merge_data` meticulously combines the predicted SWE values with the original dataset. It employs conditional logic to adjust predictions based on specific criteria, such as nullifying predictions in the absence of key environmental data. This approach underscores a commitment to precision, ensuring that the predictions reflect a nuanced understanding of the environmental context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aa_il7YriZlE"
      },
      "outputs": [],
      "source": [
        "def merge_data(original_data, predicted_data):\n",
        "    \"\"\"\n",
        "    Merge predicted SWE data with the original data.\n",
        "    Args: original_data (pd.DataFrame): Original input data.\n",
        "    predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n",
        "    Returns: pd.DataFrame: Merged dataframe.\n",
        "    \"\"\"\n",
        "    if \"date\" not in predicted_data:\n",
        "        predicted_data[\"date\"] = test_start_date\n",
        "    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n",
        "    print(\"original_data.columns: \", original_data.columns)\n",
        "    print(\"new_data_extracted.columns: \", new_data_extracted.columns)\n",
        "    print(\"new prediction statistics: \", new_data_extracted[\"predicted_swe\"].describe())\n",
        "    merged_df = original_data.merge(new_data_extracted, on=['date', 'lat', 'lon'], how='left')\n",
        "    merged_df.loc[merged_df['fsca'] == 237, 'predicted_swe'] = 0\n",
        "    merged_df.loc[merged_df['fsca'] == 239, 'predicted_swe'] = 0\n",
        "    merged_df.loc[merged_df['cumulative_fsca'] == 0, 'predicted_swe'] = 0\n",
        "    merged_df.loc[merged_df['air_temperature_tmmx'].isnull(), 'predicted_swe'] = 0\n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER1Ej6OPioDb"
      },
      "source": [
        "### 9.3.2 Technical Execution of the function\n",
        "\n",
        "Merging datasets based on date, latitude, and longitude—exemplifies the complex use of data science. It ensures that each predicted SWE value is accurately aligned with its corresponding geographical and temporal marker, preserving the integrity and utility of the predictions. This process not only highlights the technical sophistication of the SnowCast project but also its dedication to delivering reliable and actionable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slezgjgmjY6-"
      },
      "source": [
        "## 9.4 Delivering Predictions\n",
        "\n",
        "Finally, the predict function executes predict_swe, merges the predictions with the original data, and saves the enriched dataset. The choice of a dynamically generated filename for saving predictions demonstrates an understanding of operational requirements, ensuring that each prediction cycle is uniquely identifiable.\n",
        "\n",
        "![](../img/Pred_Delivery.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u61bKY8mj0eE"
      },
      "source": [
        "## 9.5 Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRHveVzRj91l"
      },
      "source": [
        "### 9.5.1 Converting the Predictions into Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHsM6EIRlQdQ"
      },
      "source": [
        "These are the different functions used for this process of predicitons to Images\n",
        "\n",
        "- **convert csvs to images simple:** This Is the function that takes the raw data and converts them into Geographical images.\n",
        "\n",
        "- **Data Loading:** This begins by ingesting the CSV containing SWE predictions, ensuring every data point is primed for visualization.\n",
        "\n",
        "- **Custom Colormap Creation:** It employs a custom colormap, crafted to represent various ranges of SWE, providing an intuitive visual understanding of snow coverage.\n",
        "\n",
        "- **Geospatial Plotting:** This utilizes the geographical coordinates within the data to accurately place each prediction on the map, ensuring a realistic representation of SWE distribution.\n",
        "\n",
        "- **Merge data:** The merge_data function combines the predicted SWE values with their corresponding geographical markers.\n",
        "\n",
        "- **Conditional Adjustments**: Conditional adjustment refines the predicted values based on specific criteria, ensuring the visual representation aligns with realistic expectations of SWE.\n",
        "\n",
        "- **Spatial Accuracy:** This aligns predictions with their exact geographical locations, ensuring that the visual output is as informative as it is accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QOdrzg6oLNh"
      },
      "source": [
        "**Custom Colormap:** A list named colors defines the color scheme for the colormap, using RGB tuples for each color. These colors are intended to represent different levels of SWE, from low to high(light gray to dark red).\n",
        "\n",
        "**Geographical Boundaries:** lon_min, lon_max, lat_min, and lat_max define the geographical area of interest by specifying the minimum and maximum longitudes and latitudes. This setting targets the visualization and analysis efforts to the Western United States."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qMeBEobUoJcW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.colors as mcolors\n",
        "\n",
        "\n",
        "colors = [\n",
        "    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n",
        "    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n",
        "    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n",
        "    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n",
        "    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n",
        "    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n",
        "    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n",
        "    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n",
        "    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n",
        "    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n",
        "    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n",
        "    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n",
        "    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n",
        "]\n",
        "cmap_name = 'custom_snow_colormap'\n",
        "custom_cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "lon_min, lon_max = -125, -100\n",
        "lat_min, lat_max = 25, 49.5\n",
        "\n",
        "# Define value ranges for color mapping\n",
        "fixed_value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gFem_1DlsPd"
      },
      "source": [
        "- **Convert csv to geotiff:** This function mainly helps in converting images to geographically accurate maps.\n",
        "\n",
        "- **Rasterization:** It transforms the CSV data into a raster format, suitable for creating detailed geospatial maps.\n",
        "\n",
        "- **Resolution and Coverage:** This carefully defines the resolution and geographical extent of the output map, ensuring that it captures the full scope of the predictions.\n",
        "\n",
        "- **Geospatial Alignment:** Geospatial Alignment utilizes rasterio and geopandas libraries to ensure that each pixel in the output map accurately represents the predicted SWE values at specific geographical coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7h9706LXVba2"
      },
      "outputs": [],
      "source": [
        "def convert_csvs_to_images():\n",
        "    \"\"\"\n",
        "    Convert CSV data to images with color-coded SWE predictions.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    global fixed_value_ranges\n",
        "    data = pd.read_csv(f\"{homedir}/gridmet_test_run/test_data_predicted_n97KJ.csv\")\n",
        "    print(\"statistic of predicted_swe: \", data['predicted_swe'].describe())\n",
        "    data['predicted_swe'].fillna(0, inplace=True)\n",
        "\n",
        "    for column in data.columns:\n",
        "        column_data = data[column]\n",
        "        print(column_data.describe())\n",
        "\n",
        "    # Create a figure with a white background\n",
        "    fig = plt.figure(facecolor='white')\n",
        "\n",
        "\n",
        "\n",
        "    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n",
        "                projection='merc', resolution='i')\n",
        "\n",
        "    x, y = m(data['lon'].values, data['lat'].values)\n",
        "    print(data.columns)\n",
        "\n",
        "    color_mapping, value_ranges = create_color_maps_with_value_range(data[\"predicted_swe\"], fixed_value_ranges)\n",
        "\n",
        "    # Plot the data using the custom colormap\n",
        "    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n",
        "\n",
        "    # Draw coastlines and other map features\n",
        "    m.drawcoastlines()\n",
        "    m.drawcountries()\n",
        "    m.drawstates()\n",
        "\n",
        "    reference_date = datetime(1900, 1, 1)\n",
        "    day_value = day_index\n",
        "\n",
        "    result_date = reference_date + timedelta(days=day_value)\n",
        "    today = result_date.strftime(\"%Y-%m-%d\")\n",
        "    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Add a title\n",
        "    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n",
        "\n",
        "    # Add labels for latitude and longitude on x and y axes with smaller font size\n",
        "    plt.xlabel('Longitude', fontsize=6)\n",
        "    plt.ylabel('Latitude', fontsize=6)\n",
        "\n",
        "    # Add longitude values to the x-axis and adjust font size\n",
        "    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n",
        "    x_tick_labels_str = [f\"{lon:.1f}°W\" if lon < 0 else f\"{lon:.1f}°E\" for lon in x_ticks_labels]\n",
        "    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n",
        "    plt.gca().set_xticklabels(x_tick_labels_str)\n",
        "\n",
        "    # Add latitude values to the y-axis and adjust font size\n",
        "    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n",
        "    y_tick_labels_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_ticks_labels]\n",
        "    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n",
        "    plt.gca().set_yticklabels(y_tick_labels_str)\n",
        "\n",
        "    # Convert map coordinates to latitude and longitude for y-axis labels\n",
        "    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n",
        "    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n",
        "    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n",
        "    y_tick_positions_lat_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_tick_positions_lat]\n",
        "    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n",
        "\n",
        "    # Create custom legend elements using the same colormap\n",
        "    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n",
        "\n",
        "    # Create the legend outside the map\n",
        "    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n",
        "    legend.set_bbox_to_anchor((1.01, 1))\n",
        "\n",
        "    # Remove the color bar\n",
        "    #plt.colorbar().remove()\n",
        "\n",
        "    plt.text(0.98, 0.02, 'Copyright © SWE Wormhole Team',\n",
        "             horizontalalignment='right', verticalalignment='bottom',\n",
        "             transform=plt.gcf().transFigure, fontsize=6, color='black')\n",
        "\n",
        "    # Set the aspect ratio to 'equal' to keep the plot at the center\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Adjust the bottom and top margins to create more white space between the title and the plot\n",
        "    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n",
        "    # Show the plot or save it to a file\n",
        "    new_plot_path = f'{homedir}/gridmet_test_run/predicted_swe-{test_start_date}.png'\n",
        "    print(f\"The new plot is saved to {new_plot_path}\")\n",
        "    plt.savefig(new_plot_path)\n",
        "    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMErFn7ul35G"
      },
      "source": [
        "### 9.5.2 Deploy images to website\n",
        "This is the process that helps in Deploying the visual insights\n",
        "\n",
        "**1. copy files to right folder** --\n",
        "\n",
        "Function: Bridging Computational Outputs with Public Access At the heart of our deployment strategy lies the copy_files_to_right_folder function.\n",
        "This function acts as the bridge, transferring the visual and data outputs of SnowCast from the secure confines of its computational environment to a publicly accessible web directory.\n",
        "\n",
        "Here's how it achieves this pivotal role:\n",
        "\n",
        "\n",
        "*   Folder Synchronization: Utilizing distutils.dir_util.copy_tree, it ensures that all visual comparisons and predictions are mirrored from the SnowCast workspace to the web server's plotting directory, maintaining up-to-date access for users worldwide.\n",
        "*   Selective Deployment: Through meticulous directory traversal, it distinguishes between .png visualizations and .tif geospatial files, ensuring each file type is deployed to its rightful place for optimal public utility.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwC4XJxHkClU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**2. create mapserver map config: Crafts interactive Maps**\n",
        "\n",
        "The magic of SnowCast is not just in its predictions but in how these predictions are presented. The create_mapserver_map_config function crafts a MapServer configuration for each GeoTIFF prediction file, transforming static data into interactive, exploratory maps.\n",
        "\n",
        "\n",
        "*   **Dynamic Configuration:** By generating a .map file for each prediction, this function lays the groundwork for interactive map services, allowing users to explore SWE predictions across different regions and times.\n",
        "*   **Intuitive Visualization:** The custom MapServer configuration leverages the power of geographical information systems (GIS) to provide an intuitive, visual representation of complex SWE data, making it accessible to experts and laypeople alike.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTfd_L9UkBt"
      },
      "source": [
        "![SWE MAP](../img/SWE_Map.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V64pCx4mT1b"
      },
      "source": [
        "**3. refresh available date list: Refreshing the Forecast**\n",
        "\n",
        "The refresh_available_date_list function ensures that the SnowCast portal remains current, reflecting the latest predictions and analyses. By dynamically updating the available date list with new predictions, it guarantees that users have access to the most recent insights.\n",
        "\n",
        "\n",
        "*   Data Frame Dynamics: This function creates a pandas DataFrame to catalog the available predictions, linking each date with its corresponding visualization and data file, thereby ensuring the portal's content is both comprehensive and current.\n",
        "*   Seamless Integration: The updated date list is saved as a CSV file, seamlessly integrating with the web portal's infrastructure to refresh the interactive calendar, guiding users to the latest SWE predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUMa9t2iUkBt"
      },
      "source": [
        "![](../img/Refresh_pred.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
