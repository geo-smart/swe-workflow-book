{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4dd90337994e43",
   "metadata": {},
   "source": [
    "# Cumulative Time Series Analysis \n",
    "\n",
    "- In this chapter we process and analyze Snow Water Equivalent (SWE) and meteorological data to prepare it for time series analysis. \n",
    "\n",
    "- This transformation is essential for analyzing how specific variables, such as `snow cover` and `temperature`, change over time at different geographic locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dea4f",
   "metadata": {},
   "source": [
    "## 5.1 Importing the libraries and Input files\n",
    "\n",
    "Here we set up the environment and define file paths for managing and processing environmental data.\n",
    "\n",
    "**Dask** is a parallel computing library in Python that enables scalable and distributed computing. \n",
    "\n",
    "- `current_ready_csv_path` holds the file path for the initial dataset that has been merged and sorted. The dataset contains data from the past three years and includes all active stations.\n",
    "- `cleaned_csv_path` defines the path for a cleaned version of the dataset. \n",
    "- `target_time_series_csv_path` specifies the file path where the time series version of the cleaned dataset will be saved. \n",
    "- `backup_time_series_csv_path`  holds the file path for a backup of the time series dataset. \n",
    "- `target_time_series_cumulative_csv_path` defines the file path for a cumulative version of the time series dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f4a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"../data\"\n",
    "\n",
    "# Set Pandas options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# Define file paths for various CSV files\n",
    "current_ready_csv_path = f'{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv'\n",
    "cleaned_csv_path = f\"{current_ready_csv_path}_cleaned_nodata.csv\"\n",
    "target_time_series_csv_path = f'{cleaned_csv_path}_time_series_v1.csv'\n",
    "backup_time_series_csv_path = f'{cleaned_csv_path}_time_series_v1_bak.csv'\n",
    "target_time_series_cumulative_csv_path = f'{cleaned_csv_path}_time_series_cumulative_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f350a",
   "metadata": {},
   "source": [
    "### 5.2 Cleaning and Preparation\n",
    "\n",
    "Here we clean the dataset to remove rows with missing SWE values. We Utilize Dask for efficient processing of potentially large datasets, ensuring scalability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe491f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask_df_filtered.shape =  (Delayed('int-5cc8b7ae-25d5-478c-b53d-664926e9ce4f'), 26)\n",
      "The filtered csv with no swe values is saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path):\n",
    "    # Read Dask DataFrame from CSV\n",
    "    dask_df = dd.read_csv(current_ready_csv_path, dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "\n",
    "    # Remove rows where 'swe_value' is empty\n",
    "    dask_df_filtered = dask_df.dropna(subset=['swe_value'])\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    dask_df_filtered.to_csv(cleaned_csv_path, index=False, single_file=True)\n",
    "    print(\"dask_df_filtered.shape = \", dask_df_filtered.shape)\n",
    "    print(f\"The filtered csv with no swe values is saved to {cleaned_csv_path}\")\n",
    "\n",
    "clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b947383",
   "metadata": {},
   "source": [
    "### 5.3 Interpolation and Missing Data Handling\n",
    "\n",
    "With a clean dataset, we proceed to interpolate missing values for specified columns. \n",
    "\n",
    "- We perform in-place interpolation of missing values for specified columns, using a polynomial of a given degree. \n",
    "- We check for specific conditions (e.g., SWE values above a certain threshold) to decide on the interpolation strategy. It ensures that no null values remain post-interpolation, maintaining the dataset's completeness.\n",
    "\n",
    "- `x = df.index`: Represents the index of the DataFrame.\n",
    "- `y = df[column_name]`: Represents the values of the specified column (`column_name`) in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf8ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_inplace(df, column_name, degree=3):\n",
    "    x = df.index\n",
    "    y = df[column_name]\n",
    "\n",
    "    # Create a mask for missing values\n",
    "    if column_name == \"SWE\":\n",
    "      mask = (y > 240) | y.isnull()\n",
    "    elif column_name == \"fsca\":\n",
    "      mask = (y > 100) | y.isnull()\n",
    "    else:\n",
    "      mask = y.isnull()\n",
    "\n",
    "    # Check if all elements in the mask array are True\n",
    "    all_true = np.all(mask)\n",
    "\n",
    "    if all_true:\n",
    "      df[column_name] = 0\n",
    "    else:\n",
    "      # Perform interpolation\n",
    "      new_y = np.interp(x, x[~mask], y[~mask])\n",
    "      # Replace missing values with interpolated values\n",
    "      df[column_name] = new_y\n",
    "\n",
    "    if np.any(df[column_name].isnull()):\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55977c",
   "metadata": {},
   "source": [
    "### 5.4 Conversion to Time Series Format\n",
    "\n",
    "Next, we convert the dataset into a time series format.\n",
    "\n",
    "- We transform the dataset into a time series format, sorting data and filling missing values with interpolated data. \n",
    "\n",
    "- We then read the cleaned CSV, sorts the data, performs interpolation for missing values, and structures the dataset for time series analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c537d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All current columns:  Index(['date', 'lat', 'lon', 'AMSR_SWE', 'station_name', 'swe_value',\n",
      "       'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n",
      "       'air_temperature_tmmn', 'potential_evapotranspiration',\n",
      "       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n",
      "       'relative_humidity_rmin', 'precipitation_amount',\n",
      "       'air_temperature_tmmx', 'wind_speed', 'stationTriplet', 'elevation',\n",
      "       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n",
      "       'fSCA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns_to_be_time_series = [\"SWE\", \n",
    "                                 'air_temperature_tmmn',\n",
    "                                 'potential_evapotranspiration', \n",
    "                                 'mean_vapor_pressure_deficit',\n",
    "                                 'relative_humidity_rmax', \n",
    "                                 'relative_humidity_rmin',\n",
    "                                 'precipitation_amount', \n",
    "                                 'air_temperature_tmmx', \n",
    "                                 'wind_speed',\n",
    "                                 'fsca']\n",
    "\n",
    "# Read the cleaned ready CSV\n",
    "df = pd.read_csv(cleaned_csv_path)\n",
    "df.sort_values(by=['lat', 'lon', 'date'], inplace=True)\n",
    "print(\"All current columns: \", df.columns)\n",
    "    \n",
    "    # rename all columns to unified names\n",
    "    #     ['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n",
    "# 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs',\n",
    "# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n",
    "# 'fsca']\n",
    "df.rename(columns={'vpd': 'mean_vapor_pressure_deficit',\n",
    "                        'vs': 'wind_speed', \n",
    "                        'pr': 'precipitation_amount', \n",
    "                        'etr': 'potential_evapotranspiration',\n",
    "                        'tmmn': 'air_temperature_tmmn',\n",
    "                        'tmmx': 'air_temperature_tmmx',\n",
    "                        'rmin': 'relative_humidity_rmin',\n",
    "                        'rmax': 'relative_humidity_rmax',\n",
    "                        'AMSR_SWE': 'SWE',\n",
    "                        'fSCA': 'fsca'\n",
    "                    }, inplace=True)\n",
    "\n",
    "filled_csv = f\"{target_time_series_csv_path}_gap_filled.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be9ed1",
   "metadata": {},
   "source": [
    "## 5.5 Handling Missing Data and Interpolation\n",
    "\n",
    "Here we handle missing data within a dataset by using polynomial interpolation.\n",
    "\n",
    "- `process_group_filling_value(group)` is defined to handle the interpolation for each group of data, sorted by date. It goes through each relevant column (`columns_to_be_time_series`) and fills in the missing values using interpolation.\n",
    "- The data is then grouped by geographic coordinates (`lat`, `lon`) to apply the interpolation within each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce41a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_v1.csv_gap_filled.csv already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(filled_csv):\n",
    "        print(f\"{filled_csv} already exists, skipping\")\n",
    "        filled_data = pd.read_csv(filled_csv)\n",
    "else:\n",
    "    # Function to perform polynomial interpolation and fill in missing values\n",
    "    def process_group_filling_value(group):\n",
    "        # Sort the group by 'date'\n",
    "        group = group.sort_values(by='date')\n",
    "    \n",
    "        for column_name in columns_to_be_time_series:\n",
    "            group = interpolate_missing_inplace(group, column_name)\n",
    "        # Return the processed group\n",
    "        return group\n",
    "    # Group the data by 'lat' and 'lon' and apply interpolation for each column\n",
    "    print(\"Start to fill in the missing values\")\n",
    "    grouped = df.groupby(['lat', 'lon'])\n",
    "    filled_data = grouped.apply(process_group_filling_value).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    if any(filled_data['fsca'] > 100):\n",
    "        raise ValueError(\"Error: shouldn't have SWE>240 at this point\")\n",
    "\n",
    "    filled_data.to_csv(filled_csv, index=False)\n",
    "    \n",
    "    print(f\"New filled values csv is saved to {filled_csv}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ee7ee",
   "metadata": {},
   "source": [
    "## 5.6 Creating Time Series Data with Previous 7 Days' Values\n",
    "\n",
    "Here we generate a new CSV file containing time series data, where each location's data is augmented with columns representing the previous 7 days' values. If the file already exists, it skips the process.\n",
    "\n",
    "- `process_group_time_series` is to handle the creation of time series columns for each location group.\n",
    "- For each group, the data is sorted by `date`.\n",
    "- Then we iterate through the previous 7 days (`num_days = 7`), creating new columns for each target variable (`columns_to_be_time_series`), with the suffix `_day` representing how many days back the data is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd07d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_cleaned_nodata.csv_time_series_v1.csv already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(target_time_series_csv_path):\n",
    "        print(f\"{target_time_series_csv_path} already exists, skipping\")\n",
    "else:\n",
    "    df = filled_data\n",
    "    # Create a new DataFrame to store the time series data for each location\n",
    "    print(\"Start to create the training csv with previous 7 days columns\")\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # Define the number of days to consider (7 days in this case)\n",
    "    num_days = 7\n",
    "\n",
    "    grouped = df.groupby(['lat', 'lon'])\n",
    "    \n",
    "    def process_group_time_series(group, num_days):\n",
    "        group = group.sort_values(by='date')\n",
    "        for day in range(1, num_days + 1):\n",
    "            for target_col in columns_to_be_time_series:\n",
    "                new_column_name = f'{target_col}_{day}'\n",
    "                group[new_column_name] = group[target_col].shift(day)\n",
    "            \n",
    "        return group\n",
    "    \n",
    "    result = grouped.apply(lambda group: process_group_time_series(group, num_days)).reset_index(drop=True)\n",
    "    result.fillna(0, inplace=True)\n",
    "    \n",
    "    result.to_csv(target_time_series_csv_path, index=False)\n",
    "    print(f\"New data is saved to {target_time_series_csv_path}\")\n",
    "    shutil.copy(target_time_series_csv_path, backup_time_series_csv_path)\n",
    "    print(f\"File is backed up to {backup_time_series_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117fa9",
   "metadata": {},
   "source": [
    "## 5.7 Addition of Cumulative Columns\n",
    "\n",
    "Here we add cumulative columns for specific variables. We take a CSV file containing time series data and adds cumulative columns for specific variables. And then save the processed data to a new CSV file and generates plots for the cumulative variables.\n",
    "\n",
    "Here we calculate cumulative values for certain climate-related variables over time and visualize the cumulative trends for a specific location.\n",
    "\n",
    "- `input_csv` The path to the input CSV file containing the time series data.\n",
    "- `output_csv`: The path to the output CSV file where the cumulative data will be saved.\n",
    "- `force`: A boolean flag to force the operation (even though it is defined, it is not explicitly used in the code).\n",
    "- `Calculate Water Year`:\n",
    "   - A water year starts on October 1st and ends on September 30th of the next year.\n",
    "   - The function calculates the water year for each row and adds it as a new column.\n",
    "- The data is grouped by latitude (`lat`), longitude (`lon`), and water year.\n",
    "- For each variable listed in `columns_to_be_cumulated`, a cumulative column is added to the DataFrame.\n",
    "- The cumulative value is calculated using the `cumsum()` function within each group.\n",
    "\n",
    "\n",
    "### Example Variables\n",
    "- `SWE`: Snow Water Equivalent\n",
    "- `air_temperature_tmmn`: Minimum air temperature\n",
    "- `potential_evapotranspiration`: Potential evapotranspiration\n",
    "- `mean_vapor_pressure_deficit`: Mean vapor pressure deficit\n",
    "- `relative_humidity_rmax`: Maximum relative humidity\n",
    "- `relative_humidity_rmin`: Minimum relative humidity\n",
    "- `precipitation_amount`: Amount of precipitation\n",
    "- `air_temperature_tmmx`: Maximum air temperature\n",
    "- `wind_speed`: Wind speed\n",
    "- `fsca`: Fractional snow-covered area\n",
    "\n",
    "- The addition of cumulative columns to the dataset offers a dynamic perspective on environmental time series data. By aggregating measurements such as temperature, precipitation, and wind speed, the function reveals the incremental changes and long-term accumulation of these factors over specific periods – particularly water years. This comprehensive view is instrumental in understanding trends and patterns that are not immediately apparent in daily or monthly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def add_cumulative_columns(input_csv, output_csv, force=False):\n",
    "    \"\"\"\n",
    "    Add cumulative columns to the time series dataset.\n",
    "\n",
    "    This function reads the time series CSV file created by `convert_to_time_series`, calculates cumulative values for specific columns, and saves the data to a new CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_be_cumulated = [\n",
    "      \"SWE\",\n",
    "      'air_temperature_tmmn',\n",
    "      'potential_evapotranspiration', \n",
    "      'mean_vapor_pressure_deficit',\n",
    "      'relative_humidity_rmax', \n",
    "      'relative_humidity_rmin',\n",
    "      'precipitation_amount', \n",
    "      'air_temperature_tmmx', \n",
    "      'wind_speed',\n",
    "      'fsca'\n",
    "    ]\n",
    "\n",
    "    # Read the time series CSV (ensure it was created using `convert_to_time_series` function)\n",
    "    # directly read from original file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    print(\"The column statistics from time series before cumulative: \", df.describe()),\n",
    "   \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    unique_years = df['date'].dt.year.unique()\n",
    "    print(\"This is our unique years\", unique_years)\n",
    "    #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n",
    "    \n",
    "    # only start from the water year 10-01\n",
    "    # Filter rows based on the date range (2019 to 2022)\n",
    "    start_date = pd.to_datetime('2018-10-01')\n",
    "    end_date = pd.to_datetime('2021-09-30')\n",
    "    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    print(\"how many rows are left in the three water years?\", len(df.index))\n",
    "    df.to_csv(f\"{current_ready_csv_path}.test_check.csv\")\n",
    "\n",
    "    # Define a function to calculate the water year\n",
    "    def calculate_water_year(date):\n",
    "        year = date.year\n",
    "        if date.month >= 10:  # Water year starts in October\n",
    "            return year + 1\n",
    "        else:\n",
    "            return year\n",
    "    \n",
    "    # every water year starts at Oct 1, and ends at Sep 30. \n",
    "    df['water_year'] = df['date'].apply(calculate_water_year)\n",
    "    \n",
    "    # Group the DataFrame by 'lat' and 'lon'\n",
    "    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n",
    "    print(\"how many groups? \", len(grouped))\n",
    "    \n",
    "    # grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n",
    "    for column in columns_to_be_cumulated:\n",
    "        df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum(), include_groups=False)\n",
    "\n",
    "    print(\"This is the dataframe after cumulative columns are added\")\n",
    "    print(df.columns)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"All the cumulative variables are added successfully! {target_time_series_cumulative_csv_path}\")\n",
    "    print(\"double check the swe_value statistics:\", df[\"swe_value\"].describe())\n",
    "    cumulative_columns = [f'cumulative_{col}' for col in columns_to_be_cumulated]\n",
    "    # Number of rows and columns for subplots\n",
    "    n_rows = len(cumulative_columns) // 2\n",
    "    n_cols = 2\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))\n",
    "\n",
    "    # Flatten axes array if more than one row\n",
    "    if n_rows > 1:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    first_group_name = list(grouped.groups.keys())[0]\n",
    "\n",
    "    # Get the first group\n",
    "    first_group = grouped.get_group(first_group_name)\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print(first_group) \n",
    "\n",
    "    # Plot each cumulative column in a subplot\n",
    "    for i, col in enumerate(cumulative_columns):\n",
    "        axes[i].plot(first_group['date'], first_group[col], label=col)\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].set_xlabel('Date')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Adjust layout for readability\n",
    "    plt.suptitle(f'Cumulative values of features for location: {first_group_name}', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.88)  # Adjust the top of the subplot box to 88% of the figure height\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "add_cumulative_columns(target_time_series_csv_path, target_time_series_cumulative_csv_path, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e6926",
   "metadata": {},
   "source": [
    "![image](../img/timeseries/cumulative-timeseries-graphs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
