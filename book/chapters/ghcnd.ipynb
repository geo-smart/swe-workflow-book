{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 GHCNd: Global Historical Climatology Network Daily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurate Snow Water Equivalent (SWE) predictions rely heavily on reliable input data, including snow depth measurements. This section details the process of retrieving, processing, and refining snow depth data from the Global Historical Climatology Network Daily (GHCNd) dataset. The script discussed here automates the retrieval of snow depth data from ground stations, filters and cleans this data, and prepares it for use in SWE prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from io import StringIO\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "all_ghcd_station_file = '../Data/all_ghcn_station_list.csv'\n",
    "only_active_ghcd_station_in_west_conus_file = '../Data/active_station_only_list.csv'\n",
    "snowdepth_csv_file = '../Data/active_station_only_list.csv_all_vars.csv'\n",
    "mask_non_snow_days_ghcd_csv_file = '../Data/active_station_only_list.csv_all_vars_masked_non_snow.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "southwest_lon = -125.0\n",
    "southwest_lat = 25.0\n",
    "northeast_lon = -100.0\n",
    "northeast_lat = 49.0\n",
    "\n",
    "# the training period is three years from 2018 to 2021\n",
    "train_start_date = \"2018-01-03\"\n",
    "train_end_date = \"2021-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Overview\n",
    "\n",
    "The GHCNd dataset provides daily climate records from numerous weather stations worldwide. This script specifically targets snow depth data (SNWD) from active ground stations within a defined geographical area. The process involves several key steps:\n",
    "\n",
    "1. Downloading and Filtering Station Data: Identifying active stations within the specified region that report snow depth measurements.\n",
    "2. Retrieving Snow Depth Observations: Downloading and processing snow depth data for the specified training period.\n",
    "3. Masking Non-Snow Days: Refining the dataset to exclude non-snow days, focusing the data on relevant observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1.1 What data is being downloaded?\n",
    " - **Source:** The data is sourced from the Global Historical Climatology Network Daily (GHCNd), a comprehensive dataset managed by the National Centers for Environmental Information (NCEI). It provides historical daily climate data from thousands of stations worldwide.\n",
    " - **Features:** The key feature targeted by this script is Snow Depth (SNWD), which is the depth of snow on the ground at the time of observation, typically measured in millimeters.\n",
    " - **Station Metadata:** Additional metadata about each station includes the station ID, geographical coordinates (latitude and longitude), and the operational period of the station (start and end years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Downloading and Filtering Data\n",
    "\n",
    "The first step in this process is to download the list of all GHCNd stations and filter them based on geographical and operational criteria. This ensures that only active stations within the region of interest are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.1 Downloading and Converting Station Data\n",
    "\n",
    "The script begins by downloading the GHCNd station inventory file and converting it into a usable format. The relevant stations are then filtered based on their operational status (active in 2024) and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_convert_and_read():\n",
    "  \n",
    "    url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "    # Download the text file from the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: Failed to download the file.\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the text content into columns using regex\n",
    "    pattern = r\"(\\S+)\\s+\"\n",
    "    parsed_data = re.findall(pattern, response.text)\n",
    "    print(\"len(parsed_data) = \", len(parsed_data))\n",
    "    \n",
    "    # Create a new list to store the rows\n",
    "    rows = []\n",
    "    for i in range(0, len(parsed_data), 6):\n",
    "        rows.append(parsed_data[i:i+6])\n",
    "    \n",
    "    print(\"rows[0:20] = \", rows[0:20])\n",
    "    # Convert the rows into a CSV-like format\n",
    "    csv_data = \"\\n\".join([\",\".join(row) for row in rows])\n",
    "    \n",
    "    # Save the CSV-like string to a file\n",
    "    with open(all_ghcd_station_file, \"w\") as file:\n",
    "        file.write(csv_data)\n",
    "    \n",
    "    # Read the CSV-like data into a pandas DataFrame\n",
    "    column_names = ['Station', 'Latitude', 'Longitude', 'Variable', 'Year_Start', 'Year_End']\n",
    "    df = pd.read_csv(all_ghcd_station_file, header=None, names=column_names)\n",
    "    print(df.head())\n",
    "    \n",
    "    # Remove rows where the last column is not equal to \"2024\"\n",
    "    df = df[(df['Year_End'] == 2024) & (df['Variable']=='SNWD')]\n",
    "    print(\"Removed non-active stations: \", df.head())\n",
    "    \n",
    "    # Filter rows within the latitude and longitude ranges\n",
    "    df = df[\n",
    "      (df['Latitude'] >= southwest_lat) & (df['Latitude'] <= northeast_lat) &\n",
    "      (df['Longitude'] >= southwest_lon) & (df['Longitude'] <= northeast_lon)\n",
    "    ]\n",
    "    \n",
    "    df.to_csv(only_active_ghcd_station_in_west_conus_file, index=False)\n",
    "    print(f\"saved to {only_active_ghcd_station_in_west_conus_file}\")\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(parsed_data) =  4537908\n",
      "rows[0:20] =  [['ACW00011604', '17.1167', '-61.7833', 'TMAX', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'TMIN', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'PRCP', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'SNOW', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'SNWD', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'PGTM', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'WDFG', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'WSFG', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'WT03', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'WT08', '1949', '1949'], ['ACW00011604', '17.1167', '-61.7833', 'WT16', '1949', '1949'], ['ACW00011647', '17.1333', '-61.7833', 'TMAX', '1961', '1961'], ['ACW00011647', '17.1333', '-61.7833', 'TMIN', '1961', '1961'], ['ACW00011647', '17.1333', '-61.7833', 'PRCP', '1957', '1970'], ['ACW00011647', '17.1333', '-61.7833', 'SNOW', '1957', '1970'], ['ACW00011647', '17.1333', '-61.7833', 'SNWD', '1957', '1970'], ['ACW00011647', '17.1333', '-61.7833', 'WT03', '1961', '1961'], ['ACW00011647', '17.1333', '-61.7833', 'WT16', '1961', '1966'], ['AE000041196', '25.3330', '55.5170', 'TMAX', '1944', '2024'], ['AE000041196', '25.3330', '55.5170', 'TMIN', '1944', '2024']]\n",
      "       Station  Latitude  Longitude Variable  Year_Start  Year_End\n",
      "0  ACW00011604   17.1167   -61.7833     TMAX        1949      1949\n",
      "1  ACW00011604   17.1167   -61.7833     TMIN        1949      1949\n",
      "2  ACW00011604   17.1167   -61.7833     PRCP        1949      1949\n",
      "3  ACW00011604   17.1167   -61.7833     SNOW        1949      1949\n",
      "4  ACW00011604   17.1167   -61.7833     SNWD        1949      1949\n",
      "Removed non-active stations:           Station  Latitude  Longitude Variable  Year_Start  Year_End\n",
      "424  AJ000037575   41.5500    46.6670     SNWD        1973      2024\n",
      "446  AJ000037675   41.3670    48.5170     SNWD        1973      2024\n",
      "454  AJ000037735   40.7167    46.4167     SNWD        1973      2024\n",
      "475  AJ000037749   40.6500    47.7500     SNWD        1973      2024\n",
      "485  AJ000037756   40.5330    48.9330     SNWD        1973      2024\n",
      "saved to ../Data/active_station_only_list.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Year_Start</th>\n",
       "      <th>Year_End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70926</th>\n",
       "      <td>CA001011500</td>\n",
       "      <td>48.9333</td>\n",
       "      <td>-123.7500</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1991</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71000</th>\n",
       "      <td>CA001012055</td>\n",
       "      <td>48.8333</td>\n",
       "      <td>-124.0500</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1980</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71151</th>\n",
       "      <td>CA001015105</td>\n",
       "      <td>48.3667</td>\n",
       "      <td>-123.5667</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1980</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71190</th>\n",
       "      <td>CA001015628</td>\n",
       "      <td>48.8167</td>\n",
       "      <td>-123.7167</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1981</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71196</th>\n",
       "      <td>CA001015630</td>\n",
       "      <td>48.8167</td>\n",
       "      <td>-123.7167</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>2007</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750909</th>\n",
       "      <td>USW00094143</td>\n",
       "      <td>43.5317</td>\n",
       "      <td>-112.9422</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1954</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750992</th>\n",
       "      <td>USW00094178</td>\n",
       "      <td>42.4786</td>\n",
       "      <td>-114.4775</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1998</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751016</th>\n",
       "      <td>USW00094182</td>\n",
       "      <td>44.8942</td>\n",
       "      <td>-116.0997</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1998</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751035</th>\n",
       "      <td>USW00094185</td>\n",
       "      <td>43.5947</td>\n",
       "      <td>-118.9578</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1973</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751532</th>\n",
       "      <td>USW00094290</td>\n",
       "      <td>47.6872</td>\n",
       "      <td>-122.2553</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>1986</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4529 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Station  Latitude  Longitude Variable  Year_Start  Year_End\n",
       "70926   CA001011500   48.9333  -123.7500     SNWD        1991      2024\n",
       "71000   CA001012055   48.8333  -124.0500     SNWD        1980      2024\n",
       "71151   CA001015105   48.3667  -123.5667     SNWD        1980      2024\n",
       "71190   CA001015628   48.8167  -123.7167     SNWD        1981      2024\n",
       "71196   CA001015630   48.8167  -123.7167     SNWD        2007      2024\n",
       "...             ...       ...        ...      ...         ...       ...\n",
       "750909  USW00094143   43.5317  -112.9422     SNWD        1954      2024\n",
       "750992  USW00094178   42.4786  -114.4775     SNWD        1998      2024\n",
       "751016  USW00094182   44.8942  -116.0997     SNWD        1998      2024\n",
       "751035  USW00094185   43.5947  -118.9578     SNWD        1973      2024\n",
       "751532  USW00094290   47.6872  -122.2553     SNWD        1986      2024\n",
       "\n",
       "[4529 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_convert_and_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **URL Download:** The script begins by downloading the GHCNd station inventory file from the National Centers for Environmental Information (NCEI) website.\n",
    " - **Data Parsing and Formatting:** The raw text data is parsed using regular expressions and converted into a CSV format that can be easily processed by pandas.\n",
    " - **Filtering:** The script filters stations based on their operational status and geographical location, focusing on active stations within the defined latitude and longitude bounds.\n",
    " - **Saving Processed Data:** The filtered station data is saved to a CSV file, which will be used in the subsequent steps to retrieve snow depth observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.2 Filtering and Saving Active Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering, the script saves the list of relevant stations, which will be used in subsequent steps to retrieve snow depth data. This step is crucial for ensuring that the data retrieval process focuses only on the most relevant stations, minimizing unnecessary data processing.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "- **Geographical Filtering:** Ensures that only stations within the defined latitudinal and longitudinal bounds are included.\n",
    "- **Activity Status:** Filters out stations that are no longer active, ensuring the data is current and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Retrieving Snow Depth Observations\n",
    "\n",
    "With the relevant stations identified, the next step is to retrieve snow depth observations from the GHCNd dataset for the specified training period. This involves downloading snow depth data from each station and processing it to ensure it is suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.1 Processing Snow Depth Data\n",
    "\n",
    "The script uses Dask to parallelize the data retrieval process, efficiently downloading snow depth records from each station within the defined period. Dask is a powerful tool for handling large datasets, allowing for the concurrent execution of multiple data retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def get_snow_depth_observations_from_ghcn():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    new_base_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n",
    "    print(new_base_df.shape)\n",
    "    \n",
    "    start_date = train_start_date\n",
    "    end_date = train_end_date\n",
    "\t\n",
    "    # Create an empty Pandas DataFrame with the desired columns\n",
    "    result_df = pd.DataFrame(columns=[\n",
    "      'station_name', \n",
    "      'date', \n",
    "      'lat', \n",
    "      'lon', \n",
    "      'snow_depth',\n",
    "    ])\n",
    "    \n",
    "    train_start_date_obj = pd.to_datetime(train_start_date)\n",
    "    train_end_date_obj = pd.to_datetime(train_end_date)\n",
    "\n",
    "    # Function to process each station\n",
    "    @dask.delayed\n",
    "    def process_station(station):\n",
    "        station_name = station['Station']\n",
    "        # print(f\"retrieving for {station_name}\")\n",
    "        station_lat = station['Latitude']\n",
    "        station_long = station['Longitude']\n",
    "        try:\n",
    "          url = f\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station_name}.csv\"\n",
    "          response = requests.get(url)\n",
    "          df = pd.read_csv(StringIO(response.text))\n",
    "          #\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNOW\",\"SNOW_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"PGTM\",\"PGTM_ATTRIBUTES\",\"WDFG\",\"WDFG_ATTRIBUTES\",\"WSFG\",\"WSFG_ATTRIBUTES\",\"WT03\",\"WT03_ATTRIBUTES\",\"WT08\",\"WT08_ATTRIBUTES\",\"WT16\",\"WT16_ATTRIBUTES\"\n",
    "          columns_to_keep = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD']\n",
    "          df = df[columns_to_keep]\n",
    "          # Convert the date column to datetime objects\n",
    "          df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "          # Filter rows based on the training period\n",
    "          df = df[(df['DATE'] >= train_start_date_obj) & (df['DATE'] <= train_end_date_obj)]\n",
    "          # print(df.head())\n",
    "          return df\n",
    "        except Exception as e:\n",
    "          print(\"An error occurred:\", e)\n",
    "\n",
    "    # List of delayed computations for each station\n",
    "    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n",
    "\n",
    "    # Compute the delayed results\n",
    "    result_lists = dask.compute(*delayed_results)\n",
    "\n",
    "    # Concatenate the lists into a Pandas DataFrame\n",
    "    result_df = pd.concat(result_lists, ignore_index=True)\n",
    "\n",
    "    # Print the final Pandas DataFrame\n",
    "    print(result_df.head())\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    result_df.to_csv(snowdepth_csv_file, index=False)\n",
    "    print(f\"All the data are saved to {snowdepth_csv_file}\")\n",
    "#     result_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Stations <br /> \n",
    "![](../img/ghcnd/Stations.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4529, 6)\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "An error occurred: \"None of [Index(['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD'], dtype='object')] are in the [columns]\"\n",
      "       STATION       DATE  LATITUDE  LONGITUDE  SNWD\n",
      "0  CA001011500 2018-01-03   48.9333    -123.75   0.0\n",
      "1  CA001011500 2018-01-04   48.9333    -123.75   0.0\n",
      "2  CA001011500 2018-01-05   48.9333    -123.75   0.0\n",
      "3  CA001011500 2018-01-06   48.9333    -123.75   0.0\n",
      "4  CA001011500 2018-01-07   48.9333    -123.75   0.0\n",
      "All the data are saved to ../Data/active_station_only_list.csv_all_vars.csv\n"
     ]
    }
   ],
   "source": [
    "get_snow_depth_observations_from_ghcn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Station Processing with Dask:** Each station's data is processed using Dask's delayed function, allowing for parallel data retrieval. This significantly speeds up the process, especially when dealing with a large number of stations.\n",
    " - **Data Features:** \n",
    " The primary features extracted from each station's dataset are:\n",
    "    - STATION: The unique identifier for the weather station.\n",
    "    - DATE: The date of the observation.\n",
    "    - LATITUDE: The latitude of the station.\n",
    "    - LONGITUDE: The longitude of the station.\n",
    "    - SNWD: Snow depth in millimeters.\n",
    " - **Data Filtering:** For each station, the script retrieves snow depth data (SNWD) and filters it based on the specified training period. This ensures that only relevant data is included in the final dataset.\n",
    " - **Data Concatenation and Saving:** The retrieved data is concatenated into a single DataFrame and saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.2 Saving the Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final snow depth data, now filtered and cleaned, is saved to a CSV file. This file serves as the foundation for subsequent analyses and SWE prediction tasks. By automating the data retrieval and cleaning process, the script ensures that the dataset is ready for immediate use in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 Masking Non-Snow Days\n",
    "\n",
    "To refine the dataset further, the script masks out days with no snow depth observations. This step is important for focusing the analysis on days where snow depth measurements are meaningful, excluding days with zero or missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_all_non_zero_snowdepth_days():\n",
    "    \n",
    "    df = pd.read_csv(snowdepth_csv_file)\n",
    "    # Create the new column 'swe_value' and assign values based on conditions\n",
    "    df['swe_value'] = 0  # Initialize all values to 0\n",
    "\n",
    "    # Assign NaN to 'swe_value' where 'snow_depth' is non-zero\n",
    "    df.loc[df['SNWD'] != 0, 'swe_value'] = -999\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df.head())\n",
    "    df.to_csv(mask_non_snow_days_ghcd_csv_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATION        DATE  LATITUDE  LONGITUDE  SNWD  swe_value\n",
      "0  CA001011500  2018-01-03   48.9333    -123.75   0.0          0\n",
      "1  CA001011500  2018-01-04   48.9333    -123.75   0.0          0\n",
      "2  CA001011500  2018-01-05   48.9333    -123.75   0.0          0\n",
      "3  CA001011500  2018-01-06   48.9333    -123.75   0.0          0\n",
      "4  CA001011500  2018-01-07   48.9333    -123.75   0.0          0\n"
     ]
    }
   ],
   "source": [
    "mask_out_all_non_zero_snowdepth_days()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Masking:** The function creates a new column, swe_value, and assigns it a default value of 0. Where snow depth (SNWD) is non-zero, the swe_value is set to -999, effectively masking out these records.\n",
    "- **Refinement for Analysis:** This refinement step ensures that the dataset focuses on periods with significant snow depth, which are most relevant for SWE predictions. The masked data is then saved to a new CSV file, ready for use in further processing or analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
