{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab13cdf",
   "metadata": {},
   "source": [
    "## Utility Functions for Data Processing and Analysis Pipeline\n",
    "\n",
    "### 1. Library Imports and Setup for Snow Data\n",
    "\n",
    "- `KDTree`: From `scipy.spatial`, used for performing efficient nearest-neighbor searches in spatial datasets.\n",
    "- `plot_all_variables_in_one_csv`: A custom function from `convert_results_to_images`, used for visualizing processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d98378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.spatial import KDTree\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import warnings\n",
    "import sys\n",
    "# from convert_results_to_images import plot_all_variables_in_one_c\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = \"../data/gridmet_test_run\"\n",
    "test_start_date = \"2024-07-18\"\n",
    "western_us_coords = \"../data/dem_file.tif.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550b08d",
   "metadata": {},
   "source": [
    "### 2. Identifying Binary Files\n",
    "\n",
    "Here we determine whether a given file is a binary file or a text file.\n",
    "\n",
    "- We attempt to open the file in binary mode (`'rb'`) and read a chunk of bytes (1024 bytes).\n",
    "- And the we check for null bytes (`b'\\x00'`), which are common in binary files. If a null byte is found, then it is binary file.\n",
    "- Next, we check for a high percentage of non-printable ASCII characters by converting the byte chunk to characters and filtering out non-printable ones. If the chunk has no printable characters, the file is considered binary.\n",
    "- If neither of the above conditions are met, the function assumes the file is a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7004281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Read a chunk of bytes from the file\n",
    "            chunk = file.read(1024)\n",
    "\n",
    "            # Check for null bytes, a common indicator of binary data\n",
    "            if b'\\x00' in chunk:\n",
    "                return True\n",
    "\n",
    "            # Check for a high percentage of non-printable ASCII characters\n",
    "            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n",
    "            if not text_characters:\n",
    "                return True\n",
    "\n",
    "            # If none of the binary indicators are found, assume it's a text file\n",
    "            return False\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b77e80",
   "metadata": {},
   "source": [
    "# 3.6 Advanced Microwave Scanning Radiometer(AMSR)\n",
    "\n",
    "AMSR (Advanced Microwave Scanning Radiometer) is a satellite sensor that measures snow cover and water content by detecting Earth's microwave emissions.\n",
    "\n",
    "This chapter focuses on the use of AMSR satellite data to monitor `snow cover` and `snow water equivalent (SWE)`, key indicators in understanding water resources and climate patterns. \n",
    "\n",
    "This chapter demonstrates the practical applications of transforming AMSR data into actionable information for scientific and environmental purposes.\n",
    "\n",
    "\n",
    "## 3.6.1 Generating Daily Snow Data Links from AMSR\n",
    "\n",
    "Our goal is to generate a list of download links for AMSR daily snow data files for a specified date range.\n",
    "\n",
    "- `AMSR`: Satellite sensor providing daily snow data.\n",
    "- `start_year`, `end_year`: Define the date range for which data links are generated.\n",
    "- `base_url`: The starting point of each download link.\n",
    "- `timedelta`: Used to iterate over each day within the date range.\n",
    "- `generate_links`: Function that builds and returns the list of URLs.\n",
    "\n",
    "\n",
    "Here we create list of download links for `AMSR snow data` by iterating over a date range from the year 2019 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "612ce42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def generate_links(start_year, end_year):\n",
    "    '''\n",
    "    Generate a list of download links for AMSR daily snow data files.\n",
    "\n",
    "    Args:\n",
    "        start_year (int): The starting year.\n",
    "        end_year (int): The ending year (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of download links for AMSR daily snow data files.\n",
    "    '''\n",
    "    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n",
    "    url_date_format = \"%Y.%m.%d\"\n",
    "    file_date_format=\"%Y%m%d\"\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year + 1, 1, 1)\n",
    "\n",
    "    links = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        url_date_format1 = current_date.strftime(url_date_format)\n",
    "        file_date_format1= current_date.strftime(file_date_format)\n",
    "        link = base_url + url_date_format1 + \"/AMSR_U2_L3_DailySnow_B02_\" + file_date_format1 + \".he5\"\n",
    "        links.append(link)\n",
    "        current_date += delta\n",
    "\n",
    "    return links\n",
    "start_year = 2019\n",
    "end_year = 2022\n",
    "work_dir = \"../data/gridmet_test_run\"\n",
    "links = generate_links(start_year, end_year)\n",
    "save_location = f'{work_dir}/amsr'\n",
    "with open(f'{work_dir}/download_links.txt', \"w\") as txt_file:\n",
    "    for l in links:\n",
    "        txt_file.write(l + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268fe3d",
   "metadata": {},
   "source": [
    "## 3.6.2 Automated Script for Secure File Download with Wget from a List of URLs\n",
    "\n",
    "The follwing shell script is to automate the downloading of files by looping through the links we obtained from `3.6.1`.\n",
    "\n",
    "- `work_dir`: Specifies the working directory where the download links and files will be stored.\n",
    "- `input_file`: The text file containing the list of URLs to be downloaded.\n",
    "- `base_wget_command`: The core command used to download files with options for authentication, session management, and secure connections.\n",
    "- `output_directory`: The folder where the downloaded files will be saved.\n",
    "- Loop: Iterates over each URL in the input file, ensuring all files are downloaded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4a959",
   "metadata": {},
   "source": [
    "> **Note**: Before proceeding with the download, ensure you have logged in to the [Earthdata](https://urs.earthdata.nasa.gov) website. You will need to retrieve the session cookies to authenticate your `wget` requests.\n",
    "> \n",
    "> 1. Visit the [Earthdata Login](https://urs.earthdata.nasa.gov) and log in with your credentials.\n",
    "> 2. After logging in, use a browser extension like [EditThisCookie](https://chromewebstore.google.com/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg) to export the session cookies.\n",
    "> 3. If the cookies are exported in JSON format, convert them to the standard `wget` format. You can use tools like the [Cookie converter](http://www.linuxonly.nl/docs/60/159_Convert_cookies_txt_format.html) or manually extract the relevant cookies.\n",
    "> 4. Save the cookies in a file named `cookies.txt` in the appropriate format as required by `wget`.\n",
    "> \n",
    "> The cookies are crucial for authenticating your requests and ensuring successful downloads.\n",
    "> In the code snippet below, the output is shown for only 6 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb0849fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.01/AMSR_U2_L3_DailySnow_B02_20190101.he5\n",
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.02/AMSR_U2_L3_DailySnow_B02_20190102.he5\n",
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.03/AMSR_U2_L3_DailySnow_B02_20190103.he5\n",
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.04/AMSR_U2_L3_DailySnow_B02_20190104.he5\n",
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.05/AMSR_U2_L3_DailySnow_B02_20190105.he5\n",
      "Downloading: https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2019.01.06/AMSR_U2_L3_DailySnow_B02_20190106.he5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Specify the file containing the download links\n",
    "input_file=\"../data/download_links_updated.txt\"\n",
    "\n",
    "# Specify the cookies file location\n",
    "cookies_file=\"../data/cookies.txt\"\n",
    "\n",
    "# Ensure the cookies file exists (assumes it's been generated previously)\n",
    "if [ ! -f \"$cookies_file\" ]; then\n",
    "    echo \"Cookies file not found: $cookies_file\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Specify the base wget command with common options\n",
    "base_wget_command=\"wget --load-cookies $cookies_file --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate --progress=bar:force --quiet\"\n",
    "\n",
    "# Specify the output directory for downloaded files\n",
    "output_directory=\"../data/gridmet_test_run/amsr\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "mkdir -p \"$output_directory\"\n",
    "\n",
    "# Loop through each line (URL) in the input file and download it using wget\n",
    "while IFS= read -r url || [[ -n \"$url\" ]]; do\n",
    "    echo \"Downloading: $url\"\n",
    "    $base_wget_command -P \"$output_directory\" \"$url\"\n",
    "done < \"$input_file\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f953a4",
   "metadata": {},
   "source": [
    "## 3.6.3 Extracting Features from AMSR data:\n",
    "\n",
    "Once the AMSR data files are downloaded, the next step is to extract relevant features from these files. \n",
    "\n",
    "The following script accomplishes this task by processing each AMSR data file and extracting `snow water equivalent (SWE)` values for specific grid cells corresponding to SNOTEL weather stations. \n",
    "\n",
    "Lets breakdown each step involved in feature extraction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1f179",
   "metadata": {},
   "source": [
    "### 3.6.3.1 Finding the Closest Grid Cell Index for Given Latitude and Longitude\n",
    "\n",
    "- `target_latitude`, `target_longitude`: The coordinates of the specific location you want to match to a grid cell.\n",
    "- `lat_grid`, `lon_grid`: Arrays representing the grid of latitude and longitude values across a region.\n",
    "- `np.unravel_index`: It identifies the indices in the grid where the sum of latitude and longitude differences is minimized.\n",
    "\n",
    "Here we calculate the absolute differences between the target location and each point in the grid to find the closest match:\n",
    "- `lat_diff = np.abs(lat_grid - target_latitude)`\n",
    "- `lon_diff = np.abs(lon_grid - target_longitude)`\n",
    "\n",
    "\n",
    "Through this code snippet, we get the row index (`lat_idx`), column index (`lon_idx`), and the actual latitude and longitude of the closest grid cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fee15083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    '''\n",
    "    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "        target_latitude (float): The target latitude.\n",
    "        target_longitude (float): The target longitude.\n",
    "        lat_grid (numpy.ndarray): An array of latitude values.\n",
    "        lon_grid (numpy.ndarray): An array of longitude values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n",
    "    '''\n",
    "    # Compute the absolute differences between target and grid coordinates\n",
    "    lat_diff = np.abs(lat_grid - target_latitude)\n",
    "    lon_diff = np.abs(lon_grid - target_longitude)\n",
    "\n",
    "    # Find the indices corresponding to the minimum differences\n",
    "    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n",
    "\n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580125e7",
   "metadata": {},
   "source": [
    "### 3.6.3.2 Function to Map SNOTEL Stations to AMSR Grid Coordinates and Create a CSV Mapper\n",
    "\n",
    "Next we map SNOTEL station locations to the nearest AMSR grid cells and save this mapping as a CSV file.\n",
    "\n",
    "- `new_base_station_list_file`: This is a CSV file containing thethe latitude and longitude of various SNOTEL stations.\n",
    "- `target_csv_path`: The file path where the output CSV file will be saved.\n",
    "\n",
    "- `hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']`: Accesses the Northern Hemisphere data group within the HDF5 file.\n",
    "\n",
    "\n",
    "Here we read station data, and check if a mapping file already exists, and downloads the necessary AMSR data file if not already present by using `cmd = f\"curl --output {target_amsr_hdf_path} ...\"`.\n",
    "\n",
    "And for each SNOTEL station, we try to identify the closest AMSR grid cell using latitude and longitude comparisons. `df.to_csv(target_csv_path, index=False)`\n",
    "\n",
    "And we map SNOTEL stations to AMSR grid cells and save it in CSV file.\n",
    "\n",
    "This is useful for comparing ground-based measurements with satellite observations. By finding the closest grid point on the AMSR dataset to each SNOTEL station, scientists and researchers can analyze and compare the data more effectively. This helps in improving weather predictions, studying climate patterns, and better understanding environmental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "adce19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snotel_station_to_amsr_mapper(new_base_station_list_file, target_csv_path):\n",
    "    station_data = pd.read_csv(new_base_station_list_file)\n",
    "    \n",
    "    date = \"2023-01-01\"\n",
    "\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    \n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {os.path.basename(target_csv_path)} already exists, skipping..\")\n",
    "        df = pd.read_csv(target_csv_path)\n",
    "        return df\n",
    "    print('date is ',date)\n",
    "    target_amsr_hdf_path = f\"../data/gridmet_test_run/amsr_testing/testing_amsr_{date}.he5\"\n",
    "\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'station_lat', 'station_lon'])\n",
    "    # Read the HDF\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    for idx, row in station_data.iterrows():\n",
    "        target_lat = row['latitude']\n",
    "        target_lon = row['longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat,\n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97be1bd",
   "metadata": {},
   "source": [
    "### 3.6.3.3 Extracting and Saving AMSR Snow Data to CSV\n",
    "\n",
    "Next, we extract `snow water equivalent (SWE)` data from AMSR files for a range of dates, match it to specific locations (such as SNOTEL stations), and save the processed data into a CSV file. \n",
    "\n",
    "- `amsr_data_dir`: Directory containing the AMSR `.he5` files.\n",
    "\n",
    "\n",
    "Here we use a pre-generated mapping of SNOTEL stations to AMSR grid cells obtained from`3.6.3.3`  to extract relevant SWE values.\n",
    "\n",
    "- Parallel Processing with Dask: Dask is utilized to efficiently process large datasets in parallel, making the extraction and processing faster.\n",
    "\n",
    "- `dask_station_data = dd.from_pandas(mapper_df, npartitions=1)`: Converts the DataFrame into a Dask DataFrame for parallel processing.\n",
    "\n",
    "- `mapper_df = create_snotel_station_to_amsr_mapper(new_base_station_list_file, target_csv_path)`: Creates a mapping between SNOTEL stations and AMSR grid cells.\n",
    "\n",
    "- `swe = hem_group['Data Fields/SWE_NorthernDaily'][:]`: Extracts the Snow Water Equivalent (SWE) data from the AMSR HDF5 file.\n",
    "\n",
    "- `delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]`: Uses Dask's delayed function to process each row of the DataFrame in parallel.\n",
    "\n",
    "- `processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()`: Processes all the AMSR files in parallel, filtering out any None results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "93048e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, new_base_station_list_file, start_date, end_date):\n",
    "    if os.path.exists(output_csv_file):\n",
    "        os.remove(output_csv_file)\n",
    "    \n",
    "    target_csv_path = \"../data/gridmet_test_run/training_snotel_station_to_amsr_mapper.csv\"\n",
    "    mapper_df = create_snotel_station_to_amsr_mapper(new_base_station_list_file, \n",
    "                                         target_csv_path)\n",
    "        \n",
    "    # station_data = pd.read_csv(new_base_station_list_file)\n",
    "\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Create a Dask DataFrame\n",
    "    dask_station_data = dd.from_pandas(mapper_df, npartitions=1)\n",
    "\n",
    "    # Function to process each file\n",
    "    def process_file(filename):\n",
    "        file_path = os.path.join(amsr_data_dir, filename)\n",
    "        file = h5py.File(file_path, 'r')\n",
    "        hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "        date_str = filename.split('_')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y%m%d')\n",
    "        if not (start_date <= date <= end_date):\n",
    "            print(f\"{date} is not in the training period, skipping..\")\n",
    "            return None\n",
    "        new_date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "        flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "        # Create an empty Pandas DataFrame with the desired columns\n",
    "        result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE'])\n",
    "        print('empty df ',result_df)\n",
    "        # Sample loop to add rows to the Pandas DataFrame using dask.delayed\n",
    "        @delayed\n",
    "        def process_row(row, swe, new_date_str):\n",
    "          closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "          closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "          closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "          \n",
    "          return pd.DataFrame([[\n",
    "            new_date_str, \n",
    "            row['station_lat'],\n",
    "            row['station_lon'],\n",
    "            closest_swe]], \n",
    "            columns=result_df.columns\n",
    "          )\n",
    "\n",
    "\n",
    "        # List of delayed computations\n",
    "        delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]\n",
    "\n",
    "        # Compute the delayed results and concatenate them into a Pandas DataFrame\n",
    "        result_df = dask.compute(*delayed_results)\n",
    "        result_df = pd.concat(result_df, ignore_index=True)\n",
    "\n",
    "        # Print the final Pandas DataFrame\n",
    "        print(result_df)\n",
    "          \n",
    "        return result_df\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(amsr_data_dir) if f.endswith('.he5')]\n",
    "\n",
    "    # Create a Dask Bag from the files\n",
    "    dask_bag = db.from_sequence(files, npartitions=2)\n",
    "\n",
    "    # Process files in parallel\n",
    "    processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()\n",
    "    print(processed_data)\n",
    "\n",
    "    # Concatenate the processed data\n",
    "    combined_df = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Merged data saved to {os.path.basename(output_csv_file)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971336a0",
   "metadata": {},
   "source": [
    "### 3.6.3.4 Running the AMSR Data Extraction Process\n",
    "\n",
    "Here we extract and save AMSR snow data for a specified range of dates, linking it to SNOTEL stations, and storing the results in a CSV file.\n",
    "\n",
    "- `new_base_station_list_file`: CSV file containing a list of active SNOTEL stations in the western U.S.\n",
    "- Date Range: The start and end dates (`train_start_date` and `train_end_date`) define the period for which the data will be processed.\n",
    "- `extract_amsr_values_save_to_csv`: Function call discussed in `3.6.3.4`, it is to processes the AMSR data and saves the output to a CSV file.\n",
    "\n",
    "Here is the main entry point for a script that prepares and processes AMSR data, mapping it to specific training points that include SNOTEL and GHCND (Global Historical Climatology Network Daily) stations.\n",
    "\n",
    "By mapping SNOTEL and GHCND stations to the closest AMSR grid points and extracting relevant data over a specified time period, this script prepares a dataset that can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "977fb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   latitude  longitude  modis_x  modis_y\n",
      "0  39.95500 -120.53800      123      251\n",
      "1  42.95000 -112.83333      337      168\n",
      "2  36.23333 -106.43333      515      354\n",
      "3  36.23700 -106.42912      515      354\n",
      "4  44.45615 -113.30097      324      126\n",
      "File training_snotel_station_to_amsr_mapper.csv already exists, skipping..\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-24  39.95500 -120.53800       255\n",
      "1     2022-12-24  42.95000 -112.83333        25\n",
      "2     2022-12-24  36.23333 -106.43333         0\n",
      "3     2022-12-24  36.23700 -106.42912         0\n",
      "4     2022-12-24  44.45615 -113.30097        18\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-24  48.40890 -106.51440        20\n",
      "8057  2022-12-24  48.54250 -109.76440        15\n",
      "8058  2022-12-24  45.54940 -100.40860        40\n",
      "8059  2022-12-24  43.53170 -112.94220        30\n",
      "8060  2022-12-24  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-30  39.95500 -120.53800         0\n",
      "1     2022-12-30  42.95000 -112.83333        70\n",
      "2     2022-12-30  36.23333 -106.43333         0\n",
      "3     2022-12-30  36.23700 -106.42912         0\n",
      "4     2022-12-30  44.45615 -113.30097        44\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-30  48.40890 -106.51440        34\n",
      "8057  2022-12-30  48.54250 -109.76440        56\n",
      "8058  2022-12-30  45.54940 -100.40860        38\n",
      "8059  2022-12-30  43.53170 -112.94220        55\n",
      "8060  2022-12-30  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-31  39.95500 -120.53800       255\n",
      "1     2022-12-31  42.95000 -112.83333         0\n",
      "2     2022-12-31  36.23333 -106.43333         0\n",
      "3     2022-12-31  36.23700 -106.42912         0\n",
      "4     2022-12-31  44.45615 -113.30097        28\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-31  48.40890 -106.51440        30\n",
      "8057  2022-12-31  48.54250 -109.76440        60\n",
      "8058  2022-12-31  45.54940 -100.40860        39\n",
      "8059  2022-12-31  43.53170 -112.94220        39\n",
      "8060  2022-12-31  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-25  39.95500 -120.53800         0\n",
      "1     2022-12-25  42.95000 -112.83333        29\n",
      "2     2022-12-25  36.23333 -106.43333       255\n",
      "3     2022-12-25  36.23700 -106.42912       255\n",
      "4     2022-12-25  44.45615 -113.30097        27\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-25  48.40890 -106.51440        18\n",
      "8057  2022-12-25  48.54250 -109.76440         0\n",
      "8058  2022-12-25  45.54940 -100.40860        33\n",
      "8059  2022-12-25  43.53170 -112.94220        27\n",
      "8060  2022-12-25  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-28  39.95500 -120.53800         0\n",
      "1     2022-12-28  42.95000 -112.83333       255\n",
      "2     2022-12-28  36.23333 -106.43333         0\n",
      "3     2022-12-28  36.23700 -106.42912         0\n",
      "4     2022-12-28  44.45615 -113.30097       255\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-28  48.40890 -106.51440        24\n",
      "8057  2022-12-28  48.54250 -109.76440         0\n",
      "8058  2022-12-28  45.54940 -100.40860        25\n",
      "8059  2022-12-28  43.53170 -112.94220       255\n",
      "8060  2022-12-28  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-29  39.95500 -120.53800        32\n",
      "1     2022-12-29  42.95000 -112.83333        55\n",
      "2     2022-12-29  36.23333 -106.43333         0\n",
      "3     2022-12-29  36.23700 -106.42912         0\n",
      "4     2022-12-29  44.45615 -113.30097        31\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-29  48.40890 -106.51440        28\n",
      "8057  2022-12-29  48.54250 -109.76440        39\n",
      "8058  2022-12-29  45.54940 -100.40860       255\n",
      "8059  2022-12-29  43.53170 -112.94220        59\n",
      "8060  2022-12-29  47.68720 -122.25530       255\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-27  39.95500 -120.53800         0\n",
      "1     2022-12-27  42.95000 -112.83333        19\n",
      "2     2022-12-27  36.23333 -106.43333       255\n",
      "3     2022-12-27  36.23700 -106.42912       255\n",
      "4     2022-12-27  44.45615 -113.30097        23\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-27  48.40890 -106.51440        17\n",
      "8057  2022-12-27  48.54250 -109.76440         0\n",
      "8058  2022-12-27  45.54940 -100.40860        19\n",
      "8059  2022-12-27  43.53170 -112.94220        30\n",
      "8060  2022-12-27  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "empty df  Empty DataFrame\n",
      "Columns: [date, lat, lon, AMSR_SWE]\n",
      "Index: []\n",
      "            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-26  39.95500 -120.53800         0\n",
      "1     2022-12-26  42.95000 -112.83333         0\n",
      "2     2022-12-26  36.23333 -106.43333         0\n",
      "3     2022-12-26  36.23700 -106.42912         0\n",
      "4     2022-12-26  44.45615 -113.30097        22\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-26  48.40890 -106.51440        23\n",
      "8057  2022-12-26  48.54250 -109.76440        31\n",
      "8058  2022-12-26  45.54940 -100.40860        37\n",
      "8059  2022-12-26  43.53170 -112.94220        29\n",
      "8060  2022-12-26  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]\n",
      "[            date       lat        lon  AMSR_SWE\n",
      "0     2022-12-28  39.95500 -120.53800         0\n",
      "1     2022-12-28  42.95000 -112.83333       255\n",
      "2     2022-12-28  36.23333 -106.43333         0\n",
      "3     2022-12-28  36.23700 -106.42912         0\n",
      "4     2022-12-28  44.45615 -113.30097       255\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-28  48.40890 -106.51440        24\n",
      "8057  2022-12-28  48.54250 -109.76440         0\n",
      "8058  2022-12-28  45.54940 -100.40860        25\n",
      "8059  2022-12-28  43.53170 -112.94220       255\n",
      "8060  2022-12-28  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-29  39.95500 -120.53800        32\n",
      "1     2022-12-29  42.95000 -112.83333        55\n",
      "2     2022-12-29  36.23333 -106.43333         0\n",
      "3     2022-12-29  36.23700 -106.42912         0\n",
      "4     2022-12-29  44.45615 -113.30097        31\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-29  48.40890 -106.51440        28\n",
      "8057  2022-12-29  48.54250 -109.76440        39\n",
      "8058  2022-12-29  45.54940 -100.40860       255\n",
      "8059  2022-12-29  43.53170 -112.94220        59\n",
      "8060  2022-12-29  47.68720 -122.25530       255\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-27  39.95500 -120.53800         0\n",
      "1     2022-12-27  42.95000 -112.83333        19\n",
      "2     2022-12-27  36.23333 -106.43333       255\n",
      "3     2022-12-27  36.23700 -106.42912       255\n",
      "4     2022-12-27  44.45615 -113.30097        23\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-27  48.40890 -106.51440        17\n",
      "8057  2022-12-27  48.54250 -109.76440         0\n",
      "8058  2022-12-27  45.54940 -100.40860        19\n",
      "8059  2022-12-27  43.53170 -112.94220        30\n",
      "8060  2022-12-27  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-26  39.95500 -120.53800         0\n",
      "1     2022-12-26  42.95000 -112.83333         0\n",
      "2     2022-12-26  36.23333 -106.43333         0\n",
      "3     2022-12-26  36.23700 -106.42912         0\n",
      "4     2022-12-26  44.45615 -113.30097        22\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-26  48.40890 -106.51440        23\n",
      "8057  2022-12-26  48.54250 -109.76440        31\n",
      "8058  2022-12-26  45.54940 -100.40860        37\n",
      "8059  2022-12-26  43.53170 -112.94220        29\n",
      "8060  2022-12-26  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-24  39.95500 -120.53800       255\n",
      "1     2022-12-24  42.95000 -112.83333        25\n",
      "2     2022-12-24  36.23333 -106.43333         0\n",
      "3     2022-12-24  36.23700 -106.42912         0\n",
      "4     2022-12-24  44.45615 -113.30097        18\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-24  48.40890 -106.51440        20\n",
      "8057  2022-12-24  48.54250 -109.76440        15\n",
      "8058  2022-12-24  45.54940 -100.40860        40\n",
      "8059  2022-12-24  43.53170 -112.94220        30\n",
      "8060  2022-12-24  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-30  39.95500 -120.53800         0\n",
      "1     2022-12-30  42.95000 -112.83333        70\n",
      "2     2022-12-30  36.23333 -106.43333         0\n",
      "3     2022-12-30  36.23700 -106.42912         0\n",
      "4     2022-12-30  44.45615 -113.30097        44\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-30  48.40890 -106.51440        34\n",
      "8057  2022-12-30  48.54250 -109.76440        56\n",
      "8058  2022-12-30  45.54940 -100.40860        38\n",
      "8059  2022-12-30  43.53170 -112.94220        55\n",
      "8060  2022-12-30  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-31  39.95500 -120.53800       255\n",
      "1     2022-12-31  42.95000 -112.83333         0\n",
      "2     2022-12-31  36.23333 -106.43333         0\n",
      "3     2022-12-31  36.23700 -106.42912         0\n",
      "4     2022-12-31  44.45615 -113.30097        28\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-31  48.40890 -106.51440        30\n",
      "8057  2022-12-31  48.54250 -109.76440        60\n",
      "8058  2022-12-31  45.54940 -100.40860        39\n",
      "8059  2022-12-31  43.53170 -112.94220        39\n",
      "8060  2022-12-31  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns],             date       lat        lon  AMSR_SWE\n",
      "0     2022-12-25  39.95500 -120.53800         0\n",
      "1     2022-12-25  42.95000 -112.83333        29\n",
      "2     2022-12-25  36.23333 -106.43333       255\n",
      "3     2022-12-25  36.23700 -106.42912       255\n",
      "4     2022-12-25  44.45615 -113.30097        27\n",
      "...          ...       ...        ...       ...\n",
      "8056  2022-12-25  48.40890 -106.51440        18\n",
      "8057  2022-12-25  48.54250 -109.76440         0\n",
      "8058  2022-12-25  45.54940 -100.40860        33\n",
      "8059  2022-12-25  43.53170 -112.94220        27\n",
      "8060  2022-12-25  47.68720 -122.25530         0\n",
      "\n",
      "[8061 rows x 4 columns]]\n",
      "Merged data saved to all_training_points_snotel_ghcnd_in_westus.csv_amsr_dask_all_training_ponits_with_ghcnd.csv\n"
     ]
    }
   ],
   "source": [
    "amsr_data_dir = \"../data/gridmet_test_run/amsr\"\n",
    "\n",
    "all_training_points_with_snotel_ghcnd_file = \"../data/gridmet_test_run/all_training_points_snotel_ghcnd_in_westus.csv\"\n",
    "new_base_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n",
    "print(new_base_df.head())\n",
    "output_csv_file = f\"{all_training_points_with_snotel_ghcnd_file}_amsr_dask_all_training_ponits_with_ghcnd.csv\"\n",
    "\n",
    "start_date = train_start_date\n",
    "end_date = train_end_date\n",
    "\n",
    "extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, all_training_points_with_snotel_ghcnd_file, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed4b18",
   "metadata": {},
   "source": [
    "## 3.6.4 AMSR Data Processing and Analysis Pipeline\n",
    "In this section we see, how we automate the process of downloading, mapping, processing, and analyzing AMSR (Advanced Microwave Scanning Radiometer) data. \n",
    "\n",
    "It provides a streamlined and automated pipeline for handling AMSR data, from initial download and grid alignment to final data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c18cd",
   "metadata": {},
   "source": [
    "### 3.6.4.1 Find Closest Point in a Grid\n",
    "\n",
    "- `target_latitude` (float): The latitude of the target point.\n",
    "- `target_longitude` (float): The longitude of the target point.\n",
    "- `lat_grid` (numpy array): A 2D array representing the grid of latitudes.\n",
    "- `lon_grid` (numpy array): A 2D array representing the grid of longitudes.\n",
    "- Here we calculate the squared Euclidean distance between the target point and each point in the grid.\n",
    "- And then find the grid point with the minimum distance to the target point and return the indices of that point along with its actual latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f7943fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    # Calculate the squared Euclidean distance between the target point and all grid points\n",
    "    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n",
    "    \n",
    "    # Find the indices of the minimum distance\n",
    "    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n",
    "    \n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d58f9e",
   "metadata": {},
   "source": [
    "### 3.6.4.2 Find the closest grid point indices for a target latitude and longitude using KDTree\n",
    "Here we find the closest grid point indices to a given target latitude and longitude using a KDTree for efficient spatial searching.\n",
    "\n",
    "- `lat_idx` (int): The index of the closest latitude in the grid.\n",
    "- `lon_idx` (int): The index of the closest longitude in the grid.\n",
    "- `lat_grid[lat_idx, lon_idx]` (float): The actual latitude of the closest grid point.\n",
    "- `lon_grid[lat_idx, lon_idx]` (float): The actual longitude of the closest grid point.\n",
    "\n",
    "- We use a global KDTree (`latlontree`) to efficiently search for the nearest grid point. If the KDTree is not already built, it constructs one using the latitude and longitude grids.\n",
    "- Before constructing the KDTree, we replace any `NaN` values in the latitude and longitude grids with `0.0` to ensure the tree is built without issues.\n",
    "- The KDTree is then queried with the target latitude and longitude to find the nearest point in the grid.\n",
    "- Then we convert the 1D index returned by the KDTree back into 2D grid indices to identify the exact location in the original latitude and longitude grids.\n",
    "- Finally, we return the indices of the closest latitude and longitude, along with the corresponding values from the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ce9f3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    \"\"\"\n",
    "    Find the closest grid point indices for a target latitude and longitude using KDTree.\n",
    "\n",
    "    Parameters:\n",
    "        target_latitude (float): Target latitude.\n",
    "        target_longitude (float): Target longitude.\n",
    "        lat_grid (numpy.ndarray): Array of latitude values.\n",
    "        lon_grid (numpy.ndarray): Array of longitude values.\n",
    "\n",
    "    Returns:\n",
    "        int: Latitude index.\n",
    "        int: Longitude index.\n",
    "        float: Closest latitude value.\n",
    "        float: Closest longitude value.\n",
    "    \"\"\"\n",
    "    global latlontree\n",
    "    \n",
    "    if latlontree is None:\n",
    "        # Create a KD-Tree from lat_grid and lon_grid\n",
    "        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n",
    "        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n",
    "        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n",
    "      \n",
    "    # Query the KD-Tree to find the nearest point\n",
    "    distance, index = latlontree.query([target_latitude, target_longitude])\n",
    "\n",
    "    # Convert the 1D index to 2D grid indices\n",
    "    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n",
    "\n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502abe49",
   "metadata": {},
   "source": [
    "### 3.6.4.3 Find the closest grid point indices for a target latitude and longitude.\n",
    "\n",
    "Here we find the grid point in a latitude-longitude array that is closest to a given target latitude and longitude.\n",
    "\n",
    "- Here we compute the absolute difference between the target latitude and longitude and all points in the `lat_grid` and `lon_grid`.\n",
    "- And then sum these differences to create a simple distance metric for each grid point.\n",
    "- Next, identify the grid point with the smallest sum of differences, which is considered the closest point to the target location.\n",
    "- Finally, return the indices and the actual latitude and longitude values of this closest grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "bdf31464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n",
    "    \"\"\"\n",
    "    Find the closest grid point indices for a target latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "        target_latitude (float): Target latitude.\n",
    "        target_longitude (float): Target longitude.\n",
    "        lat_grid (numpy.ndarray): Array of latitude values.\n",
    "        lon_grid (numpy.ndarray): Array of longitude values.\n",
    "\n",
    "    Returns:\n",
    "        int: Latitude index.\n",
    "        int: Longitude index.\n",
    "        float: Closest latitude value.\n",
    "        float: Closest longitude value.\n",
    "    \"\"\"\n",
    "    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n",
    "    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n",
    "\n",
    "    # Find the indices corresponding to the minimum differences\n",
    "    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n",
    "\n",
    "    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ff44d",
   "metadata": {},
   "source": [
    "### 3.6.4.4 Preparing the AMSR to GridMET Mapper\n",
    "\n",
    "The goal here is to create a mapping between AMSR grid data and GridMET grid points, saving the results to a CSV file. \n",
    "\n",
    "- `target_csv_path`: The file path where the mapping between AMSR and GridMET grid points will be saved as a CSV file.\n",
    "- `target_amsr_hdf_path`: The path where the AMSR data file is stored or will be downloaded to if it doesn’t exist.- `western_us_coords`: A CSV file containing the latitude and longitude of GridMET grid points for the western U.S.\n",
    "\n",
    "- `file = h5py.File(target_amsr_hdf_path, 'r')`: Opens the AMSR HDF5 file for reading, allowing access to its contents.\n",
    "\n",
    "- `lat = np.nan_to_num(lat, nan=0.0)`: Replaces any `NaN` values in the latitude data with zeros to avoid errors during processing.\n",
    "\n",
    "- `closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)`: Finds the closest AMSR grid point to each target GridMET point, which is crucial for accurate data mapping.\n",
    "\n",
    "This code is essential for linking the satellite-based AMSR data grid with the ground-based GridMET grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "fee00f36",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_amsr_grid_mapper():\n",
    "    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n",
    "                               'amsr_lat_idx', 'amsr_lon_idx',\n",
    "                               'gridmet_lat', 'gridmet_lon'])\n",
    "    date = test_start_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    \n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = \"../data/amsr_to_gridmet_mapper.csv\"\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {os.path.basename(target_csv_path)} already exists, skipping..\")\n",
    "        return\n",
    "    target_amsr_hdf_path = f\"../data/gridmet_test_run/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path):\n",
    "        print(f\"File {os.path.basename(target_amsr_hdf_path)} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ../data/cookies.txt -c ~/.mycookies.txt -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        subprocess.run(cmd, shell=True)\n",
    "    \n",
    "    # Read the HDF\n",
    "    print('target_amsr_hdf_path', target_amsr_hdf_path)\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    # Set the number of rows to process\n",
    "    num_rows_to_process = 100  # Change this value to the desired number of rows\n",
    "    # Convert the AMSR grid into our gridMET 1km grid\n",
    "    western_us_df = pd.read_csv(western_us_coords)\n",
    "    for idx, row in western_us_df.iterrows():\n",
    "        if idx >= num_rows_to_process:\n",
    "            break\n",
    "\n",
    "        target_lat = row['Latitude']\n",
    "        target_lon = row['Longitude']\n",
    "        \n",
    "        # compare the performance and find the fastest way to search nearest point\n",
    "        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n",
    "        df.loc[len(df.index)] = [closest_lat, \n",
    "                                 closest_lon,\n",
    "                                 closest_lat_idx,\n",
    "                                 closest_lon_idx,\n",
    "                                 target_lat, \n",
    "                                 target_lon]\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    df.to_csv(target_csv_path, index=False)\n",
    "  \n",
    "    print('AMSR mapper csv is created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65dccd",
   "metadata": {},
   "source": [
    "### 3.6.4.5 Downloading and Converting AMSR Snow Data to DEM Format\n",
    "\n",
    "Here we automate the downloading, conversion, and saving of AMSR data aligned with a DEM grid.\n",
    "And also adds a cumulative sum column to a DataFrame, useful for tracking cumulative metrics over time.\n",
    "\n",
    "- `target_date`: The specific date for which AMSR data is being processed, initially set to `test_start_date`.\n",
    "- `target_mapper_csv_path`: The path to the CSV file that maps AMSR grid points to GridMET grid points.\n",
    "- `target_csv_path`: The file path where the final converted data will be saved as a CSV.\n",
    "- `target_amsr_hdf_path`: The path where the downloaded AMSR HDF5 file will be stored.\n",
    "\n",
    "- `mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)`: Applies the `get_swe` function to each row in the mapping DataFrame to calculate the SWE value for each GridMET point.\n",
    "\n",
    "- `mapper_df.drop(columns=['amsr_lat', 'amsr_lon', ...])`: Removes unnecessary columns from the DataFrame before saving the final results.\n",
    "\n",
    "Here we first constructs the URL for the AMSR data corresponding to the specified date and attempts to download it using the curl command. It ensures that the necessary cookies are available for authentication.\n",
    "\n",
    "And once the data is downloaded, we read the HDF5 file using the h5py library, extracting the latitude, longitude, snow water equivalent (SWE), and flag information from the file.\n",
    "\n",
    "And then we convert the AMSR grid into a format compatible with the DEM grid by finding the corresponding grid points in the DEM grid. This involves identifying the nearest DEM grid points for each AMSR grid point.\n",
    "\n",
    "For each DEM grid point, we perform a custom calculation to determine the SWE and flag values based on the nearest AMSR grid points.\n",
    "\n",
    "And finally we save the converted data, including the latitude, longitude, SWE, and flag information, into a CSV file. This file can be further processed or analyzed in subsequent steps of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1f80846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_amsr_and_convert_grid(target_date = test_start_date):\n",
    "    \"\"\"\n",
    "    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n",
    "    \"\"\"\n",
    "    # the mapper\n",
    "    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n",
    "    mapper_df = pd.read_csv(target_mapper_csv_path)\n",
    "    #print(mapper_df.head())\n",
    "    \n",
    "    df = pd.DataFrame(columns=['date', 'lat', \n",
    "                               'lon', 'AMSR_SWE', \n",
    "                               'AMSR_Flag'])\n",
    "    date = target_date\n",
    "    date = date.replace(\"-\", \".\")\n",
    "    he5_date = date.replace(\".\", \"\")\n",
    "    target_date = \"\"\n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f\"../data/gridmet_test_run/amsr_testing/testing_ready_amsr_{date}.csv\"\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {os.path.basename(target_csv_path)} already exists, skipping..\")\n",
    "        return target_csv_path\n",
    "    \n",
    "    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n",
    "    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n",
    "        print(f\"File {os.path.basename(target_amsr_hdf_path)} already exists, skip downloading..\")\n",
    "    else:\n",
    "        cmd = f\"curl --output {target_amsr_hdf_path} -b ../data/cookies.txt -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n",
    "        print(f'Running command: {cmd}')\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        # Check the exit code\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Command failed with exit code {result.returncode}.\")\n",
    "            if os.path.exists(target_amsr_hdf_path):\n",
    "              os.remove(target_amsr_hdf_path)\n",
    "              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n",
    "            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n",
    "    \n",
    "    # Read the HDF\n",
    "    print('file is ',target_amsr_hdf_path)\n",
    "    print(f\"Reading {target_amsr_hdf_path}\")\n",
    "    file = h5py.File(target_amsr_hdf_path, 'r')\n",
    "    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n",
    "    lat = hem_group['lat'][:]\n",
    "    lon = hem_group['lon'][:]\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    lat = np.nan_to_num(lat, nan=0.0)\n",
    "    lon = np.nan_to_num(lon, nan=0.0)\n",
    "    \n",
    "    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n",
    "    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n",
    "    date = datetime.strptime(date, '%Y.%m.%d')\n",
    "    \n",
    "    # Convert the AMSR grid into our DEM 1km grid\n",
    "    \n",
    "    def get_swe(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_swe\n",
    "    \n",
    "    def get_swe_flag(row):\n",
    "        # Perform your custom calculation here\n",
    "        closest_lat_idx = int(row['amsr_lat_idx'])\n",
    "        closest_lon_idx = int(row['amsr_lon_idx'])\n",
    "        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n",
    "        return closest_flag\n",
    "    \n",
    "    # Use the apply function to apply the custom function to each row\n",
    "    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n",
    "    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n",
    "    mapper_df['date'] = date\n",
    "    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n",
    "    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n",
    "    mapper_df = mapper_df.drop(columns=['amsr_lat',\n",
    "                                        'amsr_lon',\n",
    "                                        'amsr_lat_idx',\n",
    "                                        'amsr_lon_idx'])\n",
    "    \n",
    "    print(\"result df: \", mapper_df.head())\n",
    "    # Save the new converted AMSR to CSV file\n",
    "    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n",
    "    mapper_df.to_csv(target_csv_path, index=False)\n",
    "    \n",
    "    print('Completed AMSR testing data collection.')\n",
    "    return target_csv_path\n",
    "\n",
    "def add_cumulative_column(df, column_name):\n",
    "    df[f'cumulative_{column_name}'] = df[column_name].sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c56467",
   "metadata": {},
   "source": [
    "### 3.6.4.6 Aggregate Cumulative AMSR Snow Data and Export to CSV\n",
    "\n",
    "The goal of this code is to calculate the `cumulative Snow Water Equivalent (SWE)` values from AMSR data over a specific period, filling any gaps in the data, and saving the cumulative results into a CSV file. This is particularly useful for analyzing long-term snow accumulation trends.\n",
    "\n",
    "- `target_date`: The specific date for which cumulative AMSR data is calculated.\n",
    "- `past_october_1`: The date of October 1st in the previous or current year, serving as the start of the  period.\n",
    "- `target_csv_path`: The path where the cumulative AMSR data will be saved.\n",
    "- `gap_filled_csv`: The path where the gap-filled version of the cumulative data will be saved.\n",
    "\n",
    "- `past_october_1 = datetime(selected_date.year - 1, 10, 1)`: Sets the start date for cumulative calculations to October 1st of the previous year if the target date is before October, otherwise, it uses October 1st of the current year.\n",
    "\n",
    "- `data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)`: Downloads and converts AMSR data for each day from October 1st to the target date, storing the results in a dictionary.\n",
    "\n",
    "- `filtered_columns.interpolate(axis=1, method='linear', inplace=True)`: Fills in missing values in the SWE data using linear interpolation across time.\n",
    "\n",
    "- `df[f'cumulative_{column_name}'] = sum_column`: Adds a new column to the DataFrame containing the cumulative sum of SWE values.\n",
    "\n",
    "- `filled_data.to_csv(gap_filled_csv, index=False)`: Saves the gap-filled cumulative data to a CSV file.\n",
    "\n",
    "- `result.to_csv(target_csv_path, index=False)`: Saves the final cumulative data for the target date into a CSV file.\n",
    "\n",
    "This cumulative information is valuable for understanding seasonal snow accumulation and can be used in various environmental analyses and forecasting models. The approach of handling missing data ensures the integrity and completeness of the dataset before it’s saved and used for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "2f841d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def get_cumulative_amsr_data(target_date = test_start_date, force=False):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "      past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Traverse and print every day from past October 1 to the specific date\n",
    "    current_date = past_october_1\n",
    "    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n",
    "\n",
    "    columns_to_be_cumulated = [\"AMSR_SWE\"]\n",
    "    \n",
    "    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n",
    "    if os.path.exists(gap_filled_csv) and not force:\n",
    "      print(f\"{gap_filled_csv} already exists, skipping..\")\n",
    "      df = pd.read_csv(gap_filled_csv)\n",
    "      print(df[\"AMSR_SWE\"].describe())\n",
    "    else:\n",
    "      date_keyed_objects = {}\n",
    "      data_dict = {}\n",
    "      new_df = None\n",
    "      while current_date <= selected_date:\n",
    "        print(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n",
    "        current_df = pd.read_csv(data_dict[current_date_str])\n",
    "        current_df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "        if current_date != selected_date:\n",
    "          current_df.rename(columns={\n",
    "            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n",
    "            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n",
    "          }, inplace=True)\n",
    "        #print(current_df.head())\n",
    "\n",
    "        if new_df is None:\n",
    "          new_df = current_df\n",
    "        else:\n",
    "          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n",
    "          #new_df = new_df.append(current_df, ignore_index=True)\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "      print(\"new_df.columns = \", new_df.columns)\n",
    "      print(\"new_df.head = \", new_df.head())\n",
    "      df = new_df\n",
    "\n",
    "      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n",
    "      print(\"All current head: \", df.head())\n",
    "      print(\"the new_df.shape: \", df.shape)\n",
    "\n",
    "      print(\"Start to fill in the missing values\")\n",
    "      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n",
    "      filled_data = pd.DataFrame()\n",
    "\n",
    "      # Apply the function to each group\n",
    "      for column_name in columns_to_be_cumulated:\n",
    "        start_time = time.time()\n",
    "        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n",
    "        #alike_columns = filled_data.filter(like=column_name)\n",
    "        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n",
    "        print(\"filled_data.columns = \", filled_data.columns)\n",
    "        filtered_columns = df.filter(like=column_name)\n",
    "        print(filtered_columns.columns)\n",
    "        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n",
    "        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n",
    "        filtered_columns.fillna(0, inplace=True)\n",
    "        \n",
    "        sum_column = filtered_columns.sum(axis=1)\n",
    "        # Define a specific name for the new column\n",
    "        df[f'cumulative_{column_name}'] = sum_column\n",
    "        df[filtered_columns.columns] = filtered_columns\n",
    "        \n",
    "        if filtered_columns.isnull().any().any():\n",
    "          print(\"filtered_columns :\", filtered_columns)\n",
    "          raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "      \n",
    "        print(\"filled_data.columns: \", filled_data.columns)\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n",
    "        \n",
    "      filled_data = df\n",
    "      filled_data[\"date\"] = target_date\n",
    "      print(\"Finished correctly \", filled_data.head())\n",
    "      filled_data.to_csv(gap_filled_csv, index=False)\n",
    "      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n",
    "      df = filled_data\n",
    "    \n",
    "    result = df\n",
    "    print(\"result.head = \", result.head())\n",
    "    # fill in the rest NA as 0\n",
    "    if result.isnull().any().any():\n",
    "      print(\"result :\", result)\n",
    "      raise ValueError(\"Single group: shouldn't have null values here\")\n",
    "    \n",
    "    # only retain the rows of the target date\n",
    "    print(result['date'].unique())\n",
    "    print(result.shape)\n",
    "    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n",
    "    result.to_csv(target_csv_path, index=False)\n",
    "    print(f\"New data is saved to {os.path.basename(target_csv_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de48c38",
   "metadata": {},
   "source": [
    "### 3.6.4.7 Interpolate Missing Values and Calculate Cumulative SWE In-Place for AMSR Data\n",
    "\n",
    "Here we aim to ensure that any missing or anomalous data points within a specific column are handled appropriately through interpolation, and then a cumulative sum is calculated. \n",
    "\n",
    "This is particularly useful in time series data, where continuity is crucial, and missing data could skew analysis. \n",
    "\n",
    "The cumulative sum provides an aggregated measure that can be used for further analysis, such as tracking total snowfall or snow cover over a period.\n",
    "\n",
    "- `row`: A Pandas Series representing a single row of data from a DataFrame, which contains the values to be interpolated.\n",
    "- `column_name`: The specific column in the DataFrame that needs interpolation and cumulative calculation.\n",
    "- `degree`: The degree of the polynomial used for interpolation, with a default of 1 (linear).\n",
    "\n",
    "- `x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]`: Identifies the subset of columns within the row that corresponds to the specified `column_name`, allowing focused interpolation on the relevant data.\n",
    "\n",
    "- `are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()`: Checks if all values in the specified subset are within a valid range (for SWE, between 1 and 239), which helps ensure that the data is suitable for interpolation.\n",
    "\n",
    "- `row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()`: After interpolation, this line calculates the cumulative sum of the values in the subset and adds it as a new column in the row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f290e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n",
    "  \"\"\"\n",
    "  Interpolate missing values in a Pandas Series using polynomial interpolation\n",
    "  and add a cumulative column.\n",
    "\n",
    "  Parameters:\n",
    "    - row (pd.Series): The input row containing the data to be interpolated.\n",
    "    - column_name (str): The name of the column to be interpolated.\n",
    "    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n",
    "\n",
    "  Returns:\n",
    "    - pd.Series: The row with interpolated values and a cumulative column.\n",
    "\n",
    "  Raises:\n",
    "    - ValueError: If there are unexpected null values after interpolation.\n",
    "\n",
    "  Note:\n",
    "    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n",
    "    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n",
    "\n",
    "  Examples:\n",
    "    ```python\n",
    "    # Example usage:\n",
    "    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n",
    "    ```\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  # Extract X series (column names)\n",
    "  x_all_key = row.index\n",
    "  \n",
    "  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n",
    "  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n",
    "  if are_all_values_between_0_and_240:\n",
    "    print(\"row[x_subset_key] = \", row[x_subset_key])\n",
    "    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n",
    "  # create the cumulative column after interpolation\n",
    "  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n",
    "  return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e4d15",
   "metadata": {},
   "source": [
    "### 3.6.4.8 Running the AMSR Data Extraction Process\n",
    "This script is to handle the entire workflow, from data preparation to the generation of cumulative time series data.\n",
    "\n",
    "- `prepare_amsr_grid_mapper()`: It maps the AMSR grid to the gridMET grid, preparing the necessary data for further processing.\n",
    "\n",
    "- `get_cumulative_amsr_data(force=False)`: This calculates cumulative AMSR data for a specific date range and handles any missing data. The force parameter determines whether to overwrite existing processed data.\n",
    "\n",
    "- `input_time_series_file`: Defines the file path for the cumulative AMSR data, which will be used in subsequent analyses or processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7575f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File amsr_to_gridmet_mapper.csv already exists, skipping..\n",
      "File testing_ready_amsr_2024.07.18.csv already exists, skipping..\n",
      "2024-07-18 00:00:00\n",
      "../data/gridmet_test_run/testing_ready_amsr_2024-07-18_cumulative.csv_gap_filled.csv already exists, skipping..\n",
      "count    100.0\n",
      "mean       0.0\n",
      "std        0.0\n",
      "min        0.0\n",
      "25%        0.0\n",
      "50%        0.0\n",
      "75%        0.0\n",
      "max        0.0\n",
      "Name: AMSR_SWE, dtype: float64\n",
      "result.head =     gridmet_lat  gridmet_lon  AMSR_SWE_2023-10-01  AMSR_Flag_2023-10-01  \\\n",
      "0         49.0     -125.000                  0.0                   241   \n",
      "1         49.0     -124.964                  0.0                   241   \n",
      "2         49.0     -124.928                  0.0                   241   \n",
      "3         49.0     -124.892                  0.0                   241   \n",
      "4         49.0     -124.856                  0.0                   241   \n",
      "\n",
      "   AMSR_SWE_2023-10-02  AMSR_Flag_2023-10-02  AMSR_SWE_2023-10-03  \\\n",
      "0                  0.0                   241                  0.0   \n",
      "1                  0.0                   241                  0.0   \n",
      "2                  0.0                   241                  0.0   \n",
      "3                  0.0                   241                  0.0   \n",
      "4                  0.0                   241                  0.0   \n",
      "\n",
      "   AMSR_Flag_2023-10-03  AMSR_SWE_2023-10-04  AMSR_Flag_2023-10-04  ...  \\\n",
      "0                   241                  0.0                   241  ...   \n",
      "1                   241                  0.0                   241  ...   \n",
      "2                   241                  0.0                   241  ...   \n",
      "3                   241                  0.0                   241  ...   \n",
      "4                   241                  0.0                   241  ...   \n",
      "\n",
      "   AMSR_SWE_2024-07-15  AMSR_Flag_2024-07-15  AMSR_SWE_2024-07-16  \\\n",
      "0                  0.0                   241                  0.0   \n",
      "1                  0.0                   241                  0.0   \n",
      "2                  0.0                   241                  0.0   \n",
      "3                  0.0                   241                  0.0   \n",
      "4                  0.0                   241                  0.0   \n",
      "\n",
      "   AMSR_Flag_2024-07-16  AMSR_SWE_2024-07-17  AMSR_Flag_2024-07-17  AMSR_SWE  \\\n",
      "0                   241                  0.0                   241       0.0   \n",
      "1                   241                  0.0                   241       0.0   \n",
      "2                   241                  0.0                   241       0.0   \n",
      "3                   241                  0.0                   241       0.0   \n",
      "4                   241                  0.0                   241       0.0   \n",
      "\n",
      "   AMSR_Flag  cumulative_AMSR_SWE        date  \n",
      "0        241                  7.5  2024-07-18  \n",
      "1        241                  7.5  2024-07-18  \n",
      "2        241                  7.5  2024-07-18  \n",
      "3        241                  7.5  2024-07-18  \n",
      "4        241                  7.5  2024-07-18  \n",
      "\n",
      "[5 rows x 588 columns]\n",
      "['2024-07-18']\n",
      "(100, 588)\n",
      "       AMSR_SWE   AMSR_Flag\n",
      "count     100.0  100.000000\n",
      "mean        0.0  247.750000\n",
      "std         0.0    6.927292\n",
      "min         0.0  241.000000\n",
      "25%         0.0  241.000000\n",
      "50%         0.0  241.000000\n",
      "75%         0.0  255.000000\n",
      "max         0.0  255.000000\n",
      "New data is saved to testing_ready_amsr_2024-07-18_cumulative.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the download and conversion function\n",
    "\n",
    "prepare_amsr_grid_mapper()\n",
    "download_amsr_and_convert_grid()\n",
    "\n",
    "get_cumulative_amsr_data(force=False)\n",
    "input_time_series_file = f'{work_dir}/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826941f6",
   "metadata": {},
   "source": [
    "## Utility Functions for Feature Extraction\n",
    "\n",
    "The following functions are categorized as utility functions. These functions are not central to the main discussion but play a supportive role by providing necessary functionality. They can be referenced as needed throughout the chapter to simplify and streamline the main code examples.\n",
    "\n",
    "### 1. Importing required python libraries to run the script\n",
    "\n",
    "- **Importing Libraries**: Essential libraries are imported for handling files, processing large datasets, and performing complex calculations.\n",
    "  - `os`, `shutil`, `subprocess`: For file handling, copying, and executing shell commands.\n",
    "  - `csv`, `h5py`, `numpy`, `pandas`: For reading/writing files, handling HDF5 datasets, numerical computations, and data manipulation.\n",
    "  - `dask`, `xarray`: To manage and process large datasets efficiently using parallel computing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "134fc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "import dask.bag as db\n",
    "import xarray as xr\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# For demonstration purposes, we're using one week of data for training.\n",
    "# The training period is set from December 24, 2022, to December 31, 2022.\n",
    "train_start_date = \"2022-12-24\"\n",
    "train_end_date = \"2022-12-31\"\n",
    "\n",
    "work_dir = \"../data/gridmet_test_run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36257e94",
   "metadata": {},
   "source": [
    "###  2. Function to Copy .he5 Files from Source to Destination Directory\n",
    "\n",
    "The goal here is to copy all `.he5` files from a specified source directory to a destination directory.\n",
    "\n",
    "- `source_dir`: The directory where the `.he5` files are originally located.\n",
    "- `destination_dir`: The target directory where the `.he5` files will be copied.\n",
    "- `os.walk`: A function that traverses the directory tree, accessing all subdirectories and files.\n",
    "- `shutil.copy`: A method used to copy the files from the source to the destination.\n",
    "\n",
    "The code specifically looks for files with the `.he5` extension to identify the relevant files for copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c7711af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_he5_files(source_dir, destination_dir):\n",
    "    '''\n",
    "    Copy .he5 files from the source directory to the destination directory.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): The source directory containing .he5 files to copy.\n",
    "        destination_dir (str): The destination directory where .he5 files will be copied.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get a list of all subdirectories and files in the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.he5'):\n",
    "                # Get the absolute path of the source file\n",
    "                source_file_path = os.path.join(root, file)\n",
    "                # Copy the file to the destination directory\n",
    "                shutil.copy(source_file_path, destination_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
