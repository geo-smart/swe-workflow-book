{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "source": [
    "# Feature Selection\n",
    "\n",
    "Criteria for selecting features in SWE prediction models\n",
    "Techniques and tools used for feature selection\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7532ade1ccf2869e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cada304604ef5dc8"
=======
   "id": "7387d7e6",
   "metadata": {},
   "source": [
    "# Feature Selection for SWE Prediction Models\n",
    "Criteria for selecting features in SWE prediction models, Techniques and tools used for feature selection\n",
    "\n",
    "## Introduction to the Data\n",
    "\n",
    "The three key datasets:\n",
    "\n",
    "- **Climatology Data:** Offers a broad view of weather patterns over time.\n",
    "- **SNOTEL Data:** Provides specific insights into snowpack conditions.\n",
    "- **Terrain Data:** Brings in the geographical and physical characteristics of the landscape.\n",
    "\n",
    "Each dataset comes packed with essential features like latitude, longitude, and date, ready to enrich our SWE prediction model.\n",
    "\n",
    "## Step 1: Integrating the Datasets with Dask\n",
    "\n",
    "We are combining these large datasets into one DataFrame using Dask. Dask allows us to work with big data efficiently, so we can merge the datasets quickly and easily, no matter how large they are.\n",
    "\n",
    "And also if the size of the data is larger then reading large CSV files in chunks helps manage big data more efficiently by reducing memory use, speeding up processing, and improving error handling. This approach makes it easier to work on large datasets with limited resources, ensuring flexibility and scalability in data analysis.\n",
    "\n",
    "#### Read and Convert\n",
    "- Each CSV file is read into a Dask DataFrame, with latitude and longitude data types converted to floats for uniformity. And also if the size of the data is larger then reading large CSV files in chunks helps manage big data more efficiently by reducing memory use, speeding up processing, and improving error handling. This approach makes it easier to work on large datasets with limited resources, ensuring flexibility and scalability in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25039fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "file_path1 = '../data/training_ready_climatology_data.csv'\n",
    "file_path2 = '../data/training_ready_snotel_data.csv'\n",
    "file_path3 = '../data/training_ready_terrain_data.csv'\n",
    "# Read each CSV file into a Dask DataFrame\n",
    "df1 = dd.read_csv(file_path1)\n",
    "df2 = dd.read_csv(file_path2)\n",
    "df3 = dd.read_csv(file_path3)\n",
    "# Perform data type conversion for latitude and longitude columns\n",
    "df1['lat'] = df1['lat'].astype(float)\n",
    "df1['lon'] = df1['lon'].astype(float)\n",
    "df2['lat'] = df2['lat'].astype(float)\n",
    "df2['lon'] = df2['lon'].astype(float)\n",
    "df3['lat'] = df3['lat'].astype(float)\n",
    "df3['lon'] = df3['lon'].astype(float)\n",
    "#rename the columns to match the other dataframes\n",
    "df2 = df2.rename(columns={\"Date\": \"date\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96f259",
   "metadata": {},
   "source": [
    "#### Merge on Common Ground\n",
    "- The dataframes are then merged based on shared columns (latitude, longitude, and date), ensuring that each row represents a coherent set of data from all three sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43de425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n",
    "merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'date'])\n",
    "\n",
    "# Merge the third DataFrame based on 'lat' and 'lon'\n",
    "merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c17226",
   "metadata": {},
   "source": [
    "#### Output\n",
    "- The merged DataFrame is saved as a new CSV file, ready for further processing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c200ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/vangavetisaivivek/research/swe-workflow-book/book/data/model_training_data.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df2.to_csv('../data/model_training_data.csv', index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6ad77",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing for Feature Selection\n",
    "\n",
    "Preprocessing steps are crucial for fine-tuning the data to ensure it's model-ready. This includes:\n",
    "- **Date-Range Data Clipping:** This step focuses on trimming the data to fit a specified date range, which is necessary for the analysis. After this trimming process, we save the refined data back into a CSV file.\n",
    "- **Filtering:** Select only the relevant columns needed for SWE prediction, such as weather conditions, geographic features, and snowpack measurements.\n",
    "- **Renaming:** Streamline column names for consistency and clarity (e.g., changing \"Snow Water Equivalent (in) Start of Day Values\" to \"swe_value\").\n",
    "\n",
    "save the final data into csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8df5d523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/vangavetisaivivek/research/swe-workflow-book/book/data/model_training_cleaned.csv']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_csv = '../data/model_training_data.csv'\n",
    "\n",
    "# List of columns you want to extract\n",
    "selected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n",
    "                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n",
    "                    'elevation',\n",
    "                    'slope', 'curvature', 'aspect', 'eastness',\n",
    "                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n",
    "# Read the CSV file into a Dask DataFrame\n",
    "df = dd.read_csv(input_csv, usecols=selected_columns)\n",
    "\n",
    "df = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n",
    "\n",
    "# Replace 'output.csv' with the desired output file name\n",
    "output_csv = '../data/model_training_cleaned.csv'\n",
    "\n",
    "# Write the selected columns to a new CSV file\n",
    "df.to_csv(output_csv, index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803de8a",
   "metadata": {},
   "source": [
    "## Step 3: Advanced Merging and Cleaning\n",
    "\n",
    "For a deeper dive, additional scripts provide a more intricate merging process involving multiple data sources and filters based on time ranges. The aim here is to:\n",
    "\n",
    "- **Integrate Further Data:** Additional sources like AMSR data are introduced, expanding the dataset with more variables relevant to SWE prediction.\n",
    "- **Optimize and Clean:** Repartitioning and dropping duplicates are applied post-merge to ensure the dataset is optimized for processing and free of redundancy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf07ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "working_dir = f\"../data\"\n",
    "work_dir = working_dir\n",
    "final_output_name = \"final_merged_data_3yrs_all_active_stations_v1.csv\"\n",
    "chunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67230adc",
   "metadata": {},
   "source": [
    " It begins by importing necessary libraries like dask.dataframe, os, pandas.\n",
    "\n",
    "- **dask.dataframe:** This is for handling large datasets efficiently. Dask is a Python library that allows for parallel computing and works well with datasets too large for the memory of a single computer.\n",
    "\n",
    "- **os:** This module provides a way of using operating system-dependent functionality like reading or writing to a file system.\n",
    "\n",
    "- **pandas**: This module is great for data manipulation and analysis. It's particularly used for working with tabular data (like spreadsheets and SQL database outputs).\n",
    "\n",
    "Initially, it identifies the user's home directory to establish a base location. Subsequently, within this base location, we are giving a specific path which is directory named 'data' where we have provided all the data that is needed for analysis. Then we define the name of the output file,\n",
    "final_merged_data_3yrs_all_active_stations_v1.csv. To efficiently manage computer memory during this operation, the data will be processed in segments, each limited to 10MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f8668c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amsr_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_amsr_dask.csv'\n",
    "snotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\n",
    "gridmet_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv'\n",
    "terrain_file = f'{working_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv'\n",
    "fsca_file = f'{working_dir}/fsca_final_training_all.csv'\n",
    "final_final_output_file = f'{work_dir}/{final_output_name}'\n",
    "\n",
    "if os.path.exists(final_final_output_file):\n",
    "    print(f\"The file '{final_final_output_file}' exists. Skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b4ac6",
   "metadata": {},
   "source": [
    "Here we are defining the input and output files, checks if the final output file already exists. If it does, it prints a message and skips further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a599b0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amsr.columns =  Index(['date', 'lat', 'lon', 'AMSR_SWE'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files with a smaller chunk size and compression\n",
    "amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n",
    "print(\"amsr.columns = \", amsr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a2a17",
   "metadata": {},
   "source": [
    "It reads data from CSV file into Dask DataFrames by blocks which provides a flexible and efficient approach for handling large datasets, enabling better scalability and performance in data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f3cdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snotel.columns =  Index(['station_name', 'date', 'lat', 'lon', 'swe_value', 'change_in_swe_inch',\n",
      "       'snow_depth', 'air_temperature_observed_f'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "snotel = dd.read_csv(snotel_file, blocksize=chunk_size)\n",
    "print(\"snotel.columns = \", snotel.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f57204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridmet.columns =  Index(['day', 'lat', 'lon', 'air_temperature_tmmn',\n",
      "       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n",
      "       'relative_humidity_rmax', 'relative_humidity_rmin',\n",
      "       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n",
    "# Drop the 'Unnamed: 0' column\n",
    "gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n",
    "print(\"gridmet.columns = \", gridmet.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb830353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrain.columns =  Index(['stationTriplet', 'elevation', 'lat', 'lon', 'Elevation', 'Slope',\n",
      "       'Aspect', 'Curvature', 'Northness', 'Eastness'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n",
    "# rename columns to match the other dataframes\n",
    "terrain = terrain.rename(columns={\n",
    "    \"latitude\": \"lat\", \n",
    "    \"longitude\": \"lon\"\n",
    "})\n",
    "# select only the columns we need for the final output\n",
    "terrain = terrain[[\"stationTriplet\", \"elevation\", \"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n",
    "print(\"terrain.columns = \", terrain.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcd4dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowcover.columns =  Index(['date', 'lat', 'lon', 'fSCA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n",
    "# rename columns to match the other dataframes\n",
    "snowcover = snowcover.rename(columns={\n",
    "    \"latitude\": \"lat\", \n",
    "    \"longitude\": \"lon\"\n",
    "})\n",
    "print(\"snowcover.columns = \", snowcover.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e79ad4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the dataframes are partitioned\n"
     ]
    }
   ],
   "source": [
    "# Repartition DataFrames for optimized processing\n",
    "amsr = amsr.repartition(partition_size=chunk_size)\n",
    "snotel = snotel.repartition(partition_size=chunk_size)\n",
    "gridmet = gridmet.repartition(partition_size=chunk_size)\n",
    "gridmet = gridmet.rename(columns={'day': 'date'})\n",
    "terrain = terrain.repartition(partition_size=chunk_size)\n",
    "snow_cover = snowcover.repartition(partition_size=chunk_size)\n",
    "print(\"all the dataframes are partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50240e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to merge amsr and snotel\n",
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_snotel.csv\n",
      "start to merge gridmet\n",
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_gridmet.csv\n",
      "start to merge terrain\n",
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_terrain.csv\n",
      "start to merge snowcover\n",
      "intermediate file saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_snow_cover.csv\n",
      "Merge completed. ../data/final_merged_data_3yrs_all_active_stations_v1.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames based on specified columns\n",
    "print(\"start to merge amsr and snotel\")\n",
    "merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_snotel.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge gridmet\")\n",
    "merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge terrain\")\n",
    "merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "print(\"start to merge snowcover\")\n",
    "merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n",
    "merged_df = merged_df.drop_duplicates(keep='first')\n",
    "output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f\"intermediate file saved to {output_file}\")\n",
    "\n",
    "# Save the merged DataFrame to a CSV file in chunks\n",
    "output_file = os.path.join(working_dir, final_output_name)\n",
    "merged_df.to_csv(output_file, single_file=True, index=False)\n",
    "print(f'Merge completed. {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c63a4000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n",
    "df = dd.read_csv(f'{work_dir}/{final_output_name}', dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df.to_csv(f'{work_dir}/{final_output_name}', single_file=True, index=False)\n",
    "print('Data cleaning completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5408637",
   "metadata": {},
   "source": [
    "- **Merge and Save AMSR and SNOTEL Data:**\n",
    "    - It merges AMSR and SNOTEL data on latitude, longitude, and date using an outer join. on=['lat', 'lon', 'date'] specifies the columns to merge on and how='outer' performs an outer join, retaining all rows from both Dataframes.\n",
    "    - Removes duplicate rows.\n",
    "    - Saves the merged DataFrame to a CSV file named {final_output_name}_snotel.csv.\n",
    "- **Merge and Save Gridmet Data:**\n",
    "    - It merges the previously merged DataFrame with Gridmet data on latitude, longitude, and date using an outer join.\n",
    "    - Removes duplicate rows.\n",
    "    - Saves the updated merged DataFrame to a CSV file named {final_output_name}_gridmet.csv.\n",
    "- **Merge and Save Terrain Data:**\n",
    "    - It merges the DataFrame again with terrain data on latitude and longitude using an outer join.\n",
    "    - Removes duplicate rows.\n",
    "    - Saves the updated merged DataFrame to a CSV file named {final_output_name}_terrain.csv\n",
    "- **Merge and Save Snow Cover Data:**\n",
    "    - It merges the DataFrame once more with snow cover data on latitude, longitude, and date using an outer join.\n",
    "    - Removes duplicate rows.\n",
    "    - Saves the updated merged DataFrame to a CSV file named {final_output_name}_snow_cover.csv\n",
    "- **Save Final Merged Data:**\n",
    "    - It saves the final merged DataFrame to a single CSV file named {final_output_name} in the specified working directory.\n",
    "- **Data Cleaning:**\n",
    "    - It reads the final merged DataFrame again.\n",
    "    - Removes duplicate rows.\n",
    "    - Saves the cleaned DataFrame to a new CSV file with the same name {final_output_name}.\n",
    "    \n",
    "    single_file=True: Saves data to a single file.\n",
    "    \n",
    "    index=False: Omits DataFrame index from the CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beb02f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted training data is saved to ../data/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "def sort_training_data(input_training_csv, sorted_training_csv):\n",
    "    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n",
    "    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype={'stationTriplet': 'object',\n",
    "       'station_name': 'object'})\n",
    "\n",
    "    # Persist the Dask DataFrame in memory\n",
    "    ddf = ddf.persist()\n",
    "\n",
    "    # Sort Dask DataFrame by three columns: date, lat, and Lon\n",
    "    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n",
    "\n",
    "    # Save the sorted Dask DataFrame to a new CSV file\n",
    "    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n",
    "    print(f\"sorted training data is saved to {sorted_training_csv}\")\n",
    "\n",
    "final_final_output_file = f'{work_dir}/{final_output_name}'\n",
    "sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd7545",
   "metadata": {},
   "source": [
    "Here we first read the CSV file into a Dask DataFrame, persists the Dask DataFrame in memory, which improves performance by keeping the data cached and readily accessible for further processing. Sorts the Dask DataFrame based on the specified columns (date, lat, lon). Saves the sorted Dask DataFrame to a new CSV file specified by sorted_training_csv. The index=False argument ensures that the index column is not included in the output CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48810821",
   "metadata": {},
   "source": [
    "## Conclusion: The Ready-to-Train Dataset\n",
    "\n",
    "The outcome of this journey is a rich, comprehensive dataset that stands ready for training SWE prediction models. Through meticulous merging, preprocessing, and cleaning, weâ€™ve prepared a dataset that encapsulates the complexity of the environment and the specificity of snowpack conditions, laying a solid foundation for accurate and reliable SWE predictions.\n",
    "\n",
    "This streamlined dataset not only facilitates more accurate models but also illustrates the importance of a thorough feature selection process in predictive modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0c2f4",
   "metadata": {},
   "source": []
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
<<<<<<< HEAD
    "version": 2
=======
    "version": 3
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
=======
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
