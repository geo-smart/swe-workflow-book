{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c689bcf816fa42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3.4 MODIS for fsCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476bab",
   "metadata": {},
   "source": [
    "This script is designed to download MODIS snow cover data from NASA servers, convert the downloaded HDF files to GeoTIFF format, and then merge these GeoTIFF tiles into a single file for each day within a specified date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b8046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import earthaccess\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683a825",
   "metadata": {},
   "source": [
    "**os, subprocess, threading**: Libraries for file operations, running shell commands, and multithreading.\n",
    "\n",
    "**datetime**: Library for date and time manipulation.\n",
    "\n",
    "**requests**: Library for making HTTP requests.\n",
    "\n",
    "**earthaccess**: Library for accessing Earth data.\n",
    "\n",
    "**gdal**: Library for geospatial data operations.\n",
    "\n",
    "create an account in urs.earthdata.nasa.gov for earth access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846abe3a",
   "metadata": {},
   "source": [
    "## 3.4.1 Converting date to Julian format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377eb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '../data/fsca'\n",
    "def date_to_julian(date_str):\n",
    "    \"\"\"\n",
    "    Convert a date to Julian date.\n",
    "    \"\"\"\n",
    "    date_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    tt = date_object.timetuple()\n",
    "    \n",
    "\n",
    "    # Format the result as 'YYYYDDD'\n",
    "    julian_format = str('%d%03d' % (tt.tm_year, tt.tm_yday))\n",
    "\n",
    "    return julian_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e6f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 1, 31)\n",
    "tile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \"h15v04\", \"h16v03\", \"h16v04\"]\n",
    "input_folder = f\"{work_dir}/temp/\"\n",
    "output_folder = f\"{work_dir}/output_folder/\"\n",
    "modis_day_wise = f\"{work_dir}/final_output/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(modis_day_wise, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b46111",
   "metadata": {},
   "source": [
    "Defines the date range and a list of MODIS tiles.\n",
    "\n",
    "Creates input, output, and final output directories if they don't exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d47a0",
   "metadata": {},
   "source": [
    "## 3.4.2 Convert HDF to GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b5e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hdf_to_geotiff(hdf_file, output_folder):\n",
    "  hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n",
    "\n",
    "  # Specific subdataset name you're interested in\n",
    "  target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n",
    "  # Create a name for the output file based on the HDF file name and subdataset\n",
    "  output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n",
    "  output_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "  if os.path.exists(output_path):\n",
    "    pass\n",
    "    #print(f\"The file {output_path} exists. skip.\")\n",
    "  else:\n",
    "    for subdataset in hdf_ds.GetSubDatasets():\n",
    "      # Check if the subdataset is the one we want to convert\n",
    "      if target_subdataset_name in subdataset[0]:\n",
    "        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n",
    "        # Convert to GeoTIFF\n",
    "        gdal.Translate(output_path, ds)\n",
    "        ds = None\n",
    "        break  # Exit the loop after converting the target subdataset\n",
    "\n",
    "  hdf_ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371fcc0",
   "metadata": {},
   "source": [
    "- Opens the HDF file.\n",
    "\n",
    "- Identifies the specific subdataset (\"NDSI_Snow_Cover\") to convert.\n",
    "\n",
    "- Converts the identified subdataset to GeoTIFF format and saves it to the output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ae4ac",
   "metadata": {},
   "source": [
    "## 3.4.3 Convert All HDF Files in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9143a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_hdf_in_folder(folder_path, output_folder):\n",
    "    file_lst = list()\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_lst.append(file)\n",
    "        if file.lower().endswith(\".hdf\"):\n",
    "            hdf_file = os.path.join(folder_path, file)\n",
    "            convert_hdf_to_geotiff(hdf_file, output_folder)\n",
    "    return file_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d49491",
   "metadata": {},
   "source": [
    "- iterates through all files in the specified folder.\n",
    "- Converts each HDF file to GeoTIFF format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222844e",
   "metadata": {},
   "source": [
    "## 3.4.4 Merge GeoTIFF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8622ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tifs(folder_path, target_date, output_file):\n",
    "    julian_date = date_to_julian(target_date)\n",
    "    print(\"target julian date\", julian_date)\n",
    "    tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n",
    "    if len(tif_files) == 0:\n",
    "        print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n",
    "        print(\"generate a new csv file with empty values for each point\")\n",
    "        gdal_command = ['gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '0', '255', '200', '200', f\"{modis_day_wise}/fsca_template.tif\", output_file]\n",
    "        print(\"Running \", gdal_command)\n",
    "        subprocess.run(gdal_command)\n",
    "    else:\n",
    "        gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n",
    "        print(\"Running \", gdal_command)\n",
    "        subprocess.run(gdal_command)\n",
    "        gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n",
    "        print(\"Running \", gdal_command)\n",
    "        subprocess.run(gdal_command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de91eb",
   "metadata": {},
   "source": [
    "- Merges GeoTIFF files for the specified date.\n",
    "- If no GeoTIFF files are found for the date, it creates an empty GeoTIFF using a template.\n",
    "- If files are found, it merges them using gdalwarp and reprojects them to EPSG:4326."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075be3f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23862cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory):\n",
    "    return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95a11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tiles(date, hdf_files):\n",
    "    path = f\"data/{date}/\"\n",
    "    files = list_files(path)\n",
    "    print(files)\n",
    "    merged_filename = f\"data/{date}/merged.tif\"\n",
    "    merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n",
    "    try:\n",
    "        subprocess.run(merge_command)\n",
    "        print(f\"Merged tiles into {merged_filename}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error merging tiles: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73c81e",
   "metadata": {},
   "source": [
    "Merges HDF tiles into a single GeoTIFF file for the specified date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c8b3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(date, url):\n",
    "    file_name = url.split('/')[-1]\n",
    "    if os.path.exists(f'data/{date}/{file_name}'):\n",
    "        print(f'File: {file_name} already exists, SKIPPING')\n",
    "        return\n",
    "    try:\n",
    "        os.makedirs('data/', exist_ok=True)\n",
    "        os.makedirs(f'data/{date}', exist_ok=True)\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(f'data/{date}/{file_name}', 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30f102",
   "metadata": {},
   "source": [
    "Downloads a file from a given URL and saves it to the data directory for the specified date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b66267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all(date, urls):\n",
    "    threads = []\n",
    "\n",
    "    for url in urls:\n",
    "        thread = threading.Thread(target=download_url, args=(date, url,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b04ecf",
   "metadata": {},
   "source": [
    "Downloads multiple files in parallel using threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c463ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_in_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"Folder does not exist.\")\n",
    "        return\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            else:\n",
    "                print(f\"Skipping {filename}, as it is not a file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d1f1b",
   "metadata": {},
   "source": [
    "Deletes all files in the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee4f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ../data/fsca/final_output//2023-01-01__snow_cover.tif does not exist.\n",
      "start to download files from NASA server to local\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f783fe916b404146807897d33b326446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c588c70d344b0f9f8e800c3bd257e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358e8bc14ac34ebf87487a1f4f50c593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with downloading, start to convert HDF to geotiff..\n"
     ]
    }
   ],
   "source": [
    "def download_tiles_and_merge(start_date, end_date):\n",
    "    date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "    for i in date_list:\n",
    "        current_date = i.strftime(\"%Y-%m-%d\")\n",
    "        target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n",
    "        \n",
    "        if os.path.exists(target_output_tif):\n",
    "            file_size_bytes = os.path.getsize(target_output_tif)\n",
    "            print(f\"file_size_bytes: {file_size_bytes}\")\n",
    "            print(f\"The file {target_output_tif} exists. skip.\")\n",
    "        else:\n",
    "            print(f\"The file {target_output_tif} does not exist.\")\n",
    "            print(\"start to download files from NASA server to local\")\n",
    "            earthaccess.login(strategy=\"netrc\")\n",
    "            results = earthaccess.search_data(short_name=\"MOD10A1\", cloud_hosted=True, bounding_box=(-124.77, 24.52, -66.95, 49.38), temporal=(current_date, current_date))\n",
    "            earthaccess.download(results, input_folder)\n",
    "            print(\"done with downloading, start to convert HDF to geotiff..\")\n",
    "\n",
    "            convert_all_hdf_in_folder(input_folder, output_folder)\n",
    "            print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n",
    "\n",
    "            merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n",
    "        #delete_files_in_folder(input_folder)  # cleanup\n",
    "        #delete_files_in_folder(output_folder)  # cleanup\n",
    "download_tiles_and_merge(start_date, end_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
