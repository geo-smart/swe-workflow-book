{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "source": [
    "# Gridmet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a1c3a47d3070625"
=======
   "id": "6098a345",
   "metadata": {},
   "source": [
    "# GridMET Climatology Data Downloader\n",
    "\n",
    "In this chapter, we aim to:\n",
    "\n",
    "- **Download GridMET Datan**: Obtain specific meteorological variables from the GridMET climatology dataset for a user-specified year. This involves utilizing libraries such as netCDF4 and urllib for file handling and downloading.\n",
    "\n",
    "- **Visualize Data**: Create custom color maps and scatter plots to visualize meteorological variables spatially across the western United States. This functionality aids in understanding geographical patterns and trends in meteorological data.\n",
    "\n",
    "- **Generate Cumulative history CSVs**: Generate cumulative history CSVs to aggregate meteorological data over a specified date range. This feature allows users to analyze historical meteorological patterns and long-term trends for decision-making purposes.\n",
    "\n",
    "We facilitate the retrieval of specific meteorological variables from the GridMET climatology dataset for a user-defined year. Leveraging libraries such as netCDF4, urllib, and pandas, we enable seamless data handling and manipulation, ensuring efficient processing and analysis of climatological data. We offer custom color mapping for visualizing meteorological patterns and provide functionality for generating cumulative history CSVs, allowing users to aggregate data from past October 1st to the specified target date for trend analysis. With matplotlib, we enable plotting and visualization, empowering users to gain insights through graphical representations.\n",
    "\n",
    "We provide comprehensive outputs including downloaded meteorological variables in NetCDF format, custom color-mapped visualizations showcasing meteorological patterns, and cumulative history CSV files containing aggregated data for trend analysis. With these outputs, users can explore and analyze GridMET climatology data efficiently, gaining valuable insights into long-term meteorological trends and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c3a47d3070625",
   "metadata": {},
   "source": [
    "# Gridmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92e47a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_year():\n",
    "    \"\"\"\n",
    "    Get the current year.\n",
    "\n",
    "    Returns:\n",
    "        int: The current year.\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    current_year = now.year\n",
    "    return current_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bca49",
   "metadata": {},
   "source": [
    "We retrieve the current year from the system's date and time settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b96876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_in_folder(folder_path, current_year):\n",
    "    \"\"\"\n",
    "    Remove all files in a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder to remove files from.\n",
    "    \"\"\"\n",
    "    # Get a list of files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Loop through the files and remove them\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f22a3",
   "metadata": {},
   "source": [
    "We iterate through the files within a designated folder and delete those that meet certain criteria, such as being associated with the current year and having a specific file extension. This ensures that only relevant files are removed from the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7c28a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, target_file_path, variable):\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it to a specified location.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL of the file to download.\n",
    "        target_file_path (str): Path where the downloaded file should be saved.\n",
    "        variable (str): Name of the meteorological variable being downloaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            print(f\"Downloading {url}\")\n",
    "            file_content = response.read()\n",
    "        save_path = target_file_path\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(file_content)\n",
    "        print(f\"File downloaded successfully and saved as: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525baae",
   "metadata": {},
   "source": [
    "We facilitate the download of a file from a given URL and store it in a specified location. We consider the meteorological variable's name being downloaded. Upon successful completion, we provide a confirmation message containing the saved file's path. If any errors occur during the download process, we display an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b73d3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gridmet_of_specific_variables(year_list):\n",
    "    \"\"\"\n",
    "    Download specific meteorological variables from the GridMET climatology dataset.\n",
    "    \"\"\"\n",
    "    # Make a directory to store the downloaded files\n",
    "\n",
    "    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n",
    "    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n",
    "\n",
    "    for var in variables_list:\n",
    "        for y in year_list:\n",
    "            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n",
    "            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n",
    "            if not os.path.exists(target_file_path):\n",
    "                download_file(download_link, target_file_path, var)\n",
    "            else:\n",
    "                print(f\"File {target_file_path} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730fb749",
   "metadata": {},
   "source": [
    "We download specific meteorological variables from the GridMET climatology dataset by iterating over years and variables. If a file for a specific year and variable combination doesn't exist, we download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5d51f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_year():\n",
    "    now = datetime.now()\n",
    "    current_year = now.year\n",
    "    return current_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad1f85",
   "metadata": {},
   "source": [
    "We retrieve the current year from the system's date and time settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82adf8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name_from_path(file_path):\n",
    "    # Get the file name from the file path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2704d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_from_file_name(file_name):\n",
    "    # Assuming the file name format is \"tmmm_year.csv\"\n",
    "    var_name = str(file_name.split('_')[0])\n",
    "    return var_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ac603",
   "metadata": {},
   "source": [
    "We extract the file name from a given file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0eca065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_of_template_tif():\n",
    "  \t# Load the CSV file and extract coordinates\n",
    "    coordinates = []\n",
    "    df = pd.read_csv(dem_csv)\n",
    "    for index, row in df.iterrows():\n",
    "        # Process each row here\n",
    "        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n",
    "        coordinates.append((lon, lat))\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6869942",
   "metadata": {},
   "source": [
    "We load a CSV file and extract coordinates from it, resulting in a list of latitude and longitude tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d20022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_index(array, value):\n",
    "    # Find the index of the element in the array that is closest to the given value\n",
    "    return (abs(array - value)).argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba26855",
   "metadata": {},
   "source": [
    "We find the index of the element in the array that is closest to the given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6173cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gridmet_to_dem_mapper(nc_file):\n",
    "    western_us_dem_df = pd.read_csv(western_us_coords)\n",
    "    # Check if the CSV already exists\n",
    "    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n",
    "    if os.path.exists(target_csv_path):\n",
    "        print(f\"File {target_csv_path} already exists, skipping..\")\n",
    "        return\n",
    "    \n",
    "    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n",
    "    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n",
    "    # Read the NetCDF file\n",
    "    with nc.Dataset(nc_file) as nc_file:\n",
    "      \n",
    "      # Get the values at each coordinate using rasterio's sample function\n",
    "      latitudes = nc_file.variables['lat'][:]\n",
    "      longitudes = nc_file.variables['lon'][:]\n",
    "      \n",
    "      def get_gridmet_var_value(row):\n",
    "        # Perform your custom calculation here\n",
    "        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n",
    "        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n",
    "        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n",
    "    \n",
    "      # Use the apply function to apply the custom function to each row\n",
    "      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n",
    "                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n",
    "      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n",
    "                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n",
    "    \n",
    "    # Save the new converted AMSR to CSV file\n",
    "    western_us_dem_df.to_csv(target_csv_path, index=False)\n",
    "    \n",
    "    return western_us_dem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df72f52",
   "metadata": {},
   "source": [
    "We generate a comprehensive mapping between the geographic coordinates present in a DEM template CSV file and their respective counterparts within a GridMET netCDF file. This mapping serves as a vital bridge, facilitating the seamless association and alignment of DEM and GridMET datasets. This integration is crucial for a wide array of spatial analyses, environmental modeling endeavors, and geographical studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dad59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def get_operation_day():\n",
    "  # Get the current date and time\n",
    "  current_date = datetime.now()\n",
    "\n",
    "  # Calculate three days ago\n",
    "  three_days_ago = current_date - timedelta(days=3)\n",
    "\n",
    "  # Format the date as a string\n",
    "  three_days_ago_string = three_days_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "  print(three_days_ago_string)\n",
    "  return three_days_ago_string\n",
    "\n",
    "test_start_date = get_operation_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d7687",
   "metadata": {},
   "source": [
    "We retrieve the date of an operational day by calculating the date three days prior to the current date. This operational day is crucial for setting up various time-sensitive operations or analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c09fdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nc_csv_by_coords_and_variable(nc_file, var_name, target_date=test_start_date):\n",
    "    \n",
    "    create_gridmet_to_dem_mapper(nc_file)\n",
    "  \t\n",
    "    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n",
    "    \n",
    "    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    # Read the NetCDF file\n",
    "    with nc.Dataset(nc_file) as nc_file:\n",
    "      # Get a list of all variables in the NetCDF file\n",
    "      variables = nc_file.variables.keys()\n",
    "      \n",
    "      # Get the values at each coordinate using rasterio's sample function\n",
    "      latitudes = nc_file.variables['lat'][:]\n",
    "      longitudes = nc_file.variables['lon'][:]\n",
    "      day = nc_file.variables['day'][:]\n",
    "      long_var_name = gridmet_var_mapping[var_name]\n",
    "      var_col = nc_file.variables[long_var_name][:]\n",
    "\n",
    "      \n",
    "      # Calculate the day of the year\n",
    "      day_of_year = selected_date.timetuple().tm_yday\n",
    "      day_index = day_of_year - 1\n",
    "      \n",
    "      def get_gridmet_var_value(row):\n",
    "        # Perform your custom calculation here\n",
    "        lat_index = int(row[\"gridmet_lat_idx\"])\n",
    "        lon_index = int(row[\"gridmet_lon_idx\"])\n",
    "        var_value = var_col[day_index, lat_index, lon_index]\n",
    "        \n",
    "        return var_value\n",
    "    \n",
    "      # Use the apply function to apply the custom function to each row\n",
    "      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n",
    "      \n",
    "      # drop useless columns\n",
    "      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n",
    "      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n",
    "                               \"dem_lon\": \"Longitude\"}, inplace=True)\n",
    "    return mapper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c685758",
   "metadata": {},
   "source": [
    "We extract data for a specific variable from a NetCDF file by matching coordinates from a DEM template CSV file. This enables us to create a DataFrame containing the variable values alongside the corresponding coordinates. By doing so, we can effectively extract and analyze meteorological data for specific geographical locations, aiding in various environmental and geographical studies, as well as modeling endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46b63faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_gridmet_nc_to_csv(target_date=test_start_date):\n",
    "    \n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    generated_csvs = []\n",
    "    for root, dirs, files in os.walk(gridmet_folder_name):\n",
    "        for file_name in files:\n",
    "            \n",
    "            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n",
    "                print(f\"Checking file: {file_name}\")\n",
    "                var_name = get_var_from_file_name(file_name)\n",
    "                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n",
    "\n",
    "                if os.path.exists(res_csv):\n",
    "                    #os.remove(res_csv)\n",
    "                    print(f\"{res_csv} already exists. Skipping..\")\n",
    "                    generated_csvs.append(res_csv)\n",
    "                    continue\n",
    "\n",
    "                # Perform operations on each file here\n",
    "                netcdf_file_path = os.path.join(root, file_name)\n",
    "                print(\"Processing file:\", netcdf_file_path)\n",
    "                file_name = get_file_name_from_path(netcdf_file_path)\n",
    "\n",
    "                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n",
    "                                                       var_name, target_date)\n",
    "                df.replace('--', pd.NA, inplace=True)\n",
    "                df.to_csv(res_csv, index=False)\n",
    "                print(\"gridmet var saved: \", res_csv)\n",
    "                generated_csvs.append(res_csv)\n",
    "                \n",
    "    return generated_csvs   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77512189",
   "metadata": {},
   "source": [
    "We convert GridMET NetCDF files to CSV format for a specified date. We iterate through files in the GridMET folder, checking for files corresponding to the selected date. For each matching file, we extract the variable name and generate a CSV file containing the data. If the CSV file already exists, we skip the process. This process facilitates easy access and analysis of meteorological data for a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62f7b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridmet(target_date=test_start_date):\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  var_name = \"pr\"\n",
    "  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n",
    "  gridmet_var_df = pd.read_csv(test_csv)\n",
    "  gridmet_var_df.replace('--', pd.NA, inplace=True)\n",
    "  gridmet_var_df.dropna(inplace=True)\n",
    "  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n",
    "  \n",
    "  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n",
    "  \n",
    "  # Create a scatter plot\n",
    "  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n",
    "              gridmet_var_df[\"Latitude\"].values, \n",
    "              label='Pressure', \n",
    "              color=colormaplist, \n",
    "              marker='o')\n",
    "\n",
    "  # Add labels and a legend\n",
    "  plt.xlabel('X-axis')\n",
    "  plt.ylabel('Y-axis')\n",
    "  plt.title('Scatter Plot Example')\n",
    "  plt.legend()\n",
    "  \n",
    "  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n",
    "  plt.savefig(res_png_path)\n",
    "  print(f\"test image is saved at {res_png_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27997ec",
   "metadata": {},
   "source": [
    "We plot GridMET meteorological data for a specific variable and date. We read the data from a corresponding CSV file and preprocess it, ensuring valid numerical values. Then, we create a scatter plot, mapping the variable values to geographic coordinates. The color of each point on the plot represents the magnitude of the variable value. Finally, we save the plot as a PNG image for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c643f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_folder_and_get_year_list(target_date=test_start_date):\n",
    "  # Check if the folder exists, if not, create it\n",
    "  if not os.path.exists(gridmet_folder_name):\n",
    "      os.makedirs(gridmet_folder_name)\n",
    "\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  if selected_date.month < 10:\n",
    "    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "  else:\n",
    "    past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "  year_list = [selected_date.year, past_october_1.year]\n",
    "\n",
    "  # Remove any existing files in the folder\n",
    "  if selected_date.year == datetime.now().year:\n",
    "    # check if the current year's netcdf contains the selected date\n",
    "    # get etr netcdf and read\n",
    "    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n",
    "    ifremove = False\n",
    "    if os.path.exists(nc_file):\n",
    "      with nc.Dataset(nc_file) as ncd:\n",
    "        day = ncd.variables['day'][:]\n",
    "        # Calculate the day of the year\n",
    "        day_of_year = selected_date.timetuple().tm_yday\n",
    "        day_index = day_of_year - 1\n",
    "        if len(day) <= day_index:\n",
    "          ifremove = True\n",
    "    \n",
    "    if ifremove:\n",
    "      print(\"The current year netcdf has new data. Redownloading..\")\n",
    "      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n",
    "    else:\n",
    "      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n",
    "  return year_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe1332",
   "metadata": {},
   "source": [
    "We prepare the folder structure for storing GridMET data and obtain a list of relevant years based on the target date. This process ensures that the necessary directory exists for data storage and determines the appropriate years for data retrieval without delving into technical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1621c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cumulative_column(df, column_name):\n",
    "  df[f'cumulative_{column_name}'] = df[column_name].sum()\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf6e29",
   "metadata": {},
   "source": [
    "We add a cumulative column to a DataFrame, summing the values of the specified column and storing the result in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef804f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n",
    "  \"\"\"\n",
    "    Prepare cumulative history CSVs for a specified target date.\n",
    "\n",
    "    Parameters:\n",
    "    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n",
    "    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n",
    "    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n",
    "    The cumulative values are calculated and saved in new CSV files.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n",
    "    ```\n",
    "\n",
    "    Note: This function assumes the existence of the following helper functions:\n",
    "    - download_gridmet_of_specific_variables\n",
    "    - prepare_folder_and_get_year_list\n",
    "    - turn_gridmet_nc_to_csv\n",
    "    - add_cumulative_column\n",
    "    - process_group_value_filling\n",
    "    ```\n",
    "\n",
    "    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "    print(selected_date)\n",
    "    if selected_date.month < 10:\n",
    "        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "    else:\n",
    "        past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "    # Rest of the function logic...\n",
    "\n",
    "    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n",
    "    print(\"new_df final shape: \", filled_data.head())\n",
    "    filled_data.to_csv(cumulative_target_path, index=False)\n",
    "    print(f\"new df is saved to {cumulative_target_path}\")\n",
    "    print(filled_data.describe())\n",
    "    ```\n",
    "Note: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n",
    "  \"\"\"\n",
    "  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "  print(selected_date)\n",
    "  if selected_date.month < 10:\n",
    "    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n",
    "  else:\n",
    "    past_october_1 = datetime(selected_date.year, 10, 1)\n",
    "\n",
    "  # Traverse and print every day from past October 1 to the specific date\n",
    "  current_date = past_october_1\n",
    "  \n",
    "  date_keyed_objects = {}\n",
    "  \n",
    "  download_gridmet_of_specific_variables(\n",
    "    prepare_folder_and_get_year_list(target_date=target_date)\n",
    "  )\n",
    "  \n",
    "  while current_date <= selected_date:\n",
    "    print(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date_str = current_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    \n",
    "    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n",
    "    \n",
    "    # read the csv into dataframe and merge to the big dataframe\n",
    "    date_keyed_objects[current_date_str] = generated_csvs\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "    \n",
    "  print(\"date_keyed_objects: \", date_keyed_objects)\n",
    "  target_generated_csvs = date_keyed_objects[target_date]\n",
    "  for index, single_csv in enumerate(target_generated_csvs):\n",
    "    # traverse the variables of gridmet here\n",
    "    # each variable is a loop\n",
    "    print(f\"creating cumulative for {single_csv}\")\n",
    "    \n",
    "    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n",
    "    print(\"cumulative_target_path = \", cumulative_target_path)\n",
    "    \n",
    "    if os.path.exists(cumulative_target_path) and not force:\n",
    "      print(f\"{cumulative_target_path} already exists, skipping..\")\n",
    "      continue\n",
    "    \n",
    "    # Extract the file name without extension\n",
    "    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n",
    "    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n",
    "\n",
    "\t# Split the file name using underscores\n",
    "    var_name = file_name.split('_')[1]\n",
    "    print(f\"Found variable name {var_name}\")\n",
    "    current_date = past_october_1\n",
    "    new_df = pd.read_csv(single_csv)\n",
    "    print(new_df.head())\n",
    "    \n",
    "    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n",
    "    all_df[\"date\"] = target_date\n",
    "    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n",
    "    filled_data = all_df\n",
    "    filled_data = filled_data[(filled_data['date'] == target_date)]\n",
    "    filled_data.fillna(0, inplace=True)\n",
    "    print(\"Finished correctly \", filled_data.head())\n",
    "    filled_data = filled_data[['Latitude', 'Longitude', \n",
    "                               var_name, \n",
    "#                                f'cumulative_{var_name}'\n",
    "                              ]]\n",
    "    print(filled_data.shape)\n",
    "    filled_data.to_csv(cumulative_target_path, index=False)\n",
    "    print(f\"new df is saved to {cumulative_target_path}\")\n",
    "    print(filled_data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa482514",
   "metadata": {},
   "source": [
    "We prepare cumulative history CSVs for a specified target date. We traverses the date range from the past October 1 to the target date, downloads GridMET data, converts it to CSV, and merges it into a big DataFrame. Cumulative values are then calculated and saved in new CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d202cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04 00:00:00\n",
      "The existing netcdf already covers the selected date. Avoid downloading..\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "An error occurred while downloading the file: name 'urllib' is not defined\n",
      "2023-10-01\n",
      "2023-10-02\n",
      "2023-10-03\n",
      "2023-10-04\n",
      "2023-10-05\n",
      "2023-10-06\n",
      "2023-10-07\n",
      "2023-10-08\n",
      "2023-10-09\n",
      "2023-10-10\n",
      "2023-10-11\n",
      "2023-10-12\n",
      "2023-10-13\n",
      "2023-10-14\n",
      "2023-10-15\n",
      "2023-10-16\n",
      "2023-10-17\n",
      "2023-10-18\n",
      "2023-10-19\n",
      "2023-10-20\n",
      "2023-10-21\n",
      "2023-10-22\n",
      "2023-10-23\n",
      "2023-10-24\n",
      "2023-10-25\n",
      "2023-10-26\n",
      "2023-10-27\n",
      "2023-10-28\n",
      "2023-10-29\n",
      "2023-10-30\n",
      "2023-10-31\n",
      "2023-11-01\n",
      "2023-11-02\n",
      "2023-11-03\n",
      "2023-11-04\n",
      "2023-11-05\n",
      "2023-11-06\n",
      "2023-11-07\n",
      "2023-11-08\n",
      "2023-11-09\n",
      "2023-11-10\n",
      "2023-11-11\n",
      "2023-11-12\n",
      "2023-11-13\n",
      "2023-11-14\n",
      "2023-11-15\n",
      "2023-11-16\n",
      "2023-11-17\n",
      "2023-11-18\n",
      "2023-11-19\n",
      "2023-11-20\n",
      "2023-11-21\n",
      "2023-11-22\n",
      "2023-11-23\n",
      "2023-11-24\n",
      "2023-11-25\n",
      "2023-11-26\n",
      "2023-11-27\n",
      "2023-11-28\n",
      "2023-11-29\n",
      "2023-11-30\n",
      "2023-12-01\n",
      "2023-12-02\n",
      "2023-12-03\n",
      "2023-12-04\n",
      "2023-12-05\n",
      "2023-12-06\n",
      "2023-12-07\n",
      "2023-12-08\n",
      "2023-12-09\n",
      "2023-12-10\n",
      "2023-12-11\n",
      "2023-12-12\n",
      "2023-12-13\n",
      "2023-12-14\n",
      "2023-12-15\n",
      "2023-12-16\n",
      "2023-12-17\n",
      "2023-12-18\n",
      "2023-12-19\n",
      "2023-12-20\n",
      "2023-12-21\n",
      "2023-12-22\n",
      "2023-12-23\n",
      "2023-12-24\n",
      "2023-12-25\n",
      "2023-12-26\n",
      "2023-12-27\n",
      "2023-12-28\n",
      "2023-12-29\n",
      "2023-12-30\n",
      "2023-12-31\n",
      "2024-01-01\n",
      "2024-01-02\n",
      "2024-01-03\n",
      "2024-01-04\n",
      "2024-01-05\n",
      "2024-01-06\n",
      "2024-01-07\n",
      "2024-01-08\n",
      "2024-01-09\n",
      "2024-01-10\n",
      "2024-01-11\n",
      "2024-01-12\n",
      "2024-01-13\n",
      "2024-01-14\n",
      "2024-01-15\n",
      "2024-01-16\n",
      "2024-01-17\n",
      "2024-01-18\n",
      "2024-01-19\n",
      "2024-01-20\n",
      "2024-01-21\n",
      "2024-01-22\n",
      "2024-01-23\n",
      "2024-01-24\n",
      "2024-01-25\n",
      "2024-01-26\n",
      "2024-01-27\n",
      "2024-01-28\n",
      "2024-01-29\n",
      "2024-01-30\n",
      "2024-01-31\n",
      "2024-02-01\n",
      "2024-02-02\n",
      "2024-02-03\n",
      "2024-02-04\n",
      "2024-02-05\n",
      "2024-02-06\n",
      "2024-02-07\n",
      "2024-02-08\n",
      "2024-02-09\n",
      "2024-02-10\n",
      "2024-02-11\n",
      "2024-02-12\n",
      "2024-02-13\n",
      "2024-02-14\n",
      "2024-02-15\n",
      "2024-02-16\n",
      "2024-02-17\n",
      "2024-02-18\n",
      "2024-02-19\n",
      "2024-02-20\n",
      "2024-02-21\n",
      "2024-02-22\n",
      "2024-02-23\n",
      "2024-02-24\n",
      "2024-02-25\n",
      "2024-02-26\n",
      "2024-02-27\n",
      "2024-02-28\n",
      "2024-02-29\n",
      "2024-03-01\n",
      "2024-03-02\n",
      "2024-03-03\n",
      "2024-03-04\n",
      "2024-03-05\n",
      "2024-03-06\n",
      "2024-03-07\n",
      "2024-03-08\n",
      "2024-03-09\n",
      "2024-03-10\n",
      "2024-03-11\n",
      "2024-03-12\n",
      "2024-03-13\n",
      "2024-03-14\n",
      "2024-03-15\n",
      "2024-03-16\n",
      "2024-03-17\n",
      "2024-03-18\n",
      "2024-03-19\n",
      "2024-03-20\n",
      "2024-03-21\n",
      "2024-03-22\n",
      "2024-03-23\n",
      "2024-03-24\n",
      "2024-03-25\n",
      "2024-03-26\n",
      "2024-03-27\n",
      "2024-03-28\n",
      "2024-03-29\n",
      "2024-03-30\n",
      "2024-03-31\n",
      "2024-04-01\n",
      "2024-04-02\n",
      "2024-04-03\n",
      "2024-04-04\n",
      "date_keyed_objects:  {'2023-10-01': [], '2023-10-02': [], '2023-10-03': [], '2023-10-04': [], '2023-10-05': [], '2023-10-06': [], '2023-10-07': [], '2023-10-08': [], '2023-10-09': [], '2023-10-10': [], '2023-10-11': [], '2023-10-12': [], '2023-10-13': [], '2023-10-14': [], '2023-10-15': [], '2023-10-16': [], '2023-10-17': [], '2023-10-18': [], '2023-10-19': [], '2023-10-20': [], '2023-10-21': [], '2023-10-22': [], '2023-10-23': [], '2023-10-24': [], '2023-10-25': [], '2023-10-26': [], '2023-10-27': [], '2023-10-28': [], '2023-10-29': [], '2023-10-30': [], '2023-10-31': [], '2023-11-01': [], '2023-11-02': [], '2023-11-03': [], '2023-11-04': [], '2023-11-05': [], '2023-11-06': [], '2023-11-07': [], '2023-11-08': [], '2023-11-09': [], '2023-11-10': [], '2023-11-11': [], '2023-11-12': [], '2023-11-13': [], '2023-11-14': [], '2023-11-15': [], '2023-11-16': [], '2023-11-17': [], '2023-11-18': [], '2023-11-19': [], '2023-11-20': [], '2023-11-21': [], '2023-11-22': [], '2023-11-23': [], '2023-11-24': [], '2023-11-25': [], '2023-11-26': [], '2023-11-27': [], '2023-11-28': [], '2023-11-29': [], '2023-11-30': [], '2023-12-01': [], '2023-12-02': [], '2023-12-03': [], '2023-12-04': [], '2023-12-05': [], '2023-12-06': [], '2023-12-07': [], '2023-12-08': [], '2023-12-09': [], '2023-12-10': [], '2023-12-11': [], '2023-12-12': [], '2023-12-13': [], '2023-12-14': [], '2023-12-15': [], '2023-12-16': [], '2023-12-17': [], '2023-12-18': [], '2023-12-19': [], '2023-12-20': [], '2023-12-21': [], '2023-12-22': [], '2023-12-23': [], '2023-12-24': [], '2023-12-25': [], '2023-12-26': [], '2023-12-27': [], '2023-12-28': [], '2023-12-29': [], '2023-12-30': [], '2023-12-31': [], '2024-01-01': [], '2024-01-02': [], '2024-01-03': [], '2024-01-04': [], '2024-01-05': [], '2024-01-06': [], '2024-01-07': [], '2024-01-08': [], '2024-01-09': [], '2024-01-10': [], '2024-01-11': [], '2024-01-12': [], '2024-01-13': [], '2024-01-14': [], '2024-01-15': [], '2024-01-16': [], '2024-01-17': [], '2024-01-18': [], '2024-01-19': [], '2024-01-20': [], '2024-01-21': [], '2024-01-22': [], '2024-01-23': [], '2024-01-24': [], '2024-01-25': [], '2024-01-26': [], '2024-01-27': [], '2024-01-28': [], '2024-01-29': [], '2024-01-30': [], '2024-01-31': [], '2024-02-01': [], '2024-02-02': [], '2024-02-03': [], '2024-02-04': [], '2024-02-05': [], '2024-02-06': [], '2024-02-07': [], '2024-02-08': [], '2024-02-09': [], '2024-02-10': [], '2024-02-11': [], '2024-02-12': [], '2024-02-13': [], '2024-02-14': [], '2024-02-15': [], '2024-02-16': [], '2024-02-17': [], '2024-02-18': [], '2024-02-19': [], '2024-02-20': [], '2024-02-21': [], '2024-02-22': [], '2024-02-23': [], '2024-02-24': [], '2024-02-25': [], '2024-02-26': [], '2024-02-27': [], '2024-02-28': [], '2024-02-29': [], '2024-03-01': [], '2024-03-02': [], '2024-03-03': [], '2024-03-04': [], '2024-03-05': [], '2024-03-06': [], '2024-03-07': [], '2024-03-08': [], '2024-03-09': [], '2024-03-10': [], '2024-03-11': [], '2024-03-12': [], '2024-03-13': [], '2024-03-14': [], '2024-03-15': [], '2024-03-16': [], '2024-03-17': [], '2024-03-18': [], '2024-03-19': [], '2024-03-20': [], '2024-03-21': [], '2024-03-22': [], '2024-03-23': [], '2024-03-24': [], '2024-03-25': [], '2024-03-26': [], '2024-03-27': [], '2024-03-28': [], '2024-03-29': [], '2024-03-30': [], '2024-03-31': [], '2024-04-01': [], '2024-04-02': [], '2024-04-03': [], '2024-04-04': []}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "work_dir = f\"{homedir}/gridmet_test_run\"\n",
    "gridmet_folder_name = f'{work_dir}/gridmet_climatology'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Run the download function\n",
    "  #   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n",
    "  #   turn_gridmet_nc_to_csv()\n",
    "  #   plot_gridmet()\n",
    "\n",
    "  # prepare testing data with cumulative variables\n",
    "  prepare_cumulative_history_csvs(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e148e",
   "metadata": {},
   "source": [
    "When we run the script directly (`__name__ == \"__main__\"`), our main focus is to ensure the preparation of cumulative history CSVs for testing data. By triggering the `prepare_cumulative_history_csvs` function with the `force` parameter set to True, we ensure that all necessary steps are taken to generate accurate cumulative values. Throughout this process, we manage tasks such as downloading GridMET data for specific variables, converting it into CSV format, and calculating cumulative values for each variable over the specified date range. The use of `force=True` ensures that the cumulative CSVs are regenerated if they already exist, mai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2bea0",
   "metadata": {},
   "source": [
    "# GridMET Climatology Data Retrieval and Analysis\n",
    "\n",
    "In this chapter, we do\n",
    "- **Data Collection:** : The script fetches gridMET climatology data from a specified source for various meteorological variables (e.g., temperature, precipitation) and multiple years.It ensures that the data is downloaded for each variable and year required for analysis.\n",
    "\n",
    "- **Data Processing:** After downloading, the script extracts relevant data for specific geographical locations corresponding to weather stations. It organizes the data into structured formats (CSV files) for easier handling and analysis.\n",
    "\n",
    "- **Data Integration:** We merge similar variables obtained from different years into separate CSV files. We then combine all variables together into a single comprehensive dataset for further analysis and modeling tasks. By integrating data from various sources and time periods, it creates a unified dataset that can provide insights into long-term weather patterns and trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5713a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import urllib.request\n",
    "from datetime import date, datetime\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "homedir = os.path.expanduser('~')\n",
    "\n",
    "# Changing the timeframe from 10 years to 1 year to demonstrate\n",
    "train_start_date = \"2020-01-03\"\n",
    "train_end_date = \"2021-12-31\"\n",
    "\n",
    "work_dir = f\"{homedir}/gridmet_test_run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb3626",
   "metadata": {},
   "source": [
    "We import necessary modules and libraries for its operation. These imports bring in functionalities like interacting with the operating system, handling file paths, making URL requests, managing dates and times, manipulating data, working with multi-dimensional arrays, and handling file system paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c9d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n",
    "\n",
    "year_list = [start_date.year + i for i in range(end_date.year - start_date.year + 1)]\n",
    "\n",
    "working_dir = work_dir\n",
    "#stations = pd.read_csv(f'{working_dir}/station_cell_mapping.csv')\n",
    "stations = pd.read_csv(f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\")\n",
    "gridmet_save_location = f'{working_dir}/gridmet_climatology'\n",
    "final_merged_csv = f\"{work_dir}/training_all_active_snotel_station_list_elevation.csv_gridmet.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72aace6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "In this code snippet, FutureWarnings are suppressed using `warnings.filterwarnings(\"ignore\", category=FutureWarning)`. Following that, the start and end dates for data processing are defined by converting the `train_start_date` and `train_end_date` strings into datetime objects. Subsequently, a list of years between the start and end dates is generated. The working directory path and file paths for SNOTEL stations data, GridMET climatology data storage, and the final merged CSV file are then set up accordingly. Overall, this snippet prepares the necessary parameters and file paths for subsequent data processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0bc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_directory():\n",
    "    f = list()\n",
    "    for files in glob.glob(gridmet_save_location + \"/*.nc\"):\n",
    "        f.append(files)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670deb8",
   "metadata": {},
   "source": [
    "We collect the names of files with the extension \".nc\" within a specified directory by iterating through all files, appending their names to a list, and returning the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ad571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, save_location):\n",
    "    try:\n",
    "        print(\"download_file\")\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            file_content = response.read()\n",
    "        file_name = os.path.basename(url)\n",
    "        save_path = os.path.join(save_location, file_name)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(file_content)\n",
    "        print(f\"File downloaded successfully and saved as: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87896c19",
   "metadata": {},
   "source": [
    "We attempt to download a file from a specified URL. We then save the downloaded file to a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bab2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gridmet_climatology():\n",
    "    folder_name = gridmet_save_location\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n",
    "    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n",
    "\n",
    "    for var in variables_list:\n",
    "        for y in year_list:\n",
    "            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n",
    "            print(\"downloading\", download_link)\n",
    "            if not os.path.exists(os.path.join(folder_name, var + '_' + '%s' % y + '.nc')):\n",
    "                download_file(download_link, folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697fb51",
   "metadata": {},
   "source": [
    "We set up a folder to store data. Then, we gather GridMET climatology data for a range of variables over multiple years. This involves collecting information on temperature, precipitation, vapor pressure deficit, reference evapotranspiration, maximum and minimum radiation, and wind speed. We ensure that the data for each variable and year combination is acquired and stored for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4e75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridmet_variable(file_name):\n",
    "    print(f\"reading values from {file_name}\")\n",
    "    result_data = []\n",
    "    ds = xr.open_dataset(file_name)\n",
    "    var_to_extract = list(ds.keys())\n",
    "    print(var_to_extract)\n",
    "    var_name = var_to_extract[0]\n",
    "    \n",
    "    df = pd.DataFrame(columns=['day', 'lat', 'lon', var_name])\n",
    "    \n",
    "    csv_file = f'{gridmet_save_location}/{Path(file_name).stem}.csv'\n",
    "    if os.path.exists(csv_file):\n",
    "    \tprint(f\"The file '{csv_file}' exists.\")\n",
    "    \treturn\n",
    "\n",
    "    for idx, row in stations.iterrows():\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "\t\t\n",
    "        subset_data = ds.sel(lat=lat, lon=lon, method='nearest')\n",
    "        subset_data['lat'] = lat\n",
    "        subset_data['lon'] = lon\n",
    "        converted_df = subset_data.to_dataframe()\n",
    "        converted_df = converted_df.reset_index(drop=False)\n",
    "        converted_df = converted_df.drop('crs', axis=1)\n",
    "        df = pd.concat([df, converted_df], ignore_index=True)\n",
    "        \n",
    "    result_df = df\n",
    "    print(\"got result_df : \", result_df.head())\n",
    "    result_df.to_csv(csv_file, index=False)\n",
    "    print(f'completed extracting data for {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20501ee",
   "metadata": {},
   "source": [
    "We retrieve data from a specific GridMET file, capturing information relevant to latitude and longitude coordinates provided in a dataset. Subsequently, we organize this data into a structured format, ensuring each entry corresponds to a specific day, latitude, and longitude, along with the associated variable values. Following this organization, we save this processed data as a CSV file, providing a convenient and accessible format for further analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5798ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_variables_from_different_years():\n",
    "    files = os.listdir(gridmet_save_location)\n",
    "    file_groups = {}\n",
    "\n",
    "    for filename in files:\n",
    "        base_name, year_ext = os.path.splitext(filename)\n",
    "        parts = base_name.split('_')\n",
    "        if len(parts) == 2 and year_ext == '.csv':\n",
    "            file_groups.setdefault(parts[0], []).append(filename)\n",
    "\n",
    "    for base_name, file_list in file_groups.items():\n",
    "        if len(file_list) > 1:\n",
    "            dfs = []\n",
    "            for filename in file_list[:5]:\n",
    "                df = pd.read_csv(os.path.join(gridmet_save_location, filename))\n",
    "                dfs.append(df)\n",
    "            merged_df = pd.concat(dfs, ignore_index=True)\n",
    "            merged_filename = f\"{base_name}_merged.csv\"\n",
    "            merged_df.to_csv(os.path.join(gridmet_save_location, merged_filename), index=False)\n",
    "            print(f\"Merged {file_list} into {merged_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f03a0",
   "metadata": {},
   "source": [
    "We collect list of files from a designated location. Then, we organize these files into groups based on similarities in their names. For each group of similar files, if there are multiple files present, we proceed to read each file as a DataFrame. Subsequently, we merge these DataFrames into a single comprehensive DataFrame. Following this merging process, we save the resulting merged DataFrame into a new CSV file. Finally, we print a notification message to indicate which files have been successfully merged. Through these steps, the function facilitates the consolidation of related data from different files into cohesive datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35fff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_variables_together():\n",
    "    merged_df = None\n",
    "    file_paths = []\n",
    "\n",
    "    for filename in os.listdir(gridmet_save_location):\n",
    "        if filename.endswith(\"_merged.csv\"):\n",
    "            file_paths.append(os.path.join(gridmet_save_location, filename))\n",
    "\t\n",
    "    rmin_merged_path = os.path.join(gridmet_save_location, 'rmin_merged.csv')\n",
    "    rmax_merged_path = os.path.join(gridmet_save_location, 'rmax_merged.csv')\n",
    "    tmmn_merged_path = os.path.join(gridmet_save_location, 'tmmn_merged.csv')\n",
    "    tmmx_merged_path = os.path.join(gridmet_save_location, 'tmmx_merged.csv')\n",
    "    \n",
    "    df_rmin = pd.read_csv(rmin_merged_path)\n",
    "    df_rmax = pd.read_csv(rmax_merged_path)\n",
    "    df_tmmn = pd.read_csv(tmmn_merged_path)\n",
    "    df_tmmx = pd.read_csv(tmmx_merged_path)\n",
    "    \n",
    "    df_rmin.rename(columns={'relative_humidity': 'relative_humidity_rmin'}, inplace=True)\n",
    "    df_rmax.rename(columns={'relative_humidity': 'relative_humidity_rmax'}, inplace=True)\n",
    "    df_tmmn.rename(columns={'air_temperature': 'air_temperature_tmmn'}, inplace=True)\n",
    "    df_tmmx.rename(columns={'air_temperature': 'air_temperature_tmmx'}, inplace=True)\n",
    "    \n",
    "    df_rmin.to_csv(os.path.join(gridmet_save_location, 'rmin_merged.csv'))\n",
    "    df_rmax.to_csv(os.path.join(gridmet_save_location, 'rmax_merged.csv'))\n",
    "    df_tmmn.to_csv(os.path.join(gridmet_save_location, 'tmmn_merged.csv'))\n",
    "    df_tmmx.to_csv(os.path.join(gridmet_save_location, 'tmmx_merged.csv'))\n",
    "    \n",
    "    if file_paths:\n",
    "        merged_df = pd.read_csv(file_paths[0])\n",
    "        for file_path in file_paths[1:10]:\n",
    "            df = pd.read_csv(file_path)\n",
    "            merged_df = pd.concat([merged_df, df], axis=1)\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "        merged_df.to_csv(final_merged_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d5177",
   "metadata": {},
   "source": [
    "We identify and collect CSV files with specific names from a designated location. Then, we proceed to read each of these CSV files, containing data for different variables, into separate DataFrames. Subsequently, we rename specific columns within each DataFrame to ensure clarity and consistency across variables. After updating column names, we overwrite the original CSV files with the renamed versions for consistency. Next, if there are multiple CSV files available, we merge them together into a single comprehensive DataFrame. To avoid redundancy and ensure data integrity, we remove any duplicated columns in the merged DataFrame. Finally, we save the merged DataFrame as a new CSV file, representing the combined dataset encompassing all variables. Through these steps, the function facilitates the integration of data from various sources into a unified dataset for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "803121ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - netcdf4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cftime-1.6.3               |  py311h9ea6feb_0         201 KB  conda-forge\n",
      "    netcdf4-1.6.3              |nompi_py311h40498cf_100         423 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         624 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cftime             conda-forge/osx-arm64::cftime-1.6.3-py311h9ea6feb_0 \n",
      "  netcdf4            conda-forge/osx-arm64::netcdf4-1.6.3-nompi_py311h40498cf_100 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.2.2~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "netcdf4-1.6.3        | 423 KB    |                                       |   0% \n",
      "netcdf4-1.6.3        | 423 KB    | #3                                    |   4% \u001b[A\n",
      "cftime-1.6.3         | 201 KB    | ##9                                   |   8% \u001b[A\n",
      "netcdf4-1.6.3        | 423 KB    | ##################################9   |  95% \u001b[A\n",
      "netcdf4-1.6.3        | 423 KB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfa36fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5netcdf\n",
      "  Downloading h5netcdf-1.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (from h5netcdf) (3.8.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from h5netcdf) (23.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/anaconda3/lib/python3.11/site-packages (from h5py->h5netcdf) (1.23.4)\n",
      "Downloading h5netcdf-1.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m720.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5netcdf\n",
      "Successfully installed h5netcdf-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4be46ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - h5netcdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install --force-reinstall -c conda-forge h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a49e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading http://www.northwestknowledge.net/metdata/data/tmmn_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmn_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmx_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/tmmx_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/pr_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/pr_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vpd_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vpd_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/etr_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/etr_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmax_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmax_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmin_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/rmin_2021.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vs_2020.nc\n",
      "downloading http://www.northwestknowledge.net/metdata/data/vs_2021.nc\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.nc\n",
      "['relative_humidity']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.nc\n",
      "['air_temperature']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.nc\n",
      "['potential_evapotranspiration']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/etr_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.nc\n",
      "['potential_evapotranspiration']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/etr_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.nc\n",
      "['relative_humidity']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/rmin_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.nc\n",
      "['air_temperature']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/tmmn_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.nc\n",
      "['air_temperature']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.nc\n",
      "['air_temperature']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/tmmx_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.nc\n",
      "['mean_vapor_pressure_deficit']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.nc\n",
      "['precipitation_amount']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/pr_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.nc\n",
      "['precipitation_amount']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/pr_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.nc\n",
      "['mean_vapor_pressure_deficit']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/vpd_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.nc\n",
      "['wind_speed']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/vs_2021.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.nc\n",
      "['relative_humidity']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.nc\n",
      "['wind_speed']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/vs_2020.csv' exists.\n",
      "reading values from /Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.nc\n",
      "['relative_humidity']\n",
      "The file '/Users/meghana/gridmet_test_run/gridmet_climatology/rmax_2021.csv' exists.\n",
      "Merged ['tmmn_2020.csv', 'tmmn_2021.csv', 'tmmn_merged.csv'] into tmmn_merged.csv\n",
      "Merged ['vs_merged.csv', 'vs_2020.csv', 'vs_2021.csv'] into vs_merged.csv\n",
      "Merged ['rmax_merged.csv', 'rmax_2020.csv', 'rmax_2021.csv'] into rmax_merged.csv\n",
      "Merged ['etr_merged.csv', 'etr_2020.csv', 'etr_2021.csv'] into etr_merged.csv\n",
      "Merged ['vpd_merged.csv', 'vpd_2020.csv', 'vpd_2021.csv'] into vpd_merged.csv\n",
      "Merged ['tmmx_2021.csv', 'tmmx_2020.csv', 'tmmx_merged.csv'] into tmmx_merged.csv\n",
      "Merged ['pr_2021.csv', 'pr_2020.csv', 'pr_merged.csv'] into pr_merged.csv\n",
      "Merged ['rmin_2021.csv', 'rmin_merged.csv', 'rmin_2020.csv'] into rmin_merged.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    download_gridmet_climatology()\n",
    "    \n",
    "    nc_files = get_files_in_directory()\n",
    "    for nc in nc_files:\n",
    "        get_gridmet_variable(nc)\n",
    "    \n",
    "    merge_similar_variables_from_different_years()\n",
    "    merge_all_variables_together()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71256ac1",
   "metadata": {},
   "source": [
    "We start by checking if our script is being executed directly. If it is, we proceed to download data related to GridMET climatology. After that, we retrieve a list of files from a directory, likely containing data files. Next, we iterate through each file, processing them to extract GridMET variables. Following that, we merge similar variables obtained from different years, presumably to create a comprehensive dataset. Lastly, we merge all the variables together, possibly creating a unified dataset. These actions collectively aim to handle and organize GridMET climatology data."
   ]
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
<<<<<<< HEAD
    "version": 2
=======
    "version": 3
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
=======
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
