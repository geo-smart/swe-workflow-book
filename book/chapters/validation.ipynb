{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Testing and Validation\n",
    "\n",
    "Techniques for validating SWE prediction models\n",
    "Importance of model validation in forecasting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35a5fc3f09e5e77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "898cb48fdc59bf41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrTh5-Sgr1Od"
      },
      "source": [
        "# Model Testing and Evaluation\n",
        "\n",
        "The goal of predicting Snow Water Equivalent exemplifies the integration of Machine learning with environmental science. This chapter delved into the testing and Evaluation part of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFvX25Khr2dW"
      },
      "source": [
        "To begin with, it is essential to grasp the function of the BaseHole class. This class represents the complete lifecycle of the project, guiding it from initial development through to its final deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkO0EdSFr4q1"
      },
      "source": [
        "BaseHole class is a meticulously crafted blueprint for constructing models capable of predicting SWE. It offers a structured approach to handling data, training models, and making predictions with unparalleled precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PZlyQKr48X"
      },
      "source": [
        "Let’s briefly discuss what is happening in this BaseHole class--\n",
        "\n",
        "\n",
        "\n",
        "*   Preprocessing: The model begins with preprocessing, a critical phase where raw data is transformed into a refined form suitable for training. The BaseHole class adeptly navigates this phase, loading data, cleaning it, and splitting it into training and testing sets. This preparatory step ensures that the models are fed data that is both digestible and informative, setting the stage for accurate predictions.\n",
        "*   Training: This is the center of Learning, with the data primed, the BaseHole class now moves on to the training phase. This is where the coalition of machine learning takes place as the class utilizes the power of its classifiers to learn from the training data. The model, through this process, uncovers patterns and insights hidden within the data, providing itself with the knowledge needed to predict SWE with confidence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B9qmiO7r5N9"
      },
      "source": [
        "Now comes the part that is one of the main focus of this chapter—**Testing**\n",
        "\n",
        "Within the extensive array of functionalities provided by the BaseHole class, the testing process is akin to a rigorous examination.\n",
        "Unveiling the test function:\n",
        "\n",
        "So, what is a test?\n",
        "\n",
        "The test function operates on a simple yet profound principle: it utilizes the model to predict outcomes based on the test dataset. By invoking the classifier's prediction method, the BaseHole class utilizes the trained model on the test data to forecast SWE values with precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sxGkvT-ysDR2"
      },
      "outputs": [],
      "source": [
        "def test(self):\n",
        "  '''\n",
        "  Tests the machine learning model on the testing data.\n",
        "  Returns: numpy.ndarray: The predicted results on the testing data.\n",
        "  '''\n",
        "  self.test_y_results = self.classifier.predict(self.test_x)\n",
        "  return self.test_y_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpzGEyDQsFaZ"
      },
      "source": [
        "**The Mechanics of Testing**\n",
        "\n",
        "At its core, the test function embodies the essence of machine learning validation. It executes the trained model's prediction method on the test_x dataset—a collection of features that the model has not encountered during its training phase. The function then returns the predicted SWE values, encapsulated within test_y_results, offering a glimpse into the model's predictive accuracy and reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0nRqBFQsICP"
      },
      "source": [
        "# Validation/Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toh-cEggsKZ0"
      },
      "source": [
        "So now we have made a model, trained the model, and made predictions on a test dataset, but how to evaluate all of this? For this, we use multiple Evaluation metrics. A model needs to go through a rigorous validation process that assesses its effectiveness and accuracy. Evaluation is a testament to the model’s commitment to precision, ensuring that the predictions made are not only reliable but also meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unwqjmuhsMsc"
      },
      "source": [
        "For this project we have a comprehensive suite of metrics -- Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared (R2), and Root Mean Squared Error (RMSE). Each metric offers a unique lens through which the model's performance can be scrutinized, from the average error per prediction (MAE) to the proportion of variance explained (R2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbM36bgsRCB"
      },
      "source": [
        "**Insights**\n",
        "\n",
        "Upon invoking the evaluation method, the class starts a detailed analysis of the model's predictions. By comparing these predictions against actual values from the test dataset, the method illuminates the model's strengths and areas for improvement.\n",
        "\n",
        "The output—a dictionary of metrics—serves as a beacon, guiding further refinement and optimization of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jwo8aLhsTEp"
      },
      "source": [
        "**The Testament of Metrics**\n",
        "\n",
        "\n",
        "*   MAE: This metric provides an average of the absolute errors between predicted and actual values, offering a straightforward measure of prediction accuracy.\n",
        "\n",
        "*   MSE: By squaring the errors before averaging, MSE penalizes larger errors more heavily, providing insight into the variance of the model's predictions.\n",
        "*   R2: The R2 score reveals how well the model's predictions conform to the actual data, serving as a gauge of the model's explanatory power.\n",
        "\n",
        "\n",
        "*   RMSE: As the square root of MSE, RMSE offers a measure of error in the same units as the predicted value, making it intuitively interpretable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdYjCmmbsVzW"
      },
      "source": [
        "# The Evaluation Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlRUU0lzsW4r"
      },
      "source": [
        "Upon invocation, the evaluate method undertakes the task of computing these metrics, using the predictions generated by the RandomForestHole model (self.test_y_results) and comparing them against the actual values (self.test_y) from the test dataset. This comparison is the crux of the evaluation, offering a window into the model's predictive capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OIcffx3sZAz"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "import joblib\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "# import geopandas as gpd\n",
        "# import geojson\n",
        "import os.path\n",
        "import math\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "exit(0)  # for now, the workflow is not ready yet\n",
        "\n",
        "# read the grid geometry file\n",
        "\n",
        "\n",
        "# read the grid geometry file\n",
        "homedir = os.path.expanduser('~')\n",
        "print(homedir)\n",
        "github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n",
        "modis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\n",
        "modis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n",
        "\n",
        "pd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n",
        "\n",
        "all_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\n",
        "all_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n",
        "def evaluate(self):\n",
        "    y_predicted = model.predict(test_features)\n",
        "    mae = metrics.mean_absolute_error(y_test, y_predicted)\n",
        "    mse = metrics.mean_squared_error(y_test, y_predicted)\n",
        "    r2 = metrics.r2_score(y_test, y_predicted)\n",
        "    rmse = math.sqrt(mse)\n",
        "\n",
        "    print(\"The {} model performance for testing set\".format(model_name))\n",
        "    print(\"--------------------------------------\")\n",
        "    print('MAE is {}'.format(mae))\n",
        "    print('MSE is {}'.format(mse))\n",
        "    print('R2 score is {}'.format(r2))\n",
        "    print('RMSE is {}'.format(rmse))\n",
        "\n",
        "    return y_predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjgBbKVVsbKF"
      },
      "source": [
        "**Computing the Metrics:** Leveraging the metrics module from scikit-learn, the function calculates MAE, MSE, R2, and RMSE. Each of these calculations provides a different lens through which to view the model's performance, from average error rates (MAE, RMSE) to the model's explanatory power (R2) and the variance of its predictions (MSE).\n",
        "\n",
        "**Interpreting the Results:** The function not only computes these metrics but also prints them out, offering immediate insight into the model's efficacy. This step is vital for iterative model improvement, allowing data scientists to diagnose and address specific areas where the model may fall short.\n",
        "\n",
        "**Returning the Metrics:** Finally, the function encapsulates these metrics in a dictionary and returns it. This encapsulation allows for the metrics to be easily accessed, shared, and utilized in further analyses or reports, facilitating a deeper understanding of the model's impact and areas for enhancement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3zrQxxUsmOK"
      },
      "source": [
        "![](../img/Validation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1pxcIYgsvKD"
      },
      "source": [
        "All the necessary functions are called in the model_train_validate process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2hKsntLszCu"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  print(\"Train Models\")\n",
        "  # Choose the machine learning models to train (e.g., RandomForestHole, XGBoostHole, ETHole)\n",
        "  worm_holes = [ETHole()]\n",
        "  for hole in worm_holes:\n",
        "  # Perform preprocessing for the selected model\n",
        "      hole.preprocessing()\n",
        "  print(hole.train_x.shape)\n",
        "  print(hole.train_y.shape)\n",
        "  # Train the machine learning model\n",
        "  hole.train()\n",
        "  # Test the trained model\n",
        "  hole.test()\n",
        "  # Evaluate the model's performance\n",
        "  hole.evaluate()\n",
        "  # Save the trained model\n",
        "  hole.save()\n",
        "  print(\"Finished training and validating all the models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJDaq-uls1AR"
      },
      "source": [
        "In conclusion, testing and validation form the bedrock of predictive excellence in the SnowCast project. They are not merely steps in the machine learning workflow but are the very processes that ensure the models we build are not just algorithms but are reliable interpreters of the natural world."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> bb3679fabc9b231f0a51b8a982f59cb4d1adb432
