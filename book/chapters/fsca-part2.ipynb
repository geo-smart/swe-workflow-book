{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating MODIS Snow Cover Data with Station Measurements for Training and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will explore the process of mapping snow cover data from MODIS GeoTIFF files with station locations, and merge the results into a comprehensive CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "from rasterio.enums import Resampling\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- os: For file and directory operations.\n",
    "- pandas: For data manipulation and analysis.\n",
    "- rasterio: For reading and writing raster data.\n",
    "- pyproj: For coordinate transformations.\n",
    "- rasterio.enums.Resampling: For resampling options in rasterio.\n",
    "- concurrent.futures: For parallel execution.\n",
    "- datetime, timedelta: For date and time operations.\n",
    "- dask.dataframe: For handling large datasets.\n",
    "- numpy: For numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '../data/'\n",
    "train_start_date = \"2023-01-01\"\n",
    "train_end_date = \"2023-01-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = f\"../data/fsca\"\n",
    "folder_path = f\"../data/fsca/final_output/\"\n",
    "new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n",
    "cell_to_modis_mapping = f\"{working_dir}/training_cell_to_modis_mapper_original_snotel_stations.csv\"\n",
    "non_station_random_points_file = f\"{work_dir}/non_station_random_points_in_westus.csv\"\n",
    "only_active_ghcd_station_in_west_conus_file = f\"{working_dir}/active_ghcnd_station_only_list.csv\"\n",
    "ghcd_station_to_modis_mapper_file = f\"{working_dir}/active_ghcnd_mapper_modis.csv\"\n",
    "all_training_points_with_snotel_ghcnd_file = f\"{working_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n",
    "modis_day_wise = f\"{working_dir}/final_output/\"\n",
    "os.makedirs(modis_day_wise, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defines working directories and file paths.\n",
    "- Creates necessary directories if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_modis_to_station(row, src):\n",
    "    drow, dcol = src.index(row[\"lon\"], row[\"lat\"])\n",
    "    return drow, dcol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above method Maps MODIS pixel coordinates to station coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Points in the MODIS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_non_station_points():\n",
    "    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "    print(f\"loading geotiff {sample_modis_tif}\")\n",
    "    with rasterio.open(sample_modis_tif) as src:\n",
    "        bounds = src.bounds\n",
    "        transform = src.transform\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        raster_array = src.read(1)\n",
    "\n",
    "        random_points = []\n",
    "        while len(random_points) < 4000:\n",
    "            random_x = np.random.uniform(bounds.left, bounds.right)\n",
    "            random_y = np.random.uniform(bounds.bottom, bounds.top)\n",
    "            col, row = ~transform * (random_x, random_y)\n",
    "\n",
    "            if 0 <= row < height and 0 <= col < width:\n",
    "                value = raster_array[int(row), int(col)]\n",
    "                if value != 239 and value != 255:\n",
    "                    random_points.append((random_x, random_y, col, row))\n",
    "\n",
    "        random_points = [(lat, lon, col, row) for lon, lat, col, row in random_points]\n",
    "        random_points_df = pd.DataFrame(random_points, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "        random_points_df.to_csv(non_station_random_points_file, index=False)\n",
    "        print(f\"random points are saved to {non_station_random_points_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads a sample GeoTIFF file.\n",
    "- Generates random points within the bounds of the raster, ensuring they are valid points.\n",
    "- Saves the generated points to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MODIS Grid Mapper for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modis_grid_mapper_training():\n",
    "    if os.path.exists(cell_to_modis_mapping):\n",
    "        print(f\"The file {cell_to_modis_mapping} exists. skip.\")\n",
    "    else:\n",
    "        print(f\"start to generate {cell_to_modis_mapping}\")\n",
    "        station_df = pd.read_csv(new_base_station_list_file)\n",
    "        print(\"original station_df describe() = \", station_df.describe())\n",
    "\n",
    "        sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "\n",
    "        with rasterio.open(sample_modis_tif) as src:\n",
    "            transform = src.transform\n",
    "            station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n",
    "                src.transform, \n",
    "                station_df[\"longitude\"], \n",
    "                station_df[\"latitude\"])\n",
    "            station_df.to_csv(cell_to_modis_mapping, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "            print(\"after mapped modis station_df.describe() = \", station_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this function is to create a mapping between snotel station locations and a MODIS satellite image. This involves transforming the geographical coordinates of the stations into pixel coordinates on the MODIS image.\n",
    "\n",
    "Uses rasterio to read the modis image file and get information about its spatial extent (how much of the Earth it covers) and its transformation matrix (how to convert pixel coordinates to geographic coordinates).\n",
    "\n",
    "Converts the geographic coordinates (longitude and latitude) of the snotel stations into pixel coordinates (modis_x, modis_y) for the MODIS image.\n",
    "\n",
    "Uses the transformation matrix from the MODIS image to map each station's geographic coordinates to its corresponding pixel location in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Station and Non-Station Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_station_and_non_station_to_one_csv():\n",
    "    df1 = pd.read_csv(cell_to_modis_mapping)\n",
    "    df2 = pd.read_csv(non_station_random_points_file)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    combined_df.to_csv(all_training_points_with_station_and_non_station_file, index=False)\n",
    "    print(f\"Combined CSV saved to {all_training_points_with_station_and_non_station_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges station data and non-station random points into one CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SNOTEL and GHCND Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_snotel_ghcnd_station_to_one_csv():\n",
    "    df1 = pd.read_csv(cell_to_modis_mapping)\n",
    "    df2 = pd.read_csv(ghcd_station_to_modis_mapper_file)\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    combined_df.to_csv(all_training_points_with_snotel_ghcnd_file, index=False)\n",
    "    print(f\"Combined CSV saved to {all_training_points_with_snotel_ghcnd_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GHCND Station Mapping for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ghcnd_station_mapping_training():\n",
    "    if os.path.exists(ghcd_station_to_modis_mapper_file):\n",
    "        print(f\"The file {ghcd_station_to_modis_mapper_file} exists. skip.\")\n",
    "    else:\n",
    "        print(f\"start to generate {ghcd_station_to_modis_mapper_file}\")\n",
    "        station_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n",
    "        station_df = station_df.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
    "        print(\"original station_df describe() = \", station_df.describe())\n",
    "\n",
    "        sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n",
    "\n",
    "        with rasterio.open(sample_modis_tif) as src:\n",
    "            transform = src.transform\n",
    "            station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n",
    "                src.transform, \n",
    "                station_df[\"longitude\"],\n",
    "                station_df[\"latitude\"])\n",
    "            station_df.to_csv(ghcd_station_to_modis_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n",
    "            print(f\"the new mapper to the ghcnd is saved to {ghcd_station_to_modis_mapper_file}\")\n",
    "            print(\"after mapped modis station_df.describe() = \", station_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads GHCND station data and a sample MODIS GeoTIFF file.\n",
    "- Maps GHCND station coordinates to MODIS grid coordinates.\n",
    "- Saves the mapping to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Band Value for a Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_value(row, src):\n",
    "    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n",
    "        valid_value =  src.read(1, window=((int(row[\"modis_y\"]), int(row[\"modis_y\"])+1), (int(row[\"modis_x\"]), int(row[\"modis_x\"])+1)))\n",
    "        return valid_value[0,0]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_band_value function is used to get the value of a pixel from a satellite image (in this case, from a MODIS raster image) based on specific coordinates.\n",
    "\n",
    "**Inputs**:\n",
    "\n",
    "**row**: This contains the coordinates where you want to get the pixel value. Specifically, it has modis_x and modis_y, which tell you the column and row of the pixel in the image.\n",
    "\n",
    "**src**: This is the MODIS raster image you are working with. It's like a map where each point (pixel) has a value (like color or temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves the value from the MODIS raster at the coordinates specified in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, current_date_str, outfile):\n",
    "  print(f\"processing {file_path}\")\n",
    "  station_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n",
    "  # print(\"station_df.head() = \", station_df.head())\n",
    "\n",
    "  # Apply get_band_value for each row in the DataFrame\n",
    "  with rasterio.open(file_path) as src:\n",
    "    # Apply get_band_value for each row in the DataFrame\n",
    "    # Get the affine transformation matrix\n",
    "    transform = src.transform\n",
    "\n",
    "    # Extract the spatial extent using the affine transformation\n",
    "    left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n",
    "\n",
    "    # Print the spatial extent\n",
    "    # print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n",
    "\n",
    "    station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n",
    "\n",
    "    \n",
    "  # Prepare final data\n",
    "  station_df['date'] = current_date_str\n",
    "  station_df.to_csv(outfile, index=False, \n",
    "                    columns=['date', 'latitude', 'longitude', 'fsca'])\n",
    "  print(f\"Saved to csv: {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(start_date, end_date):\n",
    "  import glob\n",
    "  # Find CSV files within the specified date range\n",
    "  csv_files = glob.glob(folder_path + '*_training_output_station_corrected.csv')\n",
    "  relevant_csv_files = []\n",
    "\n",
    "  for c in csv_files:\n",
    "    # Extract the date from the file name\n",
    "    print(\"c = \", c)\n",
    "    file_name = os.path.basename(c)\n",
    "    date_str = file_name.split('_')[0]  # Assuming the date is part of the file name\n",
    "    print(\"date_str = \", date_str)\n",
    "    file_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    # Check if the file date is within the specified range\n",
    "    if start_date <= file_date <= end_date:\n",
    "      relevant_csv_files.append(c)\n",
    "#       # Read and concatenate only relevant CSV files\n",
    "#       df = []\n",
    "#       for c in relevant_csv_files:\n",
    "#         tmp = pd.read_csv(c, low_memory=False, usecols=['date', 'latitude', 'longitude', 'fsca'])\n",
    "#         df.append(tmp)\n",
    "\n",
    "#         combined_df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "  # Initialize a Dask DataFrame\n",
    "  print(\"start to use dask to read all csv files\")\n",
    "  dask_df = dd.read_csv(relevant_csv_files)\n",
    "\n",
    "  # Save the merged DataFrame to a CSV file\n",
    "  output_file = f'{working_dir}/fsca_final_training_all.csv'\n",
    "  # Write the Dask DataFrame to a single CSV file\n",
    "  print(f\"saving all csvs into one file: {output_file}\")\n",
    "  dask_df.to_csv(output_file, index=False, single_file=True)\n",
    "  #combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "  #print(combined_df.describe())\n",
    "  print(f\"Merged data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  \n",
    "  start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n",
    "  \n",
    "  end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n",
    "  \n",
    "  prepare_modis_grid_mapper_training()\n",
    "  prepare_ghcnd_station_mapping_training()\n",
    "  # running this function will generate a new set of random points\n",
    "  # generate_random_non_station_points()\n",
    "  #merge_station_and_non_station_to_one_csv()\n",
    "  merge_snotel_ghcnd_station_to_one_csv()\n",
    "  \n",
    "  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "  for i in date_list:\n",
    "    current_date = i.strftime(\"%Y-%m-%d\")\n",
    "    #print(f\"extracting data for {current_date}\")\n",
    "    outfile = os.path.join(modis_day_wise, f'{current_date}_training_output_station_with_ghcnd.csv')\n",
    "    if os.path.exists(outfile):\n",
    "      print(f\"The file {outfile} exists. skip.\")\n",
    "      pass\n",
    "    else:\n",
    "      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date, outfile)\n",
    "  \n",
    "  merge_csv(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file ../data/fsca/training_cell_to_modis_mapper_original_snotel_stations.csv exists. skip.\n",
      "The file ../data/fsca/active_ghcnd_mapper_modis.csv exists. skip.\n",
      "Combined CSV saved to ../data/fsca/all_training_points_snotel_ghcnd_in_westus.csv\n",
      "The file ../data/fsca/final_output/2023-01-01_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-02_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-03_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-04_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-05_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-06_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-07_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-08_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-09_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-10_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-11_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-12_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-13_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-14_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-15_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-16_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-17_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-18_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-19_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-20_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-21_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-22_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-23_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-24_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-25_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-26_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-27_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-28_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-29_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-30_training_output_station_with_ghcnd.csv exists. skip.\n",
      "The file ../data/fsca/final_output/2023-01-31_training_output_station_with_ghcnd.csv exists. skip.\n",
      "start to use dask to read all csv files\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: empty urlpath sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/csv.py:771\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    759\u001b[0m     urlpath,\n\u001b[1;32m    760\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    770\u001b[0m ):\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_pandas(\n\u001b[1;32m    772\u001b[0m         reader,\n\u001b[1;32m    773\u001b[0m         urlpath,\n\u001b[1;32m    774\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[1;32m    775\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m    776\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    777\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[1;32m    778\u001b[0m         sample_rows\u001b[38;5;241m=\u001b[39msample_rows,\n\u001b[1;32m    779\u001b[0m         enforce\u001b[38;5;241m=\u001b[39menforce,\n\u001b[1;32m    780\u001b[0m         assume_missing\u001b[38;5;241m=\u001b[39massume_missing,\n\u001b[1;32m    781\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    782\u001b[0m         include_path_column\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    784\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/dataframe/io/csv.py:538\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# Translate the input urlpath to a simple path list\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     paths \u001b[38;5;241m=\u001b[39m get_fs_token_paths(urlpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)[\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    540\u001b[0m     ]\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;66;03m# Check for at least one valid path\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fsspec/core.py:617\u001b[0m, in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m urlpath:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty urlpath sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    618\u001b[0m urlpath0 \u001b[38;5;241m=\u001b[39m stringify_path(\u001b[38;5;28mlist\u001b[39m(urlpath)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: empty urlpath sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsca Data extraction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     process_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodis_day_wise\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__snow_cover.tif\u001b[39m\u001b[38;5;124m'\u001b[39m, current_date, outfile)\n\u001b[0;32m---> 25\u001b[0m merge_csv(start_date, end_date)\n",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m, in \u001b[0;36mmerge_csv\u001b[0;34m(start_date, end_date)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#       # Read and concatenate only relevant CSV files\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#       df = []\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#       for c in relevant_csv_files:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m# Initialize a Dask DataFrame\u001b[39;00m\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart to use dask to read all csv files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m   dask_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(relevant_csv_files)\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;66;03m# Save the merged DataFrame to a CSV file\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworking_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fsca_final_training_all.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_collection.py:5086\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(path, header, usecols, dtype_backend, storage_options, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   5085\u001b[0m     path \u001b[38;5;241m=\u001b[39m stringify_path(path)\n\u001b[0;32m-> 5086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m   5087\u001b[0m     ReadCSV(\n\u001b[1;32m   5088\u001b[0m         path,\n\u001b[1;32m   5089\u001b[0m         columns\u001b[38;5;241m=\u001b[39musecols,\n\u001b[1;32m   5090\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   5091\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   5092\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   5093\u001b[0m         header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   5094\u001b[0m         dataframe_backend\u001b[38;5;241m=\u001b[39mdataframe_backend,\n\u001b[1;32m   5095\u001b[0m     )\n\u001b[1;32m   5096\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/_collection.py:4764\u001b[0m, in \u001b[0;36mnew_collection\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m   4762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[1;32m   4763\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4764\u001b[0m     meta \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   4765\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[1;32m   4766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/io/csv.py:85\u001b[0m, in \u001b[0;36mReadCSV._meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddf\u001b[38;5;241m.\u001b[39m_meta\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask_expr/io/csv.py:75\u001b[0m, in \u001b[0;36mReadCSV._ddf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[1;32m     69\u001b[0m             header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[1;32m     70\u001b[0m             storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     72\u001b[0m         )\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m     73\u001b[0m         columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(meta\u001b[38;5;241m.\u001b[39mcolumns)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[1;32m     77\u001b[0m     usecols\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m     78\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[1;32m     79\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/dask/backends.py:142\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: empty urlpath sequence"
     ]
    }
   ],
   "source": [
    "main()\n",
    "print(\"fsca Data extraction complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
